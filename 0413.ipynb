{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results.pickle', 'rb') as file:\n",
    "    CV_results = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf - Average MSE: 70.30974023492061\n",
      "rf - Average R-squared: 0.8101496535053767\n",
      "lr - Average MSE: 106.82065585836162\n",
      "lr - Average R-squared: 0.7117825379556579\n",
      "svm - Average MSE: 90.41466515047632\n",
      "svm - Average R-squared: 0.7560451437845004\n",
      "gbdt - Average MSE: 70.55279908672652\n",
      "gbdt - Average R-squared: 0.8095313612591266\n",
      "gbrf - Average MSE: 68.01203470270113\n",
      "gbrf - Average R-squared: 0.8165098706484301\n",
      "transformer_cnn - Average MSE: 74.22231011646878\n",
      "transformer_cnn - Average R-squared: 0.7997836091783046\n",
      "mlp - Average MSE: 70.00989851888764\n",
      "mlp - Average R-squared: 0.8113137281841244\n",
      "cnn - Average MSE: 71.38202684349044\n",
      "cnn - Average R-squared: 0.8073058061898983\n",
      "lr-horvath - Average MSE: 83.32123928904142\n",
      "lr-horvath - Average R-squared: 0.7749070685288426\n",
      "rf-horvath - Average MSE: 68.89830705247866\n",
      "rf-horvath - Average R-squared: 0.8142857472577696\n",
      "gbdt-horvath - Average MSE: 67.15795029470992\n",
      "gbdt-horvath - Average R-squared: 0.8190221258072062\n",
      "gbrf-horvath - Average MSE: 63.587325370152996\n",
      "gbrf-horvath - Average R-squared: 0.8285430198843233\n",
      "SVM-horvath - Average MSE: 75.93715153030396\n",
      "SVM-horvath - Average R-squared: 0.7950539399156554\n",
      "mlp-horvath - Average MSE: 77.74407054351454\n",
      "mlp-horvath - Average R-squared: 0.7900751197619384\n",
      "cnn-horvath - Average MSE: 65.34574246724262\n",
      "cnn-horvath - Average R-squared: 0.823884600560096\n",
      "lr-filtered - Average MSE: 97.18560380514654\n",
      "lr-filtered - Average R-squared: 0.7377092203649753\n",
      "rf-filtered - Average MSE: 72.05063835833843\n",
      "rf-filtered - Average R-squared: 0.8056909692490489\n",
      "gbdt-filtered - Average MSE: 73.16568187664501\n",
      "gbdt-filtered - Average R-squared: 0.8025683894709305\n",
      "gbrf-filtered - Average MSE: 70.30374011038623\n",
      "gbrf-filtered - Average R-squared: 0.8103679497800812\n",
      "SVM-filtered - Average MSE: 80.99684626949893\n",
      "SVM-filtered - Average R-squared: 0.7811925972896743\n",
      "mlp-filtered - Average MSE: 73.09950323679593\n",
      "mlp-filtered - Average R-squared: 0.8026097978547702\n",
      "cnn-filtered - Average MSE: 73.04952207519013\n",
      "cnn-filtered - Average R-squared: 0.8027919934466095\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Print the average MSE and R-squared values for each model\n",
    "for model_name, model_results in CV_results.items():\n",
    "    print(f\"{model_name} - Average MSE: {np.mean(model_results['mse'])}\")\n",
    "    print(f\"{model_name} - Average R-squared: {np.mean(model_results['r_squared'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter models with names containing 'filtered' or 'horvath'\n",
    "filtered_models = {key: value for key, value in CV_results.items() if 'filtered' in key or 'horvath' in key}\n",
    "\n",
    "# Create box plots for MSE\n",
    "mse_values = [np.array(model_results['mse']) for model_results in filtered_models.values()]\n",
    "plt.boxplot(mse_values, labels=filtered_models.keys())\n",
    "plt.title('MSE Comparison Between Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "# Rotate x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=-45)\n",
    "plt.show()\n",
    "\n",
    "# Create box plots for R-squared\n",
    "r_squared_values = [np.array(model_results['r_squared']) for model_results in filtered_models.values()]\n",
    "plt.boxplot(r_squared_values, labels=filtered_models.keys())\n",
    "plt.title('R-squared Comparison Between Models')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R-squared')\n",
    "# Rotate x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=-45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the combined_pivoted_samples_data object from the local file\n",
    "with open('combined_pivoted_samples_data_2_resetsampleid_clean.pickle', 'rb') as file:\n",
    "    combined_pivoted_samples_2_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6397</th>\n",
       "      <th>6398</th>\n",
       "      <th>6399</th>\n",
       "      <th>6400</th>\n",
       "      <th>6401</th>\n",
       "      <th>6402</th>\n",
       "      <th>6403</th>\n",
       "      <th>6404</th>\n",
       "      <th>6405</th>\n",
       "      <th>6406</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cg00000029</th>\n",
       "      <td>0.464197</td>\n",
       "      <td>0.454883</td>\n",
       "      <td>0.485764</td>\n",
       "      <td>0.480785</td>\n",
       "      <td>0.501220</td>\n",
       "      <td>0.499918</td>\n",
       "      <td>0.485852</td>\n",
       "      <td>0.512442</td>\n",
       "      <td>0.518155</td>\n",
       "      <td>0.417986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.479882</td>\n",
       "      <td>0.485316</td>\n",
       "      <td>0.511304</td>\n",
       "      <td>0.534741</td>\n",
       "      <td>0.516068</td>\n",
       "      <td>0.402368</td>\n",
       "      <td>0.432730</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.452750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg00000108</th>\n",
       "      <td>0.941091</td>\n",
       "      <td>0.939033</td>\n",
       "      <td>0.918802</td>\n",
       "      <td>0.929908</td>\n",
       "      <td>0.934548</td>\n",
       "      <td>0.950543</td>\n",
       "      <td>0.925855</td>\n",
       "      <td>0.941330</td>\n",
       "      <td>0.938528</td>\n",
       "      <td>0.933814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.933958</td>\n",
       "      <td>0.922079</td>\n",
       "      <td>0.937618</td>\n",
       "      <td>0.937884</td>\n",
       "      <td>0.936129</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.921907</td>\n",
       "      <td>0.942789</td>\n",
       "      <td>0.908963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg00000109</th>\n",
       "      <td>0.911182</td>\n",
       "      <td>0.596455</td>\n",
       "      <td>0.870333</td>\n",
       "      <td>0.889689</td>\n",
       "      <td>0.890450</td>\n",
       "      <td>0.898493</td>\n",
       "      <td>0.893972</td>\n",
       "      <td>0.892010</td>\n",
       "      <td>0.900841</td>\n",
       "      <td>0.883539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904574</td>\n",
       "      <td>0.906665</td>\n",
       "      <td>0.869559</td>\n",
       "      <td>0.901234</td>\n",
       "      <td>0.921915</td>\n",
       "      <td>0.880211</td>\n",
       "      <td>0.836376</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.872703</td>\n",
       "      <td>0.917163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg00000165</th>\n",
       "      <td>0.132014</td>\n",
       "      <td>0.206917</td>\n",
       "      <td>0.162861</td>\n",
       "      <td>0.197780</td>\n",
       "      <td>0.148437</td>\n",
       "      <td>0.224093</td>\n",
       "      <td>0.400489</td>\n",
       "      <td>0.194553</td>\n",
       "      <td>0.134710</td>\n",
       "      <td>0.204569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197673</td>\n",
       "      <td>0.201347</td>\n",
       "      <td>0.235386</td>\n",
       "      <td>0.234210</td>\n",
       "      <td>0.239195</td>\n",
       "      <td>0.330298</td>\n",
       "      <td>0.225160</td>\n",
       "      <td>0.243233</td>\n",
       "      <td>0.185050</td>\n",
       "      <td>0.203322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg00000236</th>\n",
       "      <td>0.717861</td>\n",
       "      <td>0.723935</td>\n",
       "      <td>0.719196</td>\n",
       "      <td>0.704061</td>\n",
       "      <td>0.754913</td>\n",
       "      <td>0.829192</td>\n",
       "      <td>0.723782</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.731872</td>\n",
       "      <td>0.742913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684694</td>\n",
       "      <td>0.667625</td>\n",
       "      <td>0.651572</td>\n",
       "      <td>0.641138</td>\n",
       "      <td>0.697076</td>\n",
       "      <td>0.675419</td>\n",
       "      <td>0.760541</td>\n",
       "      <td>0.702650</td>\n",
       "      <td>0.662514</td>\n",
       "      <td>0.679006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch.9.98937537R</th>\n",
       "      <td>0.042808</td>\n",
       "      <td>0.036811</td>\n",
       "      <td>0.042844</td>\n",
       "      <td>0.042258</td>\n",
       "      <td>0.039613</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.031119</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.039919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066904</td>\n",
       "      <td>0.058422</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.081995</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.080378</td>\n",
       "      <td>0.072304</td>\n",
       "      <td>0.059447</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>0.079340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch.9.98957343R</th>\n",
       "      <td>0.052589</td>\n",
       "      <td>0.053343</td>\n",
       "      <td>0.045973</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>0.039254</td>\n",
       "      <td>0.043023</td>\n",
       "      <td>0.037075</td>\n",
       "      <td>0.048277</td>\n",
       "      <td>0.041296</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097625</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>0.084208</td>\n",
       "      <td>0.086902</td>\n",
       "      <td>0.094795</td>\n",
       "      <td>0.087245</td>\n",
       "      <td>0.061635</td>\n",
       "      <td>0.076514</td>\n",
       "      <td>0.075343</td>\n",
       "      <td>0.092468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch.9.98959675F</th>\n",
       "      <td>0.035624</td>\n",
       "      <td>0.075618</td>\n",
       "      <td>0.126421</td>\n",
       "      <td>0.084051</td>\n",
       "      <td>0.165874</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.084294</td>\n",
       "      <td>0.052505</td>\n",
       "      <td>0.098726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212610</td>\n",
       "      <td>0.186146</td>\n",
       "      <td>0.176043</td>\n",
       "      <td>0.161922</td>\n",
       "      <td>0.191574</td>\n",
       "      <td>0.171343</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.229945</td>\n",
       "      <td>0.186564</td>\n",
       "      <td>0.210487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch.9.98989607R</th>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.017428</td>\n",
       "      <td>0.021752</td>\n",
       "      <td>0.027504</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>0.025469</td>\n",
       "      <td>0.018837</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>0.015910</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067896</td>\n",
       "      <td>0.054130</td>\n",
       "      <td>0.049284</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>0.052651</td>\n",
       "      <td>0.062921</td>\n",
       "      <td>0.052991</td>\n",
       "      <td>0.050036</td>\n",
       "      <td>0.050322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch.9.991104F</th>\n",
       "      <td>0.043850</td>\n",
       "      <td>0.032950</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>0.053007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>0.095781</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.143222</td>\n",
       "      <td>0.250320</td>\n",
       "      <td>0.145149</td>\n",
       "      <td>0.127603</td>\n",
       "      <td>0.141558</td>\n",
       "      <td>0.163660</td>\n",
       "      <td>0.133061</td>\n",
       "      <td>0.129455</td>\n",
       "      <td>0.152954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470417 rows  6405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5     \\\n",
       "cg00000029      0.464197  0.454883  0.485764  0.480785  0.501220  0.499918   \n",
       "cg00000108      0.941091  0.939033  0.918802  0.929908  0.934548  0.950543   \n",
       "cg00000109      0.911182  0.596455  0.870333  0.889689  0.890450  0.898493   \n",
       "cg00000165      0.132014  0.206917  0.162861  0.197780  0.148437  0.224093   \n",
       "cg00000236      0.717861  0.723935  0.719196  0.704061  0.754913  0.829192   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "ch.9.98937537R  0.042808  0.036811  0.042844  0.042258  0.039613  0.035309   \n",
       "ch.9.98957343R  0.052589  0.053343  0.045973  0.048733  0.039254  0.043023   \n",
       "ch.9.98959675F  0.035624  0.075618  0.126421  0.084051  0.165874  0.088889   \n",
       "ch.9.98989607R  0.028066  0.017428  0.021752  0.027504  0.020889  0.025469   \n",
       "ch.9.991104F    0.043850  0.032950  0.022375  0.053007  0.000000  0.013991   \n",
       "\n",
       "                    6         7         8         9     ...      6397  \\\n",
       "cg00000029      0.485852  0.512442  0.518155  0.417986  ...  0.555312   \n",
       "cg00000108      0.925855  0.941330  0.938528  0.933814  ...  0.930338   \n",
       "cg00000109      0.893972  0.892010  0.900841  0.883539  ...  0.904574   \n",
       "cg00000165      0.400489  0.194553  0.134710  0.204569  ...  0.197673   \n",
       "cg00000236      0.723782  0.695142  0.731872  0.742913  ...  0.684694   \n",
       "...                  ...       ...       ...       ...  ...       ...   \n",
       "ch.9.98937537R  0.031304  0.031119  0.031373  0.039919  ...  0.066904   \n",
       "ch.9.98957343R  0.037075  0.048277  0.041296  0.049226  ...  0.097625   \n",
       "ch.9.98959675F  0.097599  0.084294  0.052505  0.098726  ...  0.212610   \n",
       "ch.9.98989607R  0.018837  0.021279  0.015910  0.020918  ...  0.067896   \n",
       "ch.9.991104F    0.037836  0.032358  0.095781  0.018752  ...  0.138564   \n",
       "\n",
       "                    6398      6399      6400      6401      6402      6403  \\\n",
       "cg00000029      0.479882  0.485316  0.511304  0.534741  0.516068  0.402368   \n",
       "cg00000108      0.933958  0.922079  0.937618  0.937884  0.936129  0.906900   \n",
       "cg00000109      0.906665  0.869559  0.901234  0.921915  0.880211  0.836376   \n",
       "cg00000165      0.201347  0.235386  0.234210  0.239195  0.330298  0.225160   \n",
       "cg00000236      0.667625  0.651572  0.641138  0.697076  0.675419  0.760541   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "ch.9.98937537R  0.058422  0.071169  0.081995  0.064735  0.080378  0.072304   \n",
       "ch.9.98957343R  0.078774  0.084208  0.086902  0.094795  0.087245  0.061635   \n",
       "ch.9.98959675F  0.186146  0.176043  0.161922  0.191574  0.171343  0.380859   \n",
       "ch.9.98989607R  0.054130  0.049284  0.063500  0.051460  0.052651  0.062921   \n",
       "ch.9.991104F    0.143222  0.250320  0.145149  0.127603  0.141558  0.163660   \n",
       "\n",
       "                    6404      6405      6406  \n",
       "cg00000029      0.432730  0.528200  0.452750  \n",
       "cg00000108      0.921907  0.942789  0.908963  \n",
       "cg00000109      0.898407  0.872703  0.917163  \n",
       "cg00000165      0.243233  0.185050  0.203322  \n",
       "cg00000236      0.702650  0.662514  0.679006  \n",
       "...                  ...       ...       ...  \n",
       "ch.9.98937537R  0.059447  0.067503  0.079340  \n",
       "ch.9.98957343R  0.076514  0.075343  0.092468  \n",
       "ch.9.98959675F  0.229945  0.186564  0.210487  \n",
       "ch.9.98989607R  0.052991  0.050036  0.050322  \n",
       "ch.9.991104F    0.133061  0.129455  0.152954  \n",
       "\n",
       "[470417 rows x 6405 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_pivoted_samples_2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_df_combined2_clean.pickle', 'rb') as file:\n",
    "    results_df_combined2_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('combined_phenodata_clean.pickle', 'rb') as file:\n",
    "    combined_phenodata_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>Cell type</th>\n",
       "      <th>GEO series id</th>\n",
       "      <th>Tissue_origin</th>\n",
       "      <th>Age at collection months</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Disease state</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Tissue_original</th>\n",
       "      <th>Twin</th>\n",
       "      <th>Gender(1=m, 2=f)</th>\n",
       "      <th>dataset</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM989827</td>\n",
       "      <td>F</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>whole</td>\n",
       "      <td>GSE40279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM989828</td>\n",
       "      <td>F</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>whole</td>\n",
       "      <td>GSE40279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM989829</td>\n",
       "      <td>F</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>whole</td>\n",
       "      <td>GSE40279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM989830</td>\n",
       "      <td>F</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>whole</td>\n",
       "      <td>GSE40279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM989831</td>\n",
       "      <td>F</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>whole</td>\n",
       "      <td>GSE40279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>GSM1354401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>CD14+ cell</td>\n",
       "      <td>GSE56046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6403</th>\n",
       "      <td>GSM1354402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>CD14+ cell</td>\n",
       "      <td>GSE56046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>GSM1354403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>CD14+ cell</td>\n",
       "      <td>GSE56046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6405</th>\n",
       "      <td>GSM1354404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>CD14+ cell</td>\n",
       "      <td>GSE56046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6406</th>\n",
       "      <td>GSM1354405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peripheral blood</td>\n",
       "      <td>CD14+ cell</td>\n",
       "      <td>GSE56046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6405 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index Gender            Tissue   Cell type GEO series id  \\\n",
       "0      GSM989827      F  peripheral blood       whole      GSE40279   \n",
       "1      GSM989828      F  peripheral blood       whole      GSE40279   \n",
       "2      GSM989829      F  peripheral blood       whole      GSE40279   \n",
       "3      GSM989830      F  peripheral blood       whole      GSE40279   \n",
       "4      GSM989831      F  peripheral blood       whole      GSE40279   \n",
       "...          ...    ...               ...         ...           ...   \n",
       "6402  GSM1354401    NaN  peripheral blood  CD14+ cell      GSE56046   \n",
       "6403  GSM1354402    NaN  peripheral blood  CD14+ cell      GSE56046   \n",
       "6404  GSM1354403    NaN  peripheral blood  CD14+ cell      GSE56046   \n",
       "6405  GSM1354404    NaN  peripheral blood  CD14+ cell      GSE56046   \n",
       "6406  GSM1354405    NaN  peripheral blood  CD14+ cell      GSE56046   \n",
       "\n",
       "     Tissue_origin  Age at collection months Ethnicity Disease state Smoking  \\\n",
       "0              NaN                       NaN       NaN           NaN     NaN   \n",
       "1              NaN                       NaN       NaN           NaN     NaN   \n",
       "2              NaN                       NaN       NaN           NaN     NaN   \n",
       "3              NaN                       NaN       NaN           NaN     NaN   \n",
       "4              NaN                       NaN       NaN           NaN     NaN   \n",
       "...            ...                       ...       ...           ...     ...   \n",
       "6402           NaN                       NaN       NaN           NaN     NaN   \n",
       "6403           NaN                       NaN       NaN           NaN     NaN   \n",
       "6404           NaN                       NaN       NaN           NaN     NaN   \n",
       "6405           NaN                       NaN       NaN           NaN     NaN   \n",
       "6406           NaN                       NaN       NaN           NaN     NaN   \n",
       "\n",
       "     Tissue_original Twin Gender(1=m, 2=f) dataset   Age  \n",
       "0                NaN  NaN              NaN     NaN  67.0  \n",
       "1                NaN  NaN              NaN     NaN  89.0  \n",
       "2                NaN  NaN              NaN     NaN  66.0  \n",
       "3                NaN  NaN              NaN     NaN  64.0  \n",
       "4                NaN  NaN              NaN     NaN  62.0  \n",
       "...              ...  ...              ...     ...   ...  \n",
       "6402             NaN  NaN              NaN     NaN  56.0  \n",
       "6403             NaN  NaN              NaN     NaN  64.0  \n",
       "6404             NaN  NaN              NaN     NaN  55.0  \n",
       "6405             NaN  NaN              NaN     NaN  64.0  \n",
       "6406             NaN  NaN              NaN     NaN  56.0  \n",
       "\n",
       "[6405 rows x 15 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_phenodata_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('combined_pivoted_samples_2_horvath_clean.pickle', 'rb') as file:\n",
    "    combined_pivoted_samples_2_horvath_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load themi_results_df_combined2_clean object from the local file\n",
    "with open('mi_results_df_combined2_clean.pickle', 'rb') as file:\n",
    "    mi_results_df_combined2_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter expression data based on selected CpGs (by R-squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = results_df_combined2_clean['R-squared'] > 0.25\n",
    "extract = results_df_combined2_clean.loc[condition, ['Probe_ID', 'R-squared']]\n",
    "extract_probes_id = extract['Probe_ID'].tolist()\n",
    "filtered_pivoted_data = combined_pivoted_samples_2_clean.loc[extract_probes_id, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_probes_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter expression data based on selected CpGs (by MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Probe_ID  Mutual_Information\n",
      "300208  cg16867657            0.662971\n",
      "125211  cg06639320            0.633896\n",
      "455889  cg26961332            0.622407\n",
      "384535  cg22454769            0.599804\n",
      "133849  cg07082267            0.583828\n",
      "...            ...                 ...\n",
      "336407  cg19222366            0.000000\n",
      "196585  cg10703506            0.000000\n",
      "336342  cg19217463            0.000000\n",
      "375733  cg21829490            0.000000\n",
      "329907  cg18771570            0.000000\n",
      "\n",
      "[470417 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mi_results_df_combined2_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming mi_results_df_combined2_clean is your DataFrame containing 'Mutual_Information' column\n",
    "# Replace 'Mutual_Information' with the actual column name if it's different\n",
    "mutual_info_values = mi_results_df_combined2_clean['Mutual_Information']\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(mutual_info_values, bins=100, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mutual Information Values')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cg16867657',\n",
       " 'cg06639320',\n",
       " 'cg26961332',\n",
       " 'cg22454769',\n",
       " 'cg07082267',\n",
       " 'cg24668061',\n",
       " 'cg14213814',\n",
       " 'cg08262002',\n",
       " 'cg02787560',\n",
       " 'cg22796704',\n",
       " 'cg24079702',\n",
       " 'cg17342469',\n",
       " 'cg21644334',\n",
       " 'cg11985321',\n",
       " 'cg18215449',\n",
       " 'cg25834632',\n",
       " 'cg09186408',\n",
       " 'cg02152290',\n",
       " 'cg21572722',\n",
       " 'cg13826564',\n",
       " 'cg21333208',\n",
       " 'cg14692377',\n",
       " 'cg00876267',\n",
       " 'cg03548062',\n",
       " 'cg19283806',\n",
       " 'cg13740135',\n",
       " 'cg20273670',\n",
       " 'cg24716664',\n",
       " 'cg07575466',\n",
       " 'cg12079303',\n",
       " 'cg14076977',\n",
       " 'cg04265051',\n",
       " 'cg14377739',\n",
       " 'cg23076870',\n",
       " 'cg18630178',\n",
       " 'cg04875128',\n",
       " 'cg18397073',\n",
       " 'cg27048684',\n",
       " 'cg08005809',\n",
       " 'cg23956190',\n",
       " 'cg00439658',\n",
       " 'cg25440680',\n",
       " 'cg11693709',\n",
       " 'cg10778915',\n",
       " 'cg10501210',\n",
       " 'cg01176141',\n",
       " 'cg04208403',\n",
       " 'cg02323405',\n",
       " 'cg09310092',\n",
       " 'cg11649376',\n",
       " 'cg12934382',\n",
       " 'cg11862144',\n",
       " 'cg04742397',\n",
       " 'cg11552829',\n",
       " 'cg07547549',\n",
       " 'cg06493994',\n",
       " 'cg03339668',\n",
       " 'cg03032497',\n",
       " 'cg23500537',\n",
       " 'cg03841065',\n",
       " 'cg25574765',\n",
       " 'cg05881698',\n",
       " 'cg22824635',\n",
       " 'cg15417875',\n",
       " 'cg11436113',\n",
       " 'cg02636004',\n",
       " 'cg26685941',\n",
       " 'cg15022400',\n",
       " 'cg27000496',\n",
       " 'cg26969888',\n",
       " 'cg17471102',\n",
       " 'cg08160331',\n",
       " 'cg21524899',\n",
       " 'cg24724428',\n",
       " 'cg02026204',\n",
       " 'cg26290632',\n",
       " 'cg00481951',\n",
       " 'cg07211259',\n",
       " 'cg22736354',\n",
       " 'cg22242842',\n",
       " 'cg19968916',\n",
       " 'cg16400350',\n",
       " 'cg11970349',\n",
       " 'cg05242402',\n",
       " 'cg07553761',\n",
       " 'cg16290275',\n",
       " 'cg05895618',\n",
       " 'cg16762684',\n",
       " 'cg16193278',\n",
       " 'cg14583999',\n",
       " 'cg16628641',\n",
       " 'cg02634861',\n",
       " 'cg01922485',\n",
       " 'cg26921969',\n",
       " 'cg22156456',\n",
       " 'cg05167561',\n",
       " 'cg04921315',\n",
       " 'cg14556683',\n",
       " 'cg04484550',\n",
       " 'cg27620176',\n",
       " 'cg00132509',\n",
       " 'cg23078123',\n",
       " 'cg05258935',\n",
       " 'cg09036468',\n",
       " 'cg01974375',\n",
       " 'cg02527886',\n",
       " 'cg06753281',\n",
       " 'cg17705066',\n",
       " 'cg26758486',\n",
       " 'cg00487989',\n",
       " 'cg24180219',\n",
       " 'cg18618815',\n",
       " 'cg14116056',\n",
       " 'cg20222376',\n",
       " 'cg05721773',\n",
       " 'cg20781880',\n",
       " 'cg14674720',\n",
       " 'cg19270739',\n",
       " 'cg00880290',\n",
       " 'cg17457912',\n",
       " 'cg05991442',\n",
       " 'cg13241681',\n",
       " 'cg25259564',\n",
       " 'cg26978172',\n",
       " 'cg11252765',\n",
       " 'cg22483030',\n",
       " 'cg14790739',\n",
       " 'cg17621438',\n",
       " 'cg13912307',\n",
       " 'cg26290219',\n",
       " 'cg09184899',\n",
       " 'cg12634306',\n",
       " 'cg24853724',\n",
       " 'cg10003032',\n",
       " 'cg26427109',\n",
       " 'cg24028809',\n",
       " 'cg18031326',\n",
       " 'cg25998745',\n",
       " 'cg00906812',\n",
       " 'cg19344626',\n",
       " 'cg01910639',\n",
       " 'cg05991454',\n",
       " 'cg25371036',\n",
       " 'cg22938135',\n",
       " 'cg24921221',\n",
       " 'cg09007841',\n",
       " 'cg19486070',\n",
       " 'cg04737046',\n",
       " 'cg05305327',\n",
       " 'cg05923226',\n",
       " 'cg13929658',\n",
       " 'cg22155724',\n",
       " 'cg01647936',\n",
       " 'cg14361627',\n",
       " 'cg11436025',\n",
       " 'cg01820374',\n",
       " 'cg11751101',\n",
       " 'cg01765930',\n",
       " 'cg02734358',\n",
       " 'cg01927162',\n",
       " 'cg05405914',\n",
       " 'cg13331354',\n",
       " 'cg08476244',\n",
       " 'cg07850154',\n",
       " 'cg24848615',\n",
       " 'cg17345741',\n",
       " 'cg24586870',\n",
       " 'cg03735592',\n",
       " 'cg22379463',\n",
       " 'cg24892069',\n",
       " 'cg05191655',\n",
       " 'cg13420364',\n",
       " 'cg05619598',\n",
       " 'cg10628205',\n",
       " 'cg14210311',\n",
       " 'cg19220719',\n",
       " 'cg12318408',\n",
       " 'cg23201812',\n",
       " 'cg19784428',\n",
       " 'cg13226232',\n",
       " 'cg23320649',\n",
       " 'cg09442613',\n",
       " 'cg07481320',\n",
       " 'cg00791074',\n",
       " 'cg13786236',\n",
       " 'cg05404236',\n",
       " 'cg22796593',\n",
       " 'cg24531955',\n",
       " 'cg26350754',\n",
       " 'cg02893429',\n",
       " 'cg10503298',\n",
       " 'cg11807280',\n",
       " 'cg07532183',\n",
       " 'cg02053964',\n",
       " 'cg20691436',\n",
       " 'cg01973676',\n",
       " 'cg08622675',\n",
       " 'cg20249566',\n",
       " 'cg11806439',\n",
       " 'cg02488934',\n",
       " 'cg03150042',\n",
       " 'cg20822990',\n",
       " 'cg20651995',\n",
       " 'cg20979979',\n",
       " 'cg21629821',\n",
       " 'cg25474442',\n",
       " 'cg23401796',\n",
       " 'cg18581405',\n",
       " 'cg01089914',\n",
       " 'cg02286081',\n",
       " 'cg21585640',\n",
       " 'cg20559422',\n",
       " 'cg15041373',\n",
       " 'cg24980572',\n",
       " 'cg16878214',\n",
       " 'cg25666403',\n",
       " 'cg05412028',\n",
       " 'cg09464466',\n",
       " 'cg01916088',\n",
       " 'cg07757611',\n",
       " 'cg02352281',\n",
       " 'cg22273555',\n",
       " 'cg17911539',\n",
       " 'cg02666566',\n",
       " 'cg26325867',\n",
       " 'cg09278098',\n",
       " 'cg17372101',\n",
       " 'cg24600221',\n",
       " 'cg04295144',\n",
       " 'cg10476085',\n",
       " 'cg21586223',\n",
       " 'cg08488615',\n",
       " 'cg01289541',\n",
       " 'cg05014837',\n",
       " 'cg03477240',\n",
       " 'cg03043157',\n",
       " 'cg09486764',\n",
       " 'cg06682024',\n",
       " 'cg15538427',\n",
       " 'cg08379738',\n",
       " 'cg03377767',\n",
       " 'cg20446334',\n",
       " 'cg10979567',\n",
       " 'cg21273275',\n",
       " 'cg17885226',\n",
       " 'cg15911153',\n",
       " 'cg20752683',\n",
       " 'cg02091781',\n",
       " 'cg13953735',\n",
       " 'cg23543318',\n",
       " 'cg04490178',\n",
       " 'cg07416237',\n",
       " 'cg04453050',\n",
       " 'cg00150520',\n",
       " 'cg11084334',\n",
       " 'cg19673155',\n",
       " 'cg14819242',\n",
       " 'cg23015118',\n",
       " 'cg18097277',\n",
       " 'cg19722847',\n",
       " 'cg23124451',\n",
       " 'cg07502389',\n",
       " 'cg22737154',\n",
       " 'cg14052728',\n",
       " 'cg04928049',\n",
       " 'cg25568114',\n",
       " 'cg00664406',\n",
       " 'cg22133973',\n",
       " 'cg00869989',\n",
       " 'cg19612068',\n",
       " 'cg24403644',\n",
       " 'cg00748589',\n",
       " 'cg22016779',\n",
       " 'cg18356785',\n",
       " 'cg13448828',\n",
       " 'cg27019093',\n",
       " 'cg05690644',\n",
       " 'cg14216068',\n",
       " 'cg10221746',\n",
       " 'cg13385220',\n",
       " 'cg06511312',\n",
       " 'cg06862374',\n",
       " 'cg08128734',\n",
       " 'cg18064714',\n",
       " 'cg14189808',\n",
       " 'cg06419432',\n",
       " 'cg16069986',\n",
       " 'cg25039325',\n",
       " 'cg22978940',\n",
       " 'cg05584950',\n",
       " 'cg03572680',\n",
       " 'cg21526357',\n",
       " 'cg21922223',\n",
       " 'cg20383948',\n",
       " 'cg19377250',\n",
       " 'cg22031783',\n",
       " 'cg06787669',\n",
       " 'cg07705835',\n",
       " 'cg17593472',\n",
       " 'cg10288525',\n",
       " 'cg20847580',\n",
       " 'cg05804846',\n",
       " 'cg23718736',\n",
       " 'cg17183905',\n",
       " 'cg27151122',\n",
       " 'cg15893929',\n",
       " 'cg04707348',\n",
       " 'cg00602811',\n",
       " 'cg19510206',\n",
       " 'cg15732530',\n",
       " 'cg20811857',\n",
       " 'cg16628135',\n",
       " 'cg04915566',\n",
       " 'cg26349957',\n",
       " 'cg07598052',\n",
       " 'cg25504605',\n",
       " 'cg20591472',\n",
       " 'cg04934595',\n",
       " 'cg18150280',\n",
       " 'cg26124968',\n",
       " 'cg16863795',\n",
       " 'cg22299097',\n",
       " 'cg07871633',\n",
       " 'cg24384437',\n",
       " 'cg04812351',\n",
       " 'cg12601118',\n",
       " 'cg03595018',\n",
       " 'cg04959790',\n",
       " 'cg00503840',\n",
       " 'cg12720152',\n",
       " 'cg00303541',\n",
       " 'cg05168229',\n",
       " 'cg12472603',\n",
       " 'cg07425646',\n",
       " 'cg12878812',\n",
       " 'cg23210109',\n",
       " 'cg03484180',\n",
       " 'cg08442149',\n",
       " 'cg21527370',\n",
       " 'cg22483095',\n",
       " 'cg10616795',\n",
       " 'cg14196395',\n",
       " 'cg18108818',\n",
       " 'cg03337218',\n",
       " 'cg04989070',\n",
       " 'cg05890550',\n",
       " 'cg15480367',\n",
       " 'cg22331159',\n",
       " 'cg22957898',\n",
       " 'cg01812894',\n",
       " 'cg04833731',\n",
       " 'cg15865026',\n",
       " 'cg23512701',\n",
       " 'cg02004723',\n",
       " 'cg17947992',\n",
       " 'cg11349594',\n",
       " 'cg00548824',\n",
       " 'cg27436259',\n",
       " 'cg11287077',\n",
       " 'cg16312514',\n",
       " 'cg11108115',\n",
       " 'cg10587082',\n",
       " 'cg21208518',\n",
       " 'cg04424885',\n",
       " 'cg20685713',\n",
       " 'cg21541341',\n",
       " 'cg09038267',\n",
       " 'cg03473532',\n",
       " 'cg09183450',\n",
       " 'cg21660452',\n",
       " 'cg10009968',\n",
       " 'cg14601621',\n",
       " 'cg01570480',\n",
       " 'cg11441533',\n",
       " 'cg05257202',\n",
       " 'cg00795584',\n",
       " 'cg14245471',\n",
       " 'cg11255590',\n",
       " 'cg23538064',\n",
       " 'cg18407309',\n",
       " 'cg03131366',\n",
       " 'cg16026647',\n",
       " 'cg19761273',\n",
       " 'cg01554474',\n",
       " 'cg16864063',\n",
       " 'cg12899747',\n",
       " 'cg24940583',\n",
       " 'cg01486610',\n",
       " 'cg04090392',\n",
       " 'cg20515136',\n",
       " 'cg02361903',\n",
       " 'cg10160612',\n",
       " 'cg21804531',\n",
       " 'cg09786329',\n",
       " 'cg10703481',\n",
       " 'cg15845821',\n",
       " 'cg18274619',\n",
       " 'cg19735250',\n",
       " 'cg11256132',\n",
       " 'cg20495738',\n",
       " 'cg05982473',\n",
       " 'cg11754374',\n",
       " 'cg08472222',\n",
       " 'cg27401724',\n",
       " 'cg14746276',\n",
       " 'cg18647570',\n",
       " 'cg24711336',\n",
       " 'cg07366503',\n",
       " 'cg02597644',\n",
       " 'cg26232412',\n",
       " 'cg10586317',\n",
       " 'cg12586707',\n",
       " 'cg02861541',\n",
       " 'cg12009872',\n",
       " 'cg20937934',\n",
       " 'cg27627006',\n",
       " 'cg00050692',\n",
       " 'cg01510696',\n",
       " 'cg15557036',\n",
       " 'cg21198455',\n",
       " 'cg23939182',\n",
       " 'cg12990614',\n",
       " 'cg24520538',\n",
       " 'cg02351277',\n",
       " 'cg24099956',\n",
       " 'cg04904318',\n",
       " 'cg13138089',\n",
       " 'cg10673833',\n",
       " 'cg27292099',\n",
       " 'cg00108715',\n",
       " 'cg20910746',\n",
       " 'cg07759042',\n",
       " 'cg25580581',\n",
       " 'cg20669012',\n",
       " 'cg13806741',\n",
       " 'cg10927841',\n",
       " 'cg23973429',\n",
       " 'cg11424674',\n",
       " 'cg13605327',\n",
       " 'cg06698795',\n",
       " 'cg06335143',\n",
       " 'cg08059678',\n",
       " 'cg14257429',\n",
       " 'cg25693132',\n",
       " 'cg23606718',\n",
       " 'cg09146892',\n",
       " 'cg18952506',\n",
       " 'cg10511890',\n",
       " 'cg06708720',\n",
       " 'cg00872683',\n",
       " 'cg01506917',\n",
       " 'cg25478614',\n",
       " 'cg16554164',\n",
       " 'cg18680977',\n",
       " 'cg10982443',\n",
       " 'cg21322248',\n",
       " 'cg10424974',\n",
       " 'cg01955153',\n",
       " 'cg21023001',\n",
       " 'cg25380464',\n",
       " 'cg06636244',\n",
       " 'cg02753354',\n",
       " 'cg13959344',\n",
       " 'cg12582330',\n",
       " 'cg01383581',\n",
       " 'cg15341124',\n",
       " 'cg13496041',\n",
       " 'cg12494545',\n",
       " 'cg18794404',\n",
       " 'cg08097290',\n",
       " 'cg17110586',\n",
       " 'cg11204212',\n",
       " 'cg12667732',\n",
       " 'cg11960435',\n",
       " 'cg16744741',\n",
       " 'cg14836555',\n",
       " 'cg26125625',\n",
       " 'cg27437510',\n",
       " 'cg23311800',\n",
       " 'cg27147556',\n",
       " 'cg00174718',\n",
       " 'cg07549381',\n",
       " 'cg27409991',\n",
       " 'cg17951244',\n",
       " 'cg00933696',\n",
       " 'cg09897970',\n",
       " 'cg08431718',\n",
       " 'cg21325715',\n",
       " 'cg06044751',\n",
       " 'cg08729279',\n",
       " 'cg06460691',\n",
       " 'cg17095171',\n",
       " 'cg16911349',\n",
       " 'cg01514538',\n",
       " 'cg03282312',\n",
       " 'cg12446246',\n",
       " 'cg03747456',\n",
       " 'cg22907985',\n",
       " 'cg00636737',\n",
       " 'cg18797590',\n",
       " 'cg13377102',\n",
       " 'cg09258479',\n",
       " 'cg12081303',\n",
       " 'cg08213607',\n",
       " 'cg23491743',\n",
       " 'cg03214130',\n",
       " 'cg00829600',\n",
       " 'cg15428620',\n",
       " 'cg15704699',\n",
       " 'cg21406967',\n",
       " 'cg04250451',\n",
       " 'cg17518965',\n",
       " 'cg05959508',\n",
       " 'cg26030804',\n",
       " 'cg18442362',\n",
       " 'cg10574566',\n",
       " 'cg18333339',\n",
       " 'cg03925072',\n",
       " 'cg11208165',\n",
       " 'cg12597839',\n",
       " 'cg24736933',\n",
       " 'cg01721454',\n",
       " 'cg18113734',\n",
       " 'cg02402091',\n",
       " 'cg20747538',\n",
       " 'cg20956407',\n",
       " 'cg14294658',\n",
       " 'cg23762657',\n",
       " 'cg06854772',\n",
       " 'cg17698295',\n",
       " 'cg24436906',\n",
       " 'cg22006392',\n",
       " 'cg19711602',\n",
       " 'cg14730524',\n",
       " 'cg09037630',\n",
       " 'cg09418000',\n",
       " 'cg14476745',\n",
       " 'cg18006637',\n",
       " 'cg12302987',\n",
       " 'cg12182580',\n",
       " 'cg10001186',\n",
       " 'cg19453093',\n",
       " 'cg26126740',\n",
       " 'cg04956949',\n",
       " 'cg25994988',\n",
       " 'cg02433979',\n",
       " 'cg13159929',\n",
       " 'cg16362181',\n",
       " 'cg13273136',\n",
       " 'cg11447922',\n",
       " 'cg13319938',\n",
       " 'cg16385335',\n",
       " 'cg02552250',\n",
       " 'cg16983588',\n",
       " 'cg14324838',\n",
       " 'cg01902066',\n",
       " 'cg17631451',\n",
       " 'cg04766061',\n",
       " 'cg10589330',\n",
       " 'cg23696432',\n",
       " 'cg22148297',\n",
       " 'cg03180302',\n",
       " 'cg01824933',\n",
       " 'cg20267101',\n",
       " 'cg00448875',\n",
       " 'cg03684062',\n",
       " 'cg00777636',\n",
       " 'cg18993949',\n",
       " 'cg26058289',\n",
       " 'cg18002126',\n",
       " 'cg05141014',\n",
       " 'cg01282174',\n",
       " 'cg25373595',\n",
       " 'cg17543112',\n",
       " 'cg16175245',\n",
       " 'cg20293650',\n",
       " 'cg00387658',\n",
       " 'cg21037265',\n",
       " 'cg02558120',\n",
       " 'cg01962821',\n",
       " 'cg25834768',\n",
       " 'cg05607079',\n",
       " 'cg23899450',\n",
       " 'cg04798016',\n",
       " 'cg04600297',\n",
       " 'cg03891318',\n",
       " 'cg05241828',\n",
       " 'cg02273889',\n",
       " 'cg25128781',\n",
       " 'cg04193015',\n",
       " 'cg16618104',\n",
       " 'cg16008966',\n",
       " 'cg22797182',\n",
       " 'cg16728604',\n",
       " 'cg27160537',\n",
       " 'cg17778888',\n",
       " 'cg23953820',\n",
       " 'cg15627628',\n",
       " 'cg10402417',\n",
       " 'cg10219789',\n",
       " 'cg14284824',\n",
       " 'cg14206056',\n",
       " 'cg22280238',\n",
       " 'cg09911316',\n",
       " 'cg03070534',\n",
       " 'cg19668951',\n",
       " 'cg18506744',\n",
       " 'cg05492306',\n",
       " 'cg25468516',\n",
       " 'cg08586426',\n",
       " 'cg25424279',\n",
       " 'cg13875012',\n",
       " 'cg24002183',\n",
       " 'cg07800544',\n",
       " 'cg17376853',\n",
       " 'cg09354037',\n",
       " 'cg16882373',\n",
       " 'cg19405484',\n",
       " 'cg01951274',\n",
       " 'cg07278047',\n",
       " 'cg26198107',\n",
       " 'cg07785717',\n",
       " 'cg20237820',\n",
       " 'cg05796838',\n",
       " 'cg00476900',\n",
       " 'cg10025514',\n",
       " 'cg26365847',\n",
       " 'cg05457684',\n",
       " 'cg00373148',\n",
       " 'cg03707168',\n",
       " 'cg00513941',\n",
       " 'cg11834106',\n",
       " 'cg25139229',\n",
       " 'cg11011533',\n",
       " 'cg07715777',\n",
       " 'cg19752861',\n",
       " 'cg07499032',\n",
       " 'cg13026729',\n",
       " 'cg08271909',\n",
       " 'cg10340117',\n",
       " 'cg24497819',\n",
       " 'cg02088996',\n",
       " 'cg12583369',\n",
       " 'cg22585786',\n",
       " 'cg22259698',\n",
       " 'cg22317846',\n",
       " 'cg00922378',\n",
       " 'cg08090640',\n",
       " 'cg05725703',\n",
       " 'cg19729744',\n",
       " 'cg12865818',\n",
       " 'cg14113739',\n",
       " 'cg16492341',\n",
       " 'cg10504000',\n",
       " 'cg18473521',\n",
       " 'cg03752138',\n",
       " 'cg06350432',\n",
       " 'cg07234388',\n",
       " 'cg27225068',\n",
       " 'cg07546334',\n",
       " 'cg23004031',\n",
       " 'cg10699884',\n",
       " 'cg10020787',\n",
       " 'cg16066219',\n",
       " 'cg07267600',\n",
       " 'cg27261597',\n",
       " 'cg27332104',\n",
       " 'cg26628907',\n",
       " 'cg00216804',\n",
       " 'cg06048102',\n",
       " 'cg20816447',\n",
       " 'cg17808634',\n",
       " 'cg00448560',\n",
       " 'cg18972123',\n",
       " 'cg26401673',\n",
       " 'cg06555468',\n",
       " 'cg05027713',\n",
       " 'cg11907323',\n",
       " 'cg20215257',\n",
       " 'cg06865913',\n",
       " 'cg08409562',\n",
       " 'cg09565597',\n",
       " 'cg26841040',\n",
       " 'cg13072940',\n",
       " 'cg23684410',\n",
       " 'cg01616967',\n",
       " 'cg15121420',\n",
       " 'cg05696950',\n",
       " 'cg05647756',\n",
       " 'cg19926250',\n",
       " 'cg12412575',\n",
       " 'cg01986767',\n",
       " 'cg25143508',\n",
       " 'cg16677191',\n",
       " 'cg17969902',\n",
       " 'cg00514486',\n",
       " 'cg25632577',\n",
       " 'cg23429794',\n",
       " 'cg20584157',\n",
       " 'cg02046143',\n",
       " 'cg22512670',\n",
       " 'cg20695297',\n",
       " 'cg09555124',\n",
       " 'cg19223190',\n",
       " 'cg01448132',\n",
       " 'cg26394916',\n",
       " 'cg22947000',\n",
       " 'cg18205668',\n",
       " 'cg01144086',\n",
       " 'cg24974704',\n",
       " 'cg03320754',\n",
       " 'cg08097417',\n",
       " 'cg17279079',\n",
       " 'cg22436753',\n",
       " 'cg00422714',\n",
       " 'cg21832906',\n",
       " 'cg02849507',\n",
       " 'cg05331472',\n",
       " 'cg00064255',\n",
       " 'cg18826637',\n",
       " 'cg19593255',\n",
       " 'cg18458509',\n",
       " 'cg20520804',\n",
       " 'cg05724065',\n",
       " 'cg16413428',\n",
       " 'cg23508887',\n",
       " 'cg06185532',\n",
       " 'cg24583987',\n",
       " 'cg05973084',\n",
       " 'cg06492796',\n",
       " 'cg00481216',\n",
       " 'cg18241469',\n",
       " 'cg08825225',\n",
       " 'cg02812891',\n",
       " 'cg15070677',\n",
       " 'cg02100008',\n",
       " 'cg21502482',\n",
       " 'cg05463027',\n",
       " 'cg13918433',\n",
       " 'cg14609289',\n",
       " 'cg25827670',\n",
       " 'cg01437204',\n",
       " 'cg04466898',\n",
       " 'cg16144864',\n",
       " 'cg21244580',\n",
       " 'cg09174502',\n",
       " 'cg03236802',\n",
       " 'cg14343652',\n",
       " 'cg02597698',\n",
       " 'cg00415011',\n",
       " 'cg25840433',\n",
       " 'cg06081238',\n",
       " 'cg03654273',\n",
       " 'cg02057013',\n",
       " 'cg02543462',\n",
       " 'cg22105162',\n",
       " 'cg18136895',\n",
       " 'cg08305942',\n",
       " 'cg09097632',\n",
       " 'cg24679890',\n",
       " 'cg03762237',\n",
       " 'cg01049678',\n",
       " 'cg27125093',\n",
       " 'cg00011616',\n",
       " 'cg06797069',\n",
       " 'cg10752421',\n",
       " 'cg08895618',\n",
       " 'cg07686441',\n",
       " 'cg06340367',\n",
       " 'cg13357602',\n",
       " 'cg08101036',\n",
       " 'cg07184988',\n",
       " 'cg16734845',\n",
       " 'cg09389824',\n",
       " 'cg24107163',\n",
       " 'cg19043450',\n",
       " 'cg07508341',\n",
       " 'cg11468953',\n",
       " 'cg00062437',\n",
       " 'cg01549977',\n",
       " 'cg06080026',\n",
       " 'cg16419235',\n",
       " 'cg09999700',\n",
       " 'cg10521403',\n",
       " 'cg01604401',\n",
       " 'cg12563935',\n",
       " 'cg06926815',\n",
       " 'cg01811583',\n",
       " 'cg06520081',\n",
       " 'cg11075561',\n",
       " 'cg21899500',\n",
       " 'cg15914316',\n",
       " 'cg21979559',\n",
       " 'cg02305961',\n",
       " 'cg07498879',\n",
       " 'cg17696044',\n",
       " 'cg26787199',\n",
       " 'cg15034393',\n",
       " 'cg05622577',\n",
       " 'cg20477147',\n",
       " 'cg06733155',\n",
       " 'cg27653134',\n",
       " 'cg06457633',\n",
       " 'cg25268718',\n",
       " 'cg22820188',\n",
       " 'cg00329615',\n",
       " 'cg09412707',\n",
       " 'cg13774505',\n",
       " 'cg10197238',\n",
       " 'cg17666418',\n",
       " 'cg11465372',\n",
       " 'cg09287864',\n",
       " 'cg25850629',\n",
       " 'cg12142865',\n",
       " 'cg13021857',\n",
       " 'cg06822067',\n",
       " 'cg14288086',\n",
       " 'cg19412669',\n",
       " 'cg01538166',\n",
       " 'cg09432775',\n",
       " 'cg03804083',\n",
       " 'cg00921266',\n",
       " 'cg00686880',\n",
       " 'cg16373526',\n",
       " 'cg15844596',\n",
       " 'cg11600161',\n",
       " 'cg07593390',\n",
       " 'cg18817529',\n",
       " 'cg09652652',\n",
       " 'cg13487445',\n",
       " 'cg09528501',\n",
       " 'cg13591783',\n",
       " 'cg22980156',\n",
       " 'cg20988565',\n",
       " 'cg19750321',\n",
       " 'cg13120986',\n",
       " 'cg04833819',\n",
       " 'cg11836829',\n",
       " 'cg06745001',\n",
       " 'cg07345108',\n",
       " 'cg08558886',\n",
       " 'cg11684826',\n",
       " 'cg01606773',\n",
       " 'cg07214270',\n",
       " 'cg24024661',\n",
       " 'cg00912772',\n",
       " 'cg02010481',\n",
       " 'cg10405075',\n",
       " 'cg24990494',\n",
       " 'cg26140475',\n",
       " 'cg25455865',\n",
       " 'cg05697539',\n",
       " 'cg14011229',\n",
       " 'cg02831955',\n",
       " 'cg14898223',\n",
       " 'cg02091951',\n",
       " 'cg03593259',\n",
       " 'cg22168796',\n",
       " 'cg05904901',\n",
       " 'cg01844642',\n",
       " 'cg14776033',\n",
       " 'cg14704980',\n",
       " 'cg09397557',\n",
       " 'cg09844573',\n",
       " 'cg06772221',\n",
       " 'cg24643158',\n",
       " 'cg16001165',\n",
       " 'cg26501007',\n",
       " 'cg15768226',\n",
       " 'cg10457436',\n",
       " 'cg18736448',\n",
       " 'cg09977852',\n",
       " 'cg01426968',\n",
       " 'cg25210134',\n",
       " 'cg01117339',\n",
       " 'cg04897892',\n",
       " 'cg19835796',\n",
       " 'cg01068601',\n",
       " 'cg25620220',\n",
       " 'cg00029246',\n",
       " 'cg23076913',\n",
       " 'cg14773523',\n",
       " 'cg17804348',\n",
       " 'cg20408776',\n",
       " 'cg13572771',\n",
       " 'cg26247508',\n",
       " 'cg22216196',\n",
       " 'cg00842595',\n",
       " 'cg18621299',\n",
       " 'cg13488570',\n",
       " 'cg02886375',\n",
       " 'cg13278478',\n",
       " 'cg11142333',\n",
       " 'cg09624551',\n",
       " 'cg25410668',\n",
       " 'cg19578183',\n",
       " 'cg02939659',\n",
       " 'cg26122963',\n",
       " 'cg05378609',\n",
       " 'cg21004490',\n",
       " 'cg16581738',\n",
       " 'cg18739675',\n",
       " 'cg12686055',\n",
       " 'cg02697979',\n",
       " 'cg11076306',\n",
       " 'cg25341313',\n",
       " 'cg22962123',\n",
       " 'cg05238713',\n",
       " 'cg14538319',\n",
       " 'cg06860460',\n",
       " 'cg00423597',\n",
       " 'cg18472912',\n",
       " 'cg13379757',\n",
       " 'cg02641539',\n",
       " 'cg26294229',\n",
       " 'cg07583137',\n",
       " 'cg03649649',\n",
       " 'cg20868817',\n",
       " 'cg11034122',\n",
       " 'cg27149179',\n",
       " 'cg05207048',\n",
       " 'cg24990400',\n",
       " 'cg12554573',\n",
       " 'cg25467652',\n",
       " 'cg11693019',\n",
       " 'cg13758724',\n",
       " 'cg19648552',\n",
       " 'cg26918756',\n",
       " 'cg02167021',\n",
       " 'cg07259651',\n",
       " 'cg04173864',\n",
       " 'cg06911753',\n",
       " 'cg17552357',\n",
       " 'cg02047547',\n",
       " 'cg26927232',\n",
       " 'cg19753867',\n",
       " 'cg26682517',\n",
       " 'cg08072862',\n",
       " 'cg08310088',\n",
       " 'cg19677259',\n",
       " 'cg12408494',\n",
       " 'cg22675660',\n",
       " 'cg11847992',\n",
       " 'cg21472517',\n",
       " 'cg06752054',\n",
       " 'cg06163904',\n",
       " 'cg12939283',\n",
       " 'cg17214023',\n",
       " 'cg13135455',\n",
       " 'cg04761746',\n",
       " 'cg24948189',\n",
       " 'cg10940914',\n",
       " 'cg05323251',\n",
       " 'cg17067993',\n",
       " 'cg19414711',\n",
       " 'cg20773033',\n",
       " 'cg16219636',\n",
       " 'cg10418289',\n",
       " 'cg08877357',\n",
       " 'cg08469255',\n",
       " 'cg26940109',\n",
       " 'cg02267488',\n",
       " 'cg13996522',\n",
       " 'cg26161816',\n",
       " 'cg22193385',\n",
       " 'cg16749930',\n",
       " 'cg20583930',\n",
       " 'cg27641628',\n",
       " 'cg13327545',\n",
       " 'cg19361181',\n",
       " 'cg07167423',\n",
       " 'cg17893857',\n",
       " 'cg07851675',\n",
       " 'cg14199187',\n",
       " 'cg08553327',\n",
       " 'cg06534752',\n",
       " 'cg23327334',\n",
       " 'cg27096032',\n",
       " 'cg17802213',\n",
       " 'cg01649611',\n",
       " 'cg21646392',\n",
       " 'cg00862408',\n",
       " 'cg02352181',\n",
       " 'cg23675587',\n",
       " 'cg25973354',\n",
       " 'cg01710361',\n",
       " 'cg00533407',\n",
       " 'cg20872072',\n",
       " 'cg07206676',\n",
       " 'cg20303441',\n",
       " 'cg02481714',\n",
       " 'cg22544881',\n",
       " 'cg17958516',\n",
       " 'cg18643029',\n",
       " 'cg06879394',\n",
       " 'cg00489401',\n",
       " 'cg07972322',\n",
       " 'cg03125531',\n",
       " 'cg21293216',\n",
       " 'cg21733794',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_results_df_combined2_clean.iloc[:len(extract_probes_id)]['Probe_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_extract_probes_id = mi_results_df_combined2_clean.iloc[:len(extract_probes_id)]['Probe_ID'].tolist()\n",
    "MI_filtered_pivoted_data = combined_pivoted_samples_2_clean.loc[MI_extract_probes_id, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0         1         2         3         4         5     \\\n",
      "cg16867657  0.651194  0.751090  0.672077  0.684481  0.664573  0.828496   \n",
      "cg06639320  0.467484  0.554684  0.497623  0.495958  0.474316  0.540942   \n",
      "cg26961332  0.996601  0.996633  0.992308  0.996558  0.995753  0.997042   \n",
      "cg22454769  0.563600  0.704345  0.605543  0.628087  0.560501  0.659044   \n",
      "cg07082267  0.401409  0.328262  0.384356  0.363681  0.379792  0.419584   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "cg21424703  0.076983  0.062618  0.064094  0.062202  0.051199  0.074657   \n",
      "cg07004075  0.820347  0.816791  0.792106  0.850124  0.852089  0.808040   \n",
      "cg08198075  0.761500  0.819816  0.786384  0.776370  0.763615  0.699602   \n",
      "cg21202240  0.858567  0.852144  0.829673  0.836315  0.846675  0.842176   \n",
      "cg12174175  0.875933  0.886099  0.884499  0.863985  0.882773  0.871162   \n",
      "\n",
      "                6         7         8         9     ...      6397      6398  \\\n",
      "cg16867657  0.715064  0.731963  0.731545  0.752308  ...  0.700381  0.741743   \n",
      "cg06639320  0.507918  0.522080  0.510266  0.524577  ...  0.541599  0.588101   \n",
      "cg26961332  0.994185  0.995170  0.996492  0.996549  ...  0.972882  0.982072   \n",
      "cg22454769  0.655298  0.660151  0.661393  0.634837  ...  0.592214  0.763054   \n",
      "cg07082267  0.350639  0.239659  0.371205  0.326528  ...  0.349785  0.334989   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "cg21424703  0.063058  0.062922  0.064493  0.065528  ...  0.084288  0.093491   \n",
      "cg07004075  0.840504  0.822384  0.805209  0.766450  ...  0.883612  0.884406   \n",
      "cg08198075  0.758711  0.751219  0.784207  0.734806  ...  0.864894  0.875521   \n",
      "cg21202240  0.845087  0.849431  0.862498  0.844770  ...  0.888613  0.897890   \n",
      "cg12174175  0.859179  0.865867  0.867078  0.858066  ...  0.891969  0.882924   \n",
      "\n",
      "                6399      6400      6401      6402      6403      6404  \\\n",
      "cg16867657  0.689321  0.604815  0.617199  0.729721  0.675615  0.685442   \n",
      "cg06639320  0.463245  0.491557  0.537190  0.528059  0.511967  0.512265   \n",
      "cg26961332  0.970583  0.980310  0.977111  0.981878  0.976957  0.966437   \n",
      "cg22454769  0.627670  0.554844  0.607208  0.622724  0.693744  0.632943   \n",
      "cg07082267  0.384509  0.411236  0.404472  0.393585  0.408813  0.429037   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "cg21424703  0.111601  0.088501  0.082382  0.098380  0.103182  0.102999   \n",
      "cg07004075  0.876951  0.872859  0.886932  0.860166  0.857483  0.891094   \n",
      "cg08198075  0.847030  0.863020  0.876044  0.849805  0.868476  0.855357   \n",
      "cg21202240  0.881239  0.893433  0.897572  0.902700  0.882824  0.889205   \n",
      "cg12174175  0.534723  0.884483  0.909837  0.870914  0.909313  0.885033   \n",
      "\n",
      "                6405      6406  \n",
      "cg16867657  0.723078  0.689217  \n",
      "cg06639320  0.503297  0.515808  \n",
      "cg26961332  0.985414  0.972038  \n",
      "cg22454769  0.649043  0.621872  \n",
      "cg07082267  0.361530  0.394048  \n",
      "...              ...       ...  \n",
      "cg21424703  0.086248  0.094855  \n",
      "cg07004075  0.876309  0.876506  \n",
      "cg08198075  0.862355  0.863689  \n",
      "cg21202240  0.903974  0.873554  \n",
      "cg12174175  0.894393  0.892721  \n",
      "\n",
      "[4320 rows x 6405 columns]\n"
     ]
    }
   ],
   "source": [
    "print(MI_filtered_pivoted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save MI_filtered_pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('MI_filtered_pivoted_data.pickle', 'wb') as file:\n",
    "    pickle.dump(MI_filtered_pivoted_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn of MI_ids and extract_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFjCAYAAACgxwiQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQc0lEQVR4nO3dd3wUdf4/8Nds3+ym94SEJISOoaOASFeqJ4p+xQaC6FnOuxPvLIeCp5xiOVFO5XenwHmIBQ+RLiKChSCg9CKEGgLppGez2d3P74+9LCwhkMAmszP7ej4eeQCT2d33DpnZVz7zKZIQQoCIiIgClkbuAoiIiEheDANEREQBjmGAiIgowDEMEBERBTiGASIiogDHMEBERBTgGAaIiIgCHMMAERFRgGMYICIiCnAMA9Ro48aNg9lsRklJSYP73H333dDr9cjLy2u5whpp0KBBkCQJkiRBo9EgODgY6enpuP322/H555/D5XLVe0xKSgomTZrU8sUq0PTp05GcnAydToewsLAG95s5c6bn/0GSJBgMBqSmpuL3v//9JX+2mur48eOQJAmvv/66z56zMRYuXAhJki67X91xKCws9GybNGmS17GxWCxISUnBzTffjAULFqCmpqY5S6cAppO7AFKOKVOmYNmyZVi8eDEeeeSRet8vLS3FF198gTFjxiA2NlaGCi8vLS0NH330EQCgsrISx44dw7Jly3D77bdjwIABWLFiBUJDQz37f/HFFwgJCZGrXMX48ssvMWvWLPzlL3/ByJEjYTQaL/uYtWvXIjQ0FOXl5Vi9ejXeeustbN26FZs3b27Uh6lamc1mbNiwAQBQXV2N7OxsrFmzBlOnTsUbb7yBtWvXolWrVjJXSWrDMECNNnLkSCQkJGD+/PkXDQMff/wxqqurMWXKFBmqaxyz2YzrrrvOa9sDDzyABQsWYPLkyXjwwQfx6aefer7XvXv3li4RAFBVVYWgoCBZXvtK7N27FwDw+OOPIyYmplGP6dmzJ6KiogAAw4cPR1FREf7zn/9g8+bN6N+//0Ufo7TjciU0Gk29n9H77rsP999/P8aMGYPx48djy5YtMlVHasXbBNRoWq0WEydOxM8//4w9e/bU+/6CBQsQHx+PkSNHAgByc3Px0EMPoVWrVp6m4BdeeAEOh8PzmPObcv/+978jNTUVVqsVffv2rXfBmzRpEqxWK7KysjBq1ChYrVYkJSVh2rRpV918ev/992PUqFFYsmQJTpw44dl+4W0Cm82GadOmoVu3bggNDUVERAT69u2LL7/8st5zlpSUYMqUKYiIiIDVasXo0aNx9OhRSJKEmTNnevaray7+5ZdfMH78eISHh6NNmzYAgO3bt+POO+9ESkoKzGYzUlJSMGHCBK8agXNN0xs2bMDUqVMRGRmJkJAQ3HfffaisrERubi7uuOMOhIWFIT4+Hk8++SRqa2sve1xcLhdeffVVdOjQAUajETExMbjvvvtw6tQpr2M0ffp0AEBsbGy999dYdR+Ade9t0KBB6NKlC7777jv069cPQUFBmDx5MgDg5MmTuOeeexATEwOj0YiOHTvijTfeuOitHpfLhVmzZiE5ORkmkwm9evXCN998U2+/w4cP46677vJ6znfeeafec7300kto3749zGYzwsLCkJGRgbfeeqvJ77epbrzxRkydOhU//fQTvvvuO8/2DRs2YNCgQYiMjITZbEZycjJuu+02VFVVNXtNpB4MA9QkkydPhiRJmD9/vtf2/fv3Y+vWrZg4cSK0Wi1yc3PRp08ffPXVV3j++eexZs0aTJkyBS+//DKmTp1a73nfeecdfP3115gzZw4++ugjVFZWYtSoUSgtLfXar7a2FjfffDOGDh2KL7/8EpMnT8abb76J2bNnX/V7u/nmmyGEwPfff9/gPjU1NSguLsaTTz6JZcuW4eOPP8b111+PW2+9FR9++KFnP5fLhbFjx2Lx4sV46qmn8MUXX+Daa6/FiBEjGnzuW2+9Fenp6ViyZAnmzZsHwB2W2rdvjzlz5uCrr77C7NmzcebMGfTu3dvrXnOdBx54AKGhofjkk08wffp0LF68GFOnTsXo0aPRtWtXfP7555g4cSLeeOMNzJ0797LH5OGHH8ZTTz2F4cOHY/ny5XjxxRexdu1a9OvXz/P6X3zxhac1aO3atcjMzMQDDzxw2ee+UFZWFgAgOjras+3MmTO45557cNddd2H16tV45JFHUFBQgH79+mHdunV48cUXsXz5cgwbNgxPPvkkHnvssXrP+49//ANr167FnDlzsGjRImg0GowcORKZmZmeffbv34/evXtj7969eOONN7By5UqMHj0ajz/+OF544QXPfq+++ipmzpyJCRMmYNWqVfj0008xZcoUr74OkyZNQnMtBnvzzTcDgCcMHD9+HKNHj4bBYMD8+fOxdu1avPLKK7BYLLDb7c1SA6mUIGqigQMHiqioKGG32z3bpk2bJgCIQ4cOCSGEeOihh4TVahUnTpzweuzrr78uAIh9+/YJIYQ4duyYACCuueYa4XA4PPtt3bpVABAff/yxZ9vEiRMFAPHZZ595PeeoUaNE+/btG1V3586dG/z+mjVrBAAxe/Zsz7bWrVuLiRMnNvgYh8MhamtrxZQpU0T37t0921etWiUAiPfee89r/5dfflkAEDNmzPBsmzFjhgAgnn/++cu+B4fDISoqKoTFYhFvvfWWZ/uCBQsEAPG73/3Oa/9bbrlFABB///vfvbZ369ZN9OjR45KvdeDAAQFAPPLII17bf/rpJwFAPPvss/XeQ0FBwWXfQ92+ubm5ora2Vpw9e1YsWrRImM1mkZSUJKqrq4UQ7v8vAOKbb77xevzTTz8tAIiffvrJa/vDDz8sJEkSv/76qxDi3M9WQkKC5zmFEKKsrExERESIYcOGebbddNNNolWrVqK0tNTrOR977DFhMplEcXGxEEKIMWPGiG7dul32PTbGxY7ZxIkThcViafAxdf8nDz/8sBBCiM8//1wAEDt37vRJTRS42DJATTZlyhQUFhZi+fLlAACHw4FFixZhwIABaNu2LQBg5cqVGDx4MBISEuBwODxfdbcQNm3a5PWco0ePhlar9fw7IyMDAOo1h0uShLFjx3pty8jIqLfflRCN/G1uyZIl6N+/P6xWK3Q6HfR6PT744AMcOHDAs0/d+7vjjju8HjthwoQGn/e2226rt62iogJPPfUU0tPTodPpoNPpYLVaUVlZ6fV6dcaMGeP1744dOwJwH98Lt1/umH377bcAUG80RZ8+fdCxY8eLNrU3RVxcHPR6PcLDw3HPPfegR48eWLt2LUwmk2ef8PBwDBkyxOtxGzZsQKdOndCnTx+v7XW/kdd1vqtz6623ej1ncHAwxo4di++++w5OpxM2mw3ffPMNxo0bh6CgIK+f11GjRsFms3luWfXp0we7du3CI488gq+++gplZWVXdQya6sKf0W7dusFgMODBBx/Ev//9bxw9erRF6yH1YBigJhs/fjxCQ0OxYMECAMDq1auRl5fn1XEwLy8PK1asgF6v9/rq3LkzANRr4o6MjPT6d11v9Orqaq/tQUFBXhf2un1tNttVv6+6D8eEhIQG91m6dCnuuOMOJCYmYtGiRcjMzMS2bdswefJkrxqKioqg0+kQERHh9fhLjbKIj4+vt+2uu+7CP/7xDzzwwAP46quvsHXrVmzbtg3R0dH1jg2Aeq9nMBga3H65Y1ZUVNRgXQkJCZ7vX6n169dj27Zt2LlzJwoLC/HDDz+gU6dOXvtc7LWLiooarOn8uuvExcXV2zcuLg52ux0VFRUoKiqCw+HA3Llz6/28jho1CsC5n9dnnnkGr7/+OrZs2YKRI0ciMjISQ4cOxfbt26/sIDTRhT+jbdq0wfr16xETE4NHH30Ubdq0QZs2bVqkDwOpC0cTUJOZzWZMmDAB//rXv3DmzBnMnz8fwcHBuP322z37REVFISMjA7Nmzbroc1zqA1cuy5cvhyRJuOGGGxrcZ9GiRUhNTcWnn37qNfztwg6MkZGRcDgcKC4u9vogzs3NbfC5LxxOV1paipUrV2LGjBl4+umnvV6ruLi40e/rStUFtDNnztQbynb69GnPSIAr1bVr18s+x8WGGEZGRuLMmTP1tp8+fRoA6j3nxY55bm4uDAYDrFYr9Ho9tFot7r33Xjz66KMXrSM1NRUAoNPp8MQTT+CJJ55ASUkJ1q9fj2effRY33XQTsrOzm32kQ11r3KBBgzzbBgwYgAEDBsDpdGL79u2YO3cu/vCHPyA2NhZ33nlns9ZD6sGWAboiU6ZMgdPpxGuvvYbVq1fjzjvv9LoQjhkzBnv37kWbNm3Qq1evel/+FgYWLFiANWvWYMKECUhOTm5wv7pJcs7/kMrNza03mmDgwIEA4DVMEQA++eSTRtckSRKEEPXG7L///vtwOp2Nfp4rVdc8v2jRIq/t27Ztw4EDBzB06NBmr+Fihg4div379+OXX37x2v7hhx9CkiQMHjzYa/vSpUu9WkHKy8uxYsUKDBgwAFqtFkFBQRg8eDB27NiBjIyMi/68XthyBQBhYWEYP348Hn30URQXF+P48ePN8n7rfP3113j//ffRr18/XH/99fW+r9Vqce2113pGQFx4fIguhS0DdEV69eqFjIwMzJkzB0KIenML/PWvf8XXX3+Nfv364fHHH0f79u1hs9lw/PhxrF69GvPmzZNl4pTq6mrP/d/q6mocPXoUy5Ytw8qVKzFw4EBPL/6GjBkzBkuXLsUjjzyC8ePHIzs7Gy+++CLi4+Nx+PBhz34jRoxA//79MW3aNJSVlaFnz57IzMz0jDjQaC6fw0NCQnDDDTfgtddeQ1RUFFJSUrBp0yZ88MEHl5zhz1fat2+PBx98EHPnzvX0wD9+/Diee+45JCUl4Y9//GOz13Axf/zjH/Hhhx9i9OjR+Otf/4rWrVtj1apVePfdd/Hwww+jXbt2XvtrtVoMHz4cTzzxBFwuF2bPno2ysjKvUQJvvfUWrr/+egwYMAAPP/wwUlJSUF5ejqysLKxYscLTD2Hs2LHo0qULevXqhejoaJw4cQJz5sxB69atPf1lrpbL5fL8jNbU1ODkyZNYs2YNPvvsM3Ts2BGfffaZZ9958+Zhw4YNGD16NJKTk2Gz2TwjfYYNG+aTeigwMAzQFZsyZQp+//vfo1OnTrj22mu9vhcfH4/t27fjxRdfxGuvvYZTp04hODgYqampGDFiBMLDw2Wp+ejRo+jbty8AwGKxIDY2Fj169MCSJUtw6623XvZD+v7770d+fj7mzZuH+fPnIy0tDU8//TROnTrl9eGi0WiwYsUKTJs2Da+88grsdjv69++PRYsW4brrrmv0h/nixYvx+9//Hn/+85/hcDjQv39/fP311/U6BDaX9957D23atMEHH3yAd955B6GhoRgxYgRefvnli/623BKio6OxefNmPPPMM3jmmWdQVlaGtLQ0vPrqq3jiiSfq7f/YY4/BZrPh8ccfR35+Pjp37oxVq1Z5TWzUqVMn/PLLL3jxxRcxffp05OfnIywsDG3btvX0GwCAwYMH47///S/ef/99lJWVIS4uDsOHD8dzzz0HvV7vk/dXXV3t+Rk1m82Ijo5G165d8a9//Qt33323px8I4O5AuG7dOsyYMQO5ubmwWq3o0qULli9fjhtvvNEn9VBgkERju1AT0VVbvHgx7r77bvz444/o16+f3OUQEQFgGCBqNh9//DFycnJwzTXXQKPRYMuWLXjttdfQvXv3ekMriYjkxNsERM0kODgYn3zyCV566SVUVlYiPj4ekyZNwksvvSR3aUREXtgyQEREFOA4tJCIiCjAMQwQEREFOIYBIiKiAMcwQEREFOAYBoiIiAIcwwAREVGAYxggIiIKcAwDREREAY5hgIiIKMAxDBAREQU4hgEiIqIAxzBAREQU4BgGiIiIAhzDABERUYBjGCAiIgpwDANEREQBjmGAiIgowDEMEBERBTiGASIiogDHMEBERBTgGAaIiIgCHMMAERFRgGMYICIiCnAMA0RERAGOYYCIiCjAMQwQEREFOIYBIiKiAMcwQEREFOAYBoiIiAIcwwAREVGAYxggIiIKcAwDREREAY5hgIiIKMAxDBAREQU4ndwFUIByOgGbDaipAVyuhr+EcP+p0wF6PWAweH9pmGfJf7iEC3anvd6X0+WERtJAkiRoJE2DX0atESadCVqNVu63QgGGYYB8z24Hzp4FysqAykqgqsr9Z2UlUF3tDgG1tb55LY3mXDAwm4HgYCAkxP1n3d8tFkCSfPN6FJCEEKiwV6DcXo7ymnKU28tRVlOG8ppyVDuqPR/6LuHyyevpNXqYdCaY9WZY9BZYDBYE6YNg0VsQYgxBuDkcBq3BJ69FBACSEELIXQQplN0OlJQAxcXuD/+6r8pKuSvzptEAVqs7HISGApGR5760/A2MznG6nCiqLkJRVRGKqotQaitFub0cFfYKn33Q+4pFb0G4ORzhpnCEm8MRYY5AmCmMIYGuCMMANY7LBRQWArm5wJkzQFERUFEhd1VXR6MBwsKA6GggJgaIjQXCw9mKECCEEDhrO4u8ijzkV+ajoKoAJbYSv/vQbyqrwYpIcyTig+MRb41HVFAUJP5M02UwDNDFuVxAfr77w//0aSAvz3dN+/5Mr3cHg8REIDkZiIiQuyLyoeLqYpwsPYmcshzkV+aj1qX+n2m9Ro9YaywSghMQb41HtCUaGol9bcgbwwCdU1ICHDsG5OS4g4DDIXdF8rNYgKQk91diortvAimG3WlHTlkOssuykV2ajcpaP7uFJQOdRocYSwwSgxORGp6KMFOY3CWRH2AYCHRFRe4AcPSoOwxQwzQa962EpCS2Gvixut/+s0uzkVeZp/hm/+YWbgpHangqUsNSERkUKXc5JBOGgUCUn+8OAMeOuXv805UJDwfatnV/WSxyVxPQKu2VOFx8GIeLDuOs7azc5ShWiDEEqWGpSA1PRYwlRu5yqAUxDASKkhLg4EF3C4DSO/75G0kCEhKAdu2A1FT3nAjU7BwuB46ePYrDRYdxuvw0BHgp8yWrwYq08DR0iOrAWwkBgGFAzZxO94f/gQPujoDU/PR6dyBo29YdENiL26eEEDhdfhqHig7hWMkxOFzs19IS4q3x6BDVAWnhaZwQSaUYBtSorAzYvx/49Vf3DH8kj+BgoEsXoEMHd0igK1brrMXBwoPYm78X5fZyucsJWEatEe2j2qNTdCeEGEPkLod8iGFATbKzgX373H/yv9V/GAxAp07uYBAUJHc1ilJVW4U9eXtwoPAA7E673OXQ/0iQkBSahM7RnZEUmiR3OeQDDANqcOwY8PPP7pkAyX9pNEB6OpCRwZEIl1FcXYzdebuRVZzF0QB+LsIcgZ7xPZEanip3KXQVGAaU7OhR4JdfGAKUqFUroGtX99wF5JFTloNdebtwquyU3KVQE0WYI9AjvgfSwtPkLoWuAMOAEjEEqEdsLNCnDxAfL3clsjpTfgZbc7YirzJP7lLoKjEUKBPDgFII4b4dwBCgTq1auUNBVJTclbSowqpCbM3ZypYAFWIoUBaGASXIzQV+/NE9WyCpW3q6OxRYrXJX0qwq7BXYmrMVWcVZcpdCzSzSHIn+yf0RZ42TuxS6BIYBf1ZVBWzZAmTxghlQtFrgmmuA7t1VNySx1lmLHbk7sCdvD5zCKXc51ILSI9JxXavrEKTniBp/xDDgj1wuYO9e9wiBQFgpkC7ObHa3ErRvL3clPvFr4a/YmrMV1Y5quUshmeg1evRM6IkuMV24cqKfYRjwNzk57lsCXDSI6iQmAjfc4J7ESIHKasrw/YnvkVOeI3cp5CfCTGHon9QfiSEcTeMvGAb8RUUFkJnp7iRIdCGdDujVy337QCFTHAshsCd/D7af3s5pg+miUsNS0TepL6wGdfeRUQKGAX9w6JC7NYC3BOhyYmLcrQR+PmlRcXUxNh3fhIKqArlLIT9n0BrQL6kf2kW2k7uUgMYwICebDfj+e7YGUNNoNEC3bkCPHu6/+xGny4kduTuwM3cnZw6kJkkNS8WA1gNg0pnkLiUgMQzIJTsb2LTJPWKA6EqEhQGDBwPR0XJXAgAoqCzAt8e/RYmtRO5SSKGC9EEY2Hog1zuQAcNAS3M43MMF9++XuxJSA40G6NsX6NxZ1jL25u/FllNb2BpAPtE5ujOubXUtdBqd3KUEDIaBllRQAHz7LUcKkO+lpbn7EhgMLfqydqcd3534DkfPHm3R1yX1CzOFYUjqEEQFBdasnHJhGGgpu3cDW7e65xAgag4hIcDw4UBkZIu8XFFVEb4++jXKaspa5PUo8GgkDfok9kFGbIbcpagew0BzczjcfQOOHJG7EgoEWi3Qrx/QsWOzvsyBggPYnL2ZswhSi2gT3gYDUwbytkEzYhhoTmVlwLp1XFiIWl56uvu2gc63F89aZy2+P/k91xSgFhdhjsCNbW5EiDFE7lJUiWGgueTkAOvXAzU1cldCgSo8HBg50meLHlXYK7Dm8BqctZ31yfMRNZVRa8SwtGGcubAZMAw0h/37gc2b2T+A5BcUBIwYcdVLIxdWFWJt1lpU1XIoLMlLI2nQL6kfOkV3krsUVWEY8CUh3CFg3z65KyE6R68Hhg0Dkq5s7HZ2aTbWH12PWhdnyCT/0Tm6M/ol9YOkkOm5/R3DgK84HO7bAidPyl0JUX0aDXD99UCHDk162MHCg/jh5A+cP4D8UnJoMoalDWPHQh9gGPCF2lpg7VrgzBm5KyG6tB493AseNcL209vxy5lfmrkgoqsTb43HiPQR0Gv1cpeiaAwDV6umBli92j2hEJEStGvnHmnQwLoGLuHCdye+w6GiQy1cGNGVibHEYGT6SBh1RrlLUSyGgatRVeUOAhw6SErTqhVw4431hh46XA6sO7IOp8pOyVQY0ZWJMEdgVNtRCNIHyV2KIjEMXKmKCmDlSvdcAkRKlJDgHmnwv0DgcDmwNmstTpeflrkwoisTagzF6HajYTX4ZjhtIGEYuBKlpcCqVe5AQKRk/wsEDg0YBEgVrAYrRrcdjVBTqNylKArDQFMVF7uDQHW13JUQ+YSrVSK+agNkV+TIXQqRT5h1ZoxpNwbh5nC5S1GMi/cgoosrL3f3EWAQIJUQGgkF+iK0LiyCVnC8NqlDtaMaqw6vQnlNudylKAbDQGPZbO4gUMUZ2EgdhAQUtjbCprHBYrOhj93ICwKpRlVtFVYfXg2bwyZ3KYrAc78xamvdQaC0VO5KiHxCAChqbUaV5tyFMqTaht52s3xFEflYaU0p1hxeg1onZ8+8HIaBy3G53CsPFhbKXQmRzxS3NqNSW/92V1hVNXrVMhCQehRUFWDdkXWcRfMyGAYuRQhgwwb3CoREKlHSKggVuob7vURWViPDybHapB455Tn49ti3YH/5hjEMXMrmzcDRo3JXQeQzVVEmlBov3+8lvrwKrYWpBSoiahlHzh7B5uzNcpfhtxgGGrJjB1cfJFWxW/QoCrU3ev/25XaEC873Tuqxr2AfdpzZIXcZfolh4GJOngS2bZO7CiKfceo1KIgTcKHx900llwvdqwSMgpcJUo9tp7fhZClXl70Qz/ILlZW5+wkQqYQAUNjKAAccTX6svtaB3jUGSLzVSiqy4dgGlNVwKvnzMQycz+FwjxywN74plcjflSQFwaa58rHWFpsNGU6OMCD1sDvtWHdkHRyupgdktWIYON9333EFQlKVymgzygxXP1FWXEU1Ul0MBKQexdXF+O7Ed3KX4TcYBurs3QtkZcldBZHP2K0GFIX4bva1thU2RAqDz56PSG5ZxVnYm79X7jL8AsMAAOTmAlu2yF0Fkc849RoUxLog4Lub/ZJLoFulCyZeNkhFtpzagtyKXLnLkB3P6qoqYP1690yDRCogABQm6a+ow+Dl6BwO9Lbp2aGQVMMlXFh/dD2qagN73RmGgW+/5eJDpCplCUGwSTXN9vxBthp0Yv8BUpGq2ipsPL5R7jJkFdhhYN8+TjVMqlIbpEepufmX2E6ssCEMnJCI1ONU2Snsyw/cieYCNwyUlAA//SR3FUQ+IwAUxWl82k+gIZIQyKjW8HYBqcpPOT+h1BaYq9MGZhgQAti40T2vAJFKlMcHoaYZbw9cyFxTg/a8XUAq4nA58O3xwFzQKDDDwK5dQH6+3FUQ+UytWYeSoOa/PXCh5EobQqBr8dclai75lfnYlbdL7jJaXOCFgZIS4Oef5a6CyKeK47UtcnvgQpJLoKtN2+KvS9Scfj79c8DdLgisMCAEsGkT4HTKXQmRz5THmZt19MDlBNlq0I7TFZOKOIUTm05sCqjbBYEVBvbtA/Ly5K6CyGccJh1KLPIFgToplTWw8nYBqUhuRS72FQTO6ILACQM2G7B9u9xVEPlUUbyuScsSNxfJ5UJXG8MAqcv209thc/huSm9/FjhhYNs2rkZIqlIRY7qq1Qh9zWqzIV3wdgGph91px/bTgfFLZGCEgeJi4OBBuasg8hmXToOS4Fq5y6gnpcIOQ4BcVigwHCg4gOJq9a9mGxhnbWamu/MgkUqUxZnghP91hNU6nejgNMldBpHPCAhsOaX+hezUHwZOnOCUw6QqToMW5Ub/uT1wobhKG1c2JFU5VXYKJ0tPyl1Gs1L3GetycWliUp3SOKNfdBpsiORyoZODrQOkLltObYFL+O95d7XUHQb27QNKA2viCFI3h0mHCn3LzzTYVFGV1RxqSKpSYivB/oL9cpfRbNQbBmpqgF9+kbsKIp8qjdXLMtNgU0lCoKOdqxqSuvx8+mfUOOSf16M5qDcM7NrlDgREKlEbpEeFzv9bBepEVFVzmWNSlRpnDXbn7Za7jGahzjBgtwP71ducQ4GpJEZ5ze4d7MqrmehS9hXsg92pvjlr1BkG9u/nBEOkKvZgA6q0ymkVqBNaVY0oGOQug8hn7E67KvsOqC8MOBzAnj1yV0HkU2ejlHuqtrdJcpdA5FN78vbA4XLIXYZPKfcK05CDB4Fq5f0GRdQQW5jRr6YdbiqrrQbxwih3GUQ+U+2oxq+Fv8pdhk+pKwy4XMBudXbuoMBVFq7836zT7Mp/D0Tn25W3S1XzDqgrDGRlARUVcldB5DMOsw7VCm4VqGOttiFYsDMhqUeFvQJZxVlyl+Ez6gkDQgA7d8pdBZFPlUepp/NdG6d63gsRAOzM3QmhknVv1BMGTpwASkrkroLIZ4QEVBrUM1dGVHWNii44RO5ZCdWyZoF6zk0uUUwqUxXlnysTXimt04nWLrPcZRD51IHCA3KX4BPqCANVVUB2ttxVEPlURbA6mh/P16pGPR2uiAAguzQbVbVVcpdx1dQRBn791d1ngEgl7BY9bJJ6bhHUCaqp4RTFpCoCQhXDDNUTBohUpCJSvR+YaQ71vjcKTL8WKf8zSPlh4PRpoKxM7iqIfMalkVCpV/5wwoZEVtmgA+cdIPUoqynD6fLTcpdxVZQfBthxkFSmKtoEF9R7b13jciHVZZK7DCKfUvqtAmWHAbsdOHZM7iqIfKrcqp4RBA1JsKn/PVJgOVZyTNGrGSo7DGRlAU5eVEg97FY97FDuBaWxTHY7ItiRkFTE4XIoekZCZYeBQ4fkroDIp6pDA+cDMsEZOO+VAsOhIuV+Jik3DFRVAfn5cldB5FPVRnUti3opkfbAea8UGPIr8xU754BywwD7CpDKOA1a1Ejqv0VQx1Rjh1lo5S6DyKeOlxyXu4QrotwwcPy43BUQ+VR1eOAt5NNKBN57JnU7dlaZv6gqMwzU1ABnzshdBZFPVQcF3iyaUbWB955J3c5UnEGNQ3mzhyozDGRnAy71jsOmwCM0Eqo1yruAXK3gmhpoBScgIvVwCRdOlZ2Su4wmU2YYOKmOJSOJ6tjCjBAIvN+SJZdAPIxyl0HkU0pc1lh5YUAI4JTyUhfRpVRbA/e34zhH4L53UqfssmwIhS2ep7wwkJ8P2NQ7bzsFpmp9rdwlyCa0JnDfO6mTzWFDQVWB3GU0ifLCQE6O3BUQ+ZTdaoADgTvmXudwIEJwAiJSF6X1G1BeGMjLk7sCIp+qDtXJXYLsElwMA6QueRXK+qxiGCCSmc3I9TXCajk6iNQlv1JZM+QqKwycPeteqZBIRewS75mbeV6TytQ4a1BiK5G7jEZTVhhgqwCpTK1ZBxf4W7HG5UKw4O0SUpfcily5S2g0hgEiGdktvFdeJ5JLGpPKKOlWAcMAkYzsZo6xrxPm5LEgdWHLQHOoqQFKSuSugsin7DreIqhjdfBYkLqU2EoUs06BcsIAWwVIhWo17DxYx1TLY0Hqo5RbBcoJA4WFcldA5FMOkw5OcFhhHa3TCSvYiZDURSkzESonDPAWAamM3cIPvgtFckQBqYxShhcyDBDJxG5WzunXUkJdPCakLqW2UrlLaBTlnHmlyjigRI1l17PD3IWCORMhqUxpjTI+u5QRBqqqAHYuIpWxs/NgPWae56QydqcdVbVVcpdxWcoIA2wVIJVx6jXsPHgRWqcTRqGMyxJRYynhVoEyzjqGAVIZp0Erdwl+Kwg8NqQuSrhVoIwwwM6DpDJOgzJOPTmYFXJZImostgz4ClsGSGWcemWcenIw8zYBqQxbBnylrEzuCoh8yqXjPPwNMQkeG1IXtgz4SnW13BUQ+ZRDK+QuwW8ZeWhIZaod/v8Z5v9hQAj3IkVEKuJkH7kGGVxMA6QuSlisyP/DgN3uDgREKuLUcHKdhuidPDakLgLC7wOB/4cBm03uCoh8zilxjoGG6F0MA6Q+NU6GgavDWwSkQpxwqGE6J48NqY/N4d+/2Pp/GGDLAKmMSytBgLe+GiK5XNBzRAGpDMPA1WIYIJVxGrlM7+UEgceI1IV9Bq4WbxOQynDCocszSzxGpC7+3jLg//GbLQON8t6mTXhv0yYcLyoCAHSOj8fzY8ZgZJcuAIBJCxfi35mZXo+5NjUVW55+2mtb5pEj+MuXX+KnY8eg12rRLSkJa373O5gNBgDArNWrsWrPHuzMzoZBp0PJnDnN/+bUpgVawPvf/Sxy8orqbb/35oF48fG7MO3VhfjvOu+fh24dUrHsH+d+Hk6cLsCs//c5tu/Ngr3WgYG9OmPm7+5EdHhIs9evEWiR46R0mz7fhE2fb0LRGff/dXxaPMY8MAZd+rvP+4UzFyJzpff/c2qXVDy90Pu8P7L7CL5890sc23sMWp0WSe2S8Lu3fweDyX3er/5gNfb8uAfZv2ZDp9dhzsY5zf/mVMbfOxD6fxhwOOSuQBFahYXhlXHjkB4TAwD4d2YmfvPuu9gxfTo6JyQAAEZ07owFEyd6HmPQef/3Zx45ghFvv41nRo7E3DvvhEGrxa5Tp6CRzl2V7Q4Hbu/ZE33T0vDBjz+2wDtTHyE1/6fc8neegfO8XvmHjp3GPU/Nwagbenq2DezdGa/96eI/D1XVNbj3qTno2KYVFr/2BADgjYVf4oHp7+CLuU9Bo2nm39wZBholLCYM4x4bh5gk93mfuTIT7057F9M/mo6ENu7zvnO/zpj4/Ln/Z53e+7w/svsI3v7d2xh5/0jc+ac7odVrcerQKUiac/8BDocDPYf2RNo1afjxS573V8Lh8u/PMv8PA5xjoFHGdu3q9e9Zt9yC9zZtwpajRz1hwKjTIS40tMHn+OOSJXh8yBA8PWKEZ1vb2FivfV64+WYAwMLNm31VeuCRmv9nOjIs2Ovf732yFq0TonFd13aebQa9DjERF/952L7vCE7lFWHVvOkItpgBAK//aSK6jnsCm3f8iut7dmy+4gGvAEoN63qD93l/y6O3YNN/N+HonqOeMKDT6xAa1fB5v+TvSzDkziEYMenceR+b7H3e3/yQ+7zfvILn/ZVyCf8eMsswoEJOlwtLfv4ZlXY7+qalebZvPHQIMU8+iTCzGQPbtcOs3/wGMSHuJt/8sjL8dOwY7u7TB/1mz8aRggJ0iIvDrFtuwfXp6XK9FVVqiZaB89lrHVi2/ic8MH4YpPNee8uuQ+g5/kmEWMy4NqMdnpz8G0T97xaAvbYWEiQYzvst0mjQQ6ORsG1vVvOHgWZ9dnVyOV34ef3PsFfbkZZx7rw/9PMhPDn8SZiDzWjXox1+88hvEBLh/n8uKy7Dsb3H0GdEH8yePBsFpwoQlxKHWx65BendeN77kvDzzzKGARXZk5ODvrNnw1ZbC6vRiC9++1t0+l+rwMjOnXF7z55oHRGBY4WFeG75cgx58038/OyzMOr1OFpYCACYuXIlXr/tNnRLSsKHW7Zg6JtvYu/zz9drIaCr0MK/9K77cSfKKqox/sZ+nm2DenfG6Bt6IjE2Atm5hXhj4XLc9ac3seLdZ2E06NG9YxqCTAa88v5S/HnyOAgh8Mr7S+FyCeQXN/+iK2wXaLycrBzMvn82au21MJqN+O1rv0VC2rlbBD2H9UREXAQKTxdi+bzlePO3b+LZRc9Cb9CjMMd93q/810rc9vvbkNQuCVtWbcGbD7+J5z99vl4LAV05fx9OzDCgIu1jY7Fz+nSUVFXhvzt2YOLChdg0bRo6JSTg/3r39uzXJTERvVJS0PqZZ7Bqzx7c2qMHXP87zg8NGID7+/cHAHRPTsY3Bw9i/ubNeHncOFnekxqJFv6o+3TNjxjUpzNio8I828YOPvfz0D41ERntUtD/7mfw7U97MGJAD0SGBeOd5x/C9Lc+wsIvvoVGknDzkN7o0jYZ2ubuLwBAYhxotNjWsZi+eDqqyquwY8MOLJy5ENP+OQ0JaQnofeO5/+fE9ESkdErBM2OewZ4f9qDHkB4Q/1sHYsCtA9D/Zvd5n9whGQe3HcTm5Zsx7jGe977CloGrxXuHjWbQ6TwdCHulpGDb8eN4a8MG/L977qm3b3xoKFpHRuJwfr7n3wDQKT7ea7+OcXE4WVzczJUHmpa7KJzKK8KPOw5g3ozfXnK/mMhQJMZG4lhOvmfbDb064bv/zEJxaQW0Wg1CrUHodfufkBQX2dxloyWPkdLp9DpPB8KUTik4vv84Nny8Aff8pf55HxoVisj4SOSfzPf8GwDiU73P+7jUOBTn8rz3JcnPP8v8/9acnx9AfyaEQE0DozGKKiqQXVzsCQEpkZFICAvDr3l5Xvsdys9H64iIZq81kLRA/0GPJWs3IzIsGEOuu+aS+50trcDp/OKLdiiMCLUi1BqEzTsOoqikHMP6db3IM/iWy89/i/JnQgg4ai9+3leUVKA4r9gTAiITIhEWHYa8E97nff6JfETE87z3JX9v7WLLgEo8+8UXGNmlC5LCw1FeU4NPtm3DxkOHsPbxx1Fhs2HmypW4rXt3xIeG4nhREZ5dtgxRVivGde8OwJ1a/zR8OGasWIGurVqhW1IS/p2ZiYO5ufj8oYc8r3OyuBjFlZU4WVwMp8uFndnZAID06GhYTSZZ3rvStFQYcLlc+PyrzbhteF/otOfWTK6stmHOhysxYkB3xESE4lRuEV6bvwwRoVbcdH13z36frf0R6cnxiAwLxi/7j+CFdz7DlNuGok1SXLPX3tKdLJXqi3e+QJd+XRAeG46aqhps+2obDv18CI+//ThsVTas/OdKdB/SHaFRoSg6XYRl7y6DNcyK7oPPnffD7x2OFf9vBVq1bYWk9knIXJmJ3BO5eOjVc+d9cW4xKksrUZxbDJfLhexf3ed9dFI0TEE87xvD31sGGAZUIq+8HPcuWIAzpaUINZuRkZiItY8/juGdOqHabseenBx8uGULSqqqEB8aisHt2+PTqVMRfN4H+B+GDYPN4cAflyxBcWUlurZqha//8Ae0iY727PP88uVekxd1f+klAMC3TzyBQe3bt9wbVrIWCgM//HIQOfnFuGNkf6/tWo0GB4/mYOnXW1BWUYWYiFBc1609/jF9KqznXdiPZufh1Q+WobS8Eq1iI/HY3SMx5bZhLVK7fw/C8h/lReVY8PwClBaWwmw1I7FtIh5/+3F0uq4T7DY7crJysGXVFlSVVyE0KhTte7XH1L9Nhcly7v952F3D4LA7sOTNJagsrUSrdq3wh3f+gOhW58775fOWe01e9NLd7vP+iXlPoH0vnveNofHzWTUl4e+9GrZuBXbulLsKIp+pDjMiP9K/ZyOT255QI05LPEakHt3iuqFPYh+5y2iQf0cVADAa5a6AyKfY1nV5Lh4kUhmj1r8/yxgGiFqYxsFG8Mux80YBqYxJ5999K/w/DLBTGqmMtsYpdwl+rwo8RqQuRp1//2LLMEDUwrS1Lt4quBQJsLFlgFSGLQNXi7cJSIW0ChjIIxeHlseG1Idh4GqxZYBUSCv8/9STi0PLY0Pqww6EV4stA6RCDAMNs7fA2gdELY19Bq6WRgMYDHJXQeRTWid7DTSkVsNjQ+pi0Br8ftIh/66uDm8VkMowDDSshmGAVMbf+wsASgkDwcFyV0DkU1qHf0/8KSdbS67kRNQCgg3+/xmmjDAQWn8lNSIl09byA68hDAOkNmGmMLlLuCxlhIGwMLkrIPIpbS3H0TekWuKxIXUJNfn/L7TKCANsGSCV0do5w15DOPsgqU2o0f8/wxgGiGSgqXFC4jyE9QiNhGrBMEDqwpYBXwkOdg8xJFIJCYBBcMjshWx6AwQzEqmIRtKwA6HPSBIQEiJ3FUQ+ZXBo5S7B71ToeUxIXUKMIZAk/0+4yggDADsRkuoYbOw1f6EyLY8JqYsSRhIASgoD7DdAKmOocshdgt8pAo8JqYsSOg8CSgoDkZFyV0DkU/qKWnYiPI/QSChBrdxlEPlUVFCU3CU0inLCQGys3BUQ+ZS7E6Fe7jL8hk2nZ+dBUp1YqzI+u5QTBoKDgaAguasg8il2Ijynkp0HSWUsegusBqvcZTSKcsIAwNYBUh1DjdwV+I8yZgFSGaW0CgAMA0SyMlRygp06RRI7D5K6xFhi5C6h0RgGiGSkr7SzEyEAIUkokdh5kNQlzhondwmNpqwwEB0NaNmWSOohCUDPToSw6fXg8kSkJlpJq5iRBIDSwoBGA0Qp5+ASNYbRoZO7BNlVGHgMSF2igqKgkZTzEaucSuvwVgGpjLmc/QZytTwGpC5K6jwIKDEMJCTIXQGRT5lKaqBR4KnoKy6NBrkSh1WQuiQEK+uzSnlXoIQE9hsgVZEEYHIa5S5DNuUmI/sLkKpoJS3DQLPT6dg6QKpjrpK7AvkUsLsAqUxCcAJ0GmX9YCsvDABAUpLcFRD5lPls4DaTn+ItAlKZ5NBkuUtoMmWGgdat5a6AyKe0tS4YReDdKqg2GlEj8SYBqQvDQEsJDgYiIuSugsinzLbA6wtTZAi890zqFmGOQLAxWO4ymkyZYQAAUlPlroDIp8ylgTcDX4428N4zqVtqmDI/m5QbBlJS5K6AyKcMlbXQQVmdjq5GrV6HEjAMkLqkhjMMtKzISCAkRO4qiHzKXGuQu4QWU2IMnPdKgSHEGIIIszJvYSs3DABAmzZyV0DkU4E0GyFnHSS1aROu3M8kZYeB9u3lroDIp0xna6ANgFsFDp0OZ8AhhaQu7aOU+5mk7DAQEsIJiEhVJABWm/pXMcw3GyC4cjOpSEJwAkKMyr11rewwAAAdOshdAZFPBRfaoerPSQk4omWrAKlLhyhlfxYpPwykpgIGdkQi9dDWOGF2muQuo9mUmUyoAvsLkHoYtAbFDimso/wwoNUC6elyV0HkU9ZSuStoPicMQu4SiHyqbURbaDXKnkBL+WEA4K0CUh3zWZsq5xyo1etxmmsRkMoo/RYBoJYwEBXl/iJSkeBq9d3+yjWrv3MkBZaooChEBkXKXcZVU0cYADjMkFTHUmCDpKKuhEKScESyyV0GkU+poVUAUFMYaNcOMAbeqm+kXtpaF4JU1JGwxMwVCkldjFoj2kW2k7sMn1BPGNDrgS5d5K6CyKeCz6rnw/OEnh0HSV26xHSBTqOOvj3qCQOAOwzoeU+S1MNYWgM9lN93oMZgQB47DpKK6DV6dIlRzy+g6goDRiPQsaPcVRD5VEiFsocsAUCOSfnvgeh8HaM7wqhTz61pdYUBAMjIcM89QKQSlrxq6KHcFi+7Xo8sqVruMoh8RitpkRGbIXcZPqW+MBAU5O5MSKQSEoDQUuXelzwWpOM6BKQq7SLbIUgfJHcZPqW+MAAAXbsCEq8+pB6WwmoYFNh3oMZgwHG2CpCKSJDQNa6r3GX4nDrDQEgI0Ea560oTXUzYWeWdrofNyquZ6FLaRLRR9OqEDVHvmdq9O1sHSFXMxTaYhHI6LFUZjcjhJEOkIhIkdI/rLncZzUK9YSA8nCMLSHVCi+SuoPEOqWe+JCIA7hEE4eZwuctoFuoNAwDQqxeXNyZVMZXWwOzy/0/ZCrOJ8wqQqhi0BvRK6CV3Gc1G3WHAZAJ69JC7CiKfCivw/1kJDxidcpdA5FM94nvApPP/IH6l1B0GAPeshCHq6+xBgctQYYfF4b8XpZIgM4pRK3cZRD4TagxV1WyDF6P+MKDRANddJ3cVRD4Vmu/wz/UMJeCAgUGA1OXaVtdCI6n741Ld765OSgqQkCB3FUQ+o692INjuf5Oe5FuCUAaH3GUQ+UxicCJSwlLkLqPZBUYYAIC+fTnUkFQlNKcaOvjPzIS1ej326DjBEKmHBAl9k/rKXUaLCJwwEBnJoYakKhqXQGSR/6zDsd+igQNcppjUo2N0R0SYI+Quo0UEThgAgD59AItF7iqIfMZUUoPgWrPcZaDQYkYuOJSQ1MOit6BPYh+5y2gxgRUGDAZgwAC5qyDyqbAcm6y3Cxw6HXbrONMgqcuA1gNg0AbOPDWBFQYAIDkZSE+Xuwoin9E4BSKK5QsDByxa1Eq8PUDq0TaiLZJDk+Uuo0UFXhgAgH793EsdE6mE+awN1tqWn3ug2GLGac40SCoSpA8KmE6D5wvMMGAyATfcIHcVRD4VftoOLVquQ6FDq8MuPYMAqcsNrW9Q9UyDDQnMMAC4bxd06CB3FUQ+o3G4EFGib7HXO2TVwQ7/nxqZqLE6RHUIuNsDdQI3DADuuQc4VTGpSFCRDRZn848uOGs2IZvLE5OKhBhD0LdV4N0eqBPYYUCvBwYPdk9ZTKQS4Tk1zTq6wKHTYbfR3mzPT9TSNJIGQ1KHQK9tuZY1f8NPwdhYdwsBkUpoa12IztdAaobVC4RGwi6rBjbeHiAV6duqL2IsMXKXISuGAQDo3Blo21buKoh8xlBuR0S57ztBHbGaUAi2CpB6tI1oi84xneUuQ3YMA3UGDHBPWUykEtb8aoT4cDGjfKsZRzRce4DUI9IciQGtOREdwDBwjk4HDB8OGI1yV0LkM2HZVTCJq28hqDSZsFPLIEDqYdQaMbzNcOg0/rPYl5wYBs4XEuLuUEikEhKAqGz7VXUodOh02G6yQ3DRT1IJCRKGpA5BiJGjyeowDFwoORno2VPuKoh8RlvrQnSedEUdCoVGwg6rxA6DpCo9E3oiKTRJ7jL8CsPAxfTsCbRuLXcVRD5jqKhFZGnTbxccCjahGLXNUBGRPFqHtkaP+B5yl+F3GAYaMnQoEBPYQ01IXSyFTetQmGcNwnGJ/QRIPWIsMRiaNlTuMvwSw0BDdDpg5EggLEzuSoh8Jiy7CmbX5VsIKswm7NJWtUBFRC0j3BSOkekj2WGwAQwDl2I0AqNGAVar3JUQ+URdh0KDaHiddpvBgG1Gdhgk9bAarBjVdhSMOo4WawjDwOVYre5AYAq8VaxInTQOF2KyHdCj/tSrNQYDtpgdXICIVMOkM2FU21GwGCxyl+LXGAYaIywMGDHCfeuASAW0tS7EnnJ5DTms1euxNciJGolBgNRBp9FhRPoIhJnC5C7F7zEMNFZMjHtSIi5qRCqhrXEiNgfQQQeHToetFoEqOOUui8gnNJIGN7a5MeDXHGgsfrI1RVISMGgQIPFmKqmDzuZATEkQ9ocFoQIOucsh8gkJEgalDEKrkFZyl6IYDANNlZ7uHnbIFgJSg6Ag6IeNxID2IxCk9906BkRy0UgaDE0bivSIdLlLURRJCCHkLkKRsrOBr78GHPxtihQqKAgYOxYIDQUAlNpKseLQClTVckghKZNOo8PwtOGcXfAKMAxcjTNngLVrgVrO0EYKY7EAY8Z4gkCdUlspVh5aicraSpkKI7oyeo0eI9JHID44Xu5SFIlh4GoVFACrVwM1NXJXQtQ44eHuCbUamD+jwl6BNYfX4KztbAsXRnRljFojRrUdhWhLtNylKBbDgC8UF7sDQRWbV8nPJSQAN94IGBqedAgA7E471h1Zh9Plp1uoMKIrE6QPwqi2oxBhjpC7FEVjGPCV0lJg1SqgokLuSoguLj3dPRqmkZ1fXcKFjcc3Iqs4q3nrIrpCVoMVY9qN4VLEPsAw4EtVVe4+BIWFcldC5K1bN6BPnyt66NacrdiZu9On5RBdraigKIxI5ygYX2EY8DWHA9i4ETh6VO5KiNxzYvTvD3TqdFVPs79gP348+SMEeLkg+aWFp2FQyiAuOuRDDAPNZft24Jdf5K6CAplO554To3VrnzzdiZIT+ObYN3C4OJyW5NMjvgd6JfSSuwzVYRhoTkePulsJOBcBtbTQUGDYMCAy0qdPW1RVhPVH16O0ptSnz0t0OTqNDoNSBiEtPE3uUlSJYaC5FRcD69YBZWVyV0KBIi0NGDgQ0NdfldAX7E47vjvxHY6e5a0wahkhxhDc1OYmhJvD5S5FtRgGWoLdDmzYAJw8KXclpGYaDdC3L9C5c4u83L78fcg8lQmX4CqH1HySQ5MxJHUIDNpLD4elq8Mw0JJ27wa2bQOcXBmOfCw42L2qZlRUi75sQWUB1h9dj3J7eYu+LqmfVtKid2JvZMRmyF1KQGAYaGnFxe5WguJiuSshtUhJcc8fcJmJhJqL3WnHxuMbcbzkuCyvT+oTYY7AkNQhnEioBTEMyMHpdLcQ7NkD8PDTldJq3XMHXHON3JUAAPbk7cHWnK1wCrZ80ZWRICEjNgO9E3tDI3Fl2JbEMCCnM2eAb7/lrIXUdHFxwA03AGFhclfipcRWgu9OfIfcily5SyGFsRqsGJwymAsNyYRhQG52O/Djj8Dhw3JXQkqg17tbA1qok+CV2pe/D1tztqLWxRU96fLaRrRF/+T+7CQoI4YBf3HsGLB5M1DJpWOpAcnJwPXXN7jaoL+psFfgh5M/4GQpR9HQxVn0FvRL6ofU8FS5Swl4DAP+xOFwz1q4ezfg4nAt+h+TCejXz73QkAJlFWdhc/Zm2Bw2uUshP6GRNMiIzUCP+B6cUthPMAz4o9JSdytBdrbclZDc0tPdQcBkkruSq2Jz2LA5ezNXQCQkhSShX1I/hJpC5S6FzsMw4M+OHwcyM4FyjuEOODEx7gmEYmPlrsSncityseXUFuRX5stdCrWwYEMw+iX1Q+sw36yVQb7FMODvnE5g505g1y6ucRAIrFZ3B0GF3hJorKziLGzN2YoKO0fSqJ1Oo0O3uG7oGtsVWo1W7nKoAQwDSlFRAezYAfz6K/sTqJHJBHTr5h4loA2MC6bT5cS+gn3YcWYHapw1cpdDPqaRNGgf2R7d47vDalBGp9dAxjCgNAwF6qLXuycNysiQbQZBudmdduzO243debu5PLIKaCQNOkR1QLe4bgwBCsIwoFQVFe7bBwcPMhQokcEAdOrkDgJms9zV+IXq2mrsyd+D/QX7YXfa5S6HmoghQNkYBpSuLhT8+isXQFICq9UdADp0aLYlhpWu1lmLg4UHsSd/D/sUKIBW0qJ9VHt0j+sOi8Eidzl0hRgG1KKy0r3Wwa+/AjW8/+p3oqPdtwLS0gBJkrsaRRBC4OjZo9iVtwuFVYVyl0MXMGqNaB/VHtfEXMMQoAIMA2rjcABZWcC+fUBRkdzVUHIy0LUrEM/51q/G6fLT2J23m7MZ+oFIcyQ6x3RGekQ6JwxSEYYBNcvNdfcpOHqUwxJbktXqHhrYrp3fLSSkdCW2EhwqOoSs4izeQmhBOo0OaeFp6BjVEbFWdc19QW4MA4HAbne3Fhw8CBSyubVZ6PVAaqo7ACQkyF2N6gkhcKbiDA4VHcKxs8e4IFIziQqKQoeoDkiPSOciQirHMBBoSkvdiyIdOwYUFMhdjbJJEpCY6A4AKSmAjk2mcnC4HDh29hgOFx9GTlkOBHhJuxrRQdFIDU9FalgqpwwOIAwDgayiwh0Kjh4F8vLkrkYZtFr3/f+kJKBNGyAoSO6K6DxVtVU4UnwEJ0tP4kzFGbgEh902Rpw1DqlhqUgNT+WwwADFMEBuVVXnWgzy8jhM8XzBwe4P/+Rk9y0AtgAogsPlQE5ZDrLLspFdmo1yO9f4qKOVtIi1xnoCQJCeoTbQMQxQfU4nkJ8PnDnj/srLC6wOiOf/9p+UxE6AKlFiK0F2aTayy7JxpvwMnCJwAq9Oo0OsJRbxwfGIt8YjxhLDdQLIC8MAXZ7L5e5fUBcOcnOBWhV12LJa3asExsa6vyIjA2Z9gEDldDlRVF2EvIo85FXmIb8yX1WjEwxag+fDPyE4AVFBUdBIGrnLIj/GMEBNJ4R7WeXiYuDs2XNfJSX+fXtBo3E3+UdGAlFR575MJrkrIz9gc9hQWFXo+SqqKkK5vdyv+x1oJS3CTGEIN4cj3BSOcHM4IswRCDYEQ+LkVtQEDAPkO0IAZWXuYFBc7P57VZV7dsTKSvcQx+ZmNgMhIe4P/bo/6/5usXD2P2oSIQQqaytRVlOG8ppylNvLPX8vqylDtaO62WswaA2w6C2wGCwI0gchxBiCCHMEwk3hCDGG8EOffIJhgFqOwwFUVwM2m/urutodEJxOd5BwuRr+0unci/sYDO4x/ef/WfdlMrFzH7Uoh8sBm8MGu9Pu+ap11rr/dNV6tjlcDmgkTYNfEiRoNVoYtAaYdWaYdCaYdCaY9WbO8kctgmGAiIgowLFHCRERUYBjGCAiIgpwDANEREQBjmGAiIgowDEMEBERBTiGASIiogDHMEBERBTgGAaIiIgCHMMAERFRgGMYICIiCnAMA0RERAGOYYCIiCjAMQz42MKFCyFJEiRJwsaNG+t9XwiB9PR0SJKEQYMGebZLkoTHHnus5QptokmTJsFqtcpdBpFqfPrpp+jcuTPMZjMkScItt9xSbzniQYMGeV0nqqqqMHPmzIteW/zBzJkz/XpJ5brr8/bt2+Uuxe8wDDST4OBgfPDBB/W2b9q0CUeOHEFwcLAMVRGRPygoKMC9996LNm3aYO3atcjMzMTbb7+NzMzMSz6uqqoKL7zwgt+GAVIuLpTdTP7v//4PH330Ed555x2EhIR4tn/wwQfo27cvysrKZKwOqK2thSRJ0On4I0DU0g4dOoTa2lrcc889GDhwoGd7cnKyLPUo9XoghIDNZoPZbJa7FMVjy0AzmTBhAgDg448/9mwrLS3Ff//7X0yePNknr5GSkoIxY8bgiy++QEZGBkwmE9LS0vD222977bdx40ZIkoT//Oc/mDZtGhITE2E0GpGVlQUAmD9/Prp27QqTyYSIiAiMGzcOBw4cuOhr7tu3D0OHDoXFYkF0dDQee+wxVFVVee0jhMC7776Lbt26wWw2Izw8HOPHj8fRo0e99tuxYwfGjBmDmJgYGI1GJCQkYPTo0Th16pRPjg+RP5o0aRKuv/56AO5fGupuGV6uif348eOIjo4GALzwwgue25GTJk3y7HP48GHcddddnnOqY8eOeOedd7ye53LXg/Xr12Po0KEICQlBUFAQ+vfvj2+++aZePatWrUK3bt1gNBqRmpqK119//YqPh9VqbdS1pe526rx589CxY0cYjUb8+9//BgD88MMPGDp0KIKDgxEUFIR+/fph1apVF33Ns2fP4v7770dERAQsFgvGjh1b7/rU2GNRUFCABx98EElJSTAajYiOjkb//v2xfv36KzoeshHkUwsWLBAAxLZt28S9994r+vTp4/nee++9JywWiygrKxOdO3cWAwcO9HwPgHj00Ueb9FqtW7cWiYmJIjk5WcyfP1+sXr1a3H333QKAeO211zz7ffvttwKASExMFOPHjxfLly8XK1euFEVFReJvf/ubACAmTJggVq1aJT788EORlpYmQkNDxaFDhzzPMXHiRGEwGERycrKYNWuWWLdunZg5c6bQ6XRizJgxXnVNnTpV6PV6MW3aNLF27VqxePFi0aFDBxEbGytyc3OFEEJUVFSIyMhI0atXL/HZZ5+JTZs2iU8//VT89re/Ffv372/ScSBSkqysLPHOO+8IAOJvf/ubyMzMFPv27RMzZswQF16SBw4c6LlO2Gw2sXbtWgFATJkyRWRmZorMzEyRlZUlhBBi3759IjQ0VFxzzTXiww8/FOvWrRPTpk0TGo1GzJw50/Ocl7oe/Oc//xGSJIlbbrlFLF26VKxYsUKMGTNGaLVasX79es9zrF+/Xmi1WnH99deLpUuXiiVLlojevXuL5OTkeu/hcppybamrOyMjQyxevFhs2LBB7N27V2zcuFHo9XrRs2dP8emnn4ply5aJG2+8UUiSJD755BPP4+uuz0lJSWLy5MlizZo14p///KeIiYkRSUlJ4uzZs559G3ssbrrpJhEdHS3++c9/io0bN4ply5aJ559/3ut1lYBhwMfODwN1J93evXuFEEL07t1bTJo0SQghfBYGJEkSO3fu9No+fPhwERISIiorK4UQ507+G264wWu/s2fPCrPZLEaNGuW1/eTJk8JoNIq77rrLs23ixIkCgHjrrbe89p01a5YAIH744QchhBCZmZkCgHjjjTe89svOzhZms1n8+c9/FkIIsX37dgFALFu2rEnvmUgN6s7JJUuWeLZdLgwIIURBQYEAIGbMmFHvOW+66SbRqlUrUVpa6rX9scceEyaTSRQXF3u99oXXg8rKShERESHGjh3rtd3pdIquXbt6/WJz7bXXioSEBFFdXe3ZVlZWJiIiIq4oDDTm2iKE+zoZGhrqeS91rrvuOhETEyPKy8s92xwOh+jSpYto1aqVcLlcQohz1+dx48Z5Pf7HH38UAMRLL73U5GNhtVrFH/7whya9Z3/E2wTNaODAgWjTpg3mz5+PPXv2YNu2bT67RVCnc+fO6Nq1q9e2u+66C2VlZfjll1+8tt92221e/87MzER1dbVXMyMAJCUlYciQIRdtGrz77rvrvRYAfPvttwCAlStXQpIk3HPPPXA4HJ6vuLg4dO3a1dPxKT09HeHh4Xjqqacwb9487N+/v8nvnYjcbDYbvvnmG4wbNw5BQUFe596oUaNgs9mwZcsWr8dceD3YvHkziouLMXHiRK/Hu1wujBgxAtu2bUNlZSUqKyuxbds23HrrrTCZTJ7HBwcHY+zYsVf8Hi53bakzZMgQhIeHe/5dWVmJn376CePHj/ca8aTVanHvvffi1KlT+PXXXy/5Wv369UPr1q09r9XYYwEAffr0wcKFC/HSSy9hy5YtqK2tveJjICdl9RZRGEmScP/99+Ptt9+GzWZDu3btMGDAAJ++RlxcXIPbioqKvLbHx8d7/bvu+xduB4CEhAR8/fXXXtt0Oh0iIyMv+Vp5eXkQQiA2Nvai9aalpQEAQkNDsWnTJsyaNQvPPvsszp49i/j4eEydOhXTp0+HXq+/+BsmonqKiorgcDgwd+5czJ0796L7FBYWev37wvM+Ly8PADB+/PgGX6e4uBiSJMHlcl3y2tNUjbm2NFT32bNnIYRo8Dp2sedoqPbzr2PA5Y+FxWLBp59+ipdeegnvv/8+nnvuOVitVowbNw6vvvrqFR8POTAMNLNJkybh+eefx7x58zBr1iyfP39ubm6D2y48uS7snFT3/TNnztR7jtOnTyMqKsprm8PhQFFRkdfzXvhaUVFRkCQJ33//PYxGY73nPX/bNddcg08++QRCCOzevRsLFy7EX//6V5jNZjz99NMNv2ki8hIeHu75TfjRRx+96D6pqale/77welB3vs+dOxfXXXfdRZ8jNjbWM/LgUteepmrMtaWhusPDw6HRaBq8jgGody1rqPb09HSv/S93LOr2nTNnDubMmYOTJ09i+fLlePrpp5Gfn4+1a9c2/Kb9DMNAM0tMTMSf/vQnHDx4EBMnTvT58+/btw+7du3yulWwePFiBAcHo0ePHpd8bN++fWE2m7Fo0SLcfvvtnu2nTp3Chg0bLpqKP/roIzz++ONerwXAMzHKmDFj8MorryAnJwd33HFHo96DJEno2rUr3nzzTSxcuLDe7Q0icqsL09XV1V7bg4KCMHjwYOzYsQMZGRkwGAxNfu7+/fsjLCwM+/fvv+QEaAaDAX369MHSpUvx2muveW4VlJeXY8WKFU1+3TqXu7Y0xGKx4Nprr8XSpUvx+uuve4YZulwuLFq0CK1atUK7du3qvdb5t0k2b96MEydO4IEHHgDQ+GNxoeTkZDz22GP45ptv8OOPPzb6cf6AYaAFvPLKK8323AkJCbj55psxc+ZMxMfHY9GiRfj6668xe/ZsBAUFXfKxYWFheO655/Dss8/ivvvuw4QJE1BUVIQXXngBJpMJM2bM8NrfYDDgjTfeQEVFBXr37o3NmzfjpZdewsiRIz1Dpfr3748HH3wQ999/P7Zv344bbrgBFosFZ86cwQ8//IBrrrkGDz/8MFauXIl3330Xt9xyC9LS0iCEwNKlS1FSUoLhw4c32/EiUrLg4GC0bt0aX375JYYOHYqIiAhERUUhJSUFb731Fq6//noMGDAADz/8MFJSUlBeXo6srCysWLECGzZsuORzW61WzJ07FxMnTkRxcTHGjx+PmJgYFBQUYNeuXSgoKMB7770HAHjxxRcxYsQIDB8+HNOmTYPT6cTs2bNhsVhQXFzc5PfVmGvLpbz88ssYPnw4Bg8ejCeffBIGgwHvvvsu9u7di48//rhea8L27dvxwAMP4Pbbb0d2djb+8pe/IDExEY888kiTjkVpaSkGDx6Mu+66Cx06dEBwcDC2bduGtWvX4tZbb23ycZCVvP0X1ef80QSX4qvRBKNHjxaff/656Ny5szAYDCIlJUX8/e9/99rvYj2Xz/f++++LjIwMYTAYRGhoqPjNb34j9u3b57XPxIkThcViEbt37xaDBg0SZrNZREREiIcfflhUVFTUe8758+eLa6+9VlgsFmE2m0WbNm3EfffdJ7Zv3y6EEOLgwYNiwoQJok2bNsJsNovQ0FDRp08fsXDhwiYdAyIlutLRBEK4h/V1795dGI1GAUBMnDjR871jx46JyZMni8TERKHX60V0dLTo16+fp5d8Q699vk2bNonRo0eLiIgIodfrRWJiohg9enS9/ZcvX+65biQnJ4tXXnnlou/hcppybbnUdfL7778XQ4YM8VxzrrvuOrFixQqvfequz+vWrRP33nuvCAsL84yoOnz4cJOPhc1mE7/97W9FRkaGCAkJEWazWbRv317MmDHDM5pLKSQhhJAriNDVSUlJQZcuXbBy5Uq5SyEiuiKTJk3C559/joqKCrlLCWgcWkhERBTg2GfADzmdTlyqwUaSJGi12hasiIioaVwuF1wu1yX3UdpaCGrG2wR+KCUlBSdOnGjw+wMHDuSqZUTk1yZNmuRZN6Ah/PjxHwwDfmjPnj2oqalp8PvBwcFo3759C1ZERNQ0x48frzfR0YV69erVQtXQ5TAMEBERBTh2ICQiIgpwDANEREQBjmGAiIgowDEMEBERBTiGASIiogDHMEBERBTgGAaIiIgCHMMAERFRgGMYICIiCnD/HxrcSMF2bSrDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Convert lists to sets for easy comparison\n",
    "MI_set = set(MI_extract_probes_id)\n",
    "extract_set = set(extract_probes_id)\n",
    "\n",
    "# Create the Venn diagram\n",
    "venn2([MI_set, extract_set], ('MI_probes', 'filtered_probes'))\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Venn Diagram of Probes' IDs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dic to store CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_results2 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'rb') as file:\n",
    "    CV_results2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lr-filtered-R025', 'rf-filtered-R025', 'lr-horvath', 'rf-horvath', 'svr-filtered', 'svr-horvath', 'mlp-filtered', 'mlp-horvath', 'gbdt-filtered', 'gbdt-horvath', 'cnn-horvath'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_results2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR & RF - filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 292.3923759763344\n",
      "Average R-squared: 0.2118888571002277\n",
      "Average prop10: 0.46714045436817475\n",
      "Average prop5: 0.24215581123244928\n",
      "Average MSE: 61.73598703494836\n",
      "Average R-squared: 0.8333415926116017\n",
      "Average prop10: 0.8014021060842433\n",
      "Average prop5: 0.5579948322932917\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Tissue', 'Cell type']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "    ('scaler', RobustScaler())  # You can use other scalers as well\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply different preprocessing to numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, min_samples_split=4, min_samples_leaf=3, random_state=10)\n",
    "\n",
    "# Create a linear regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Create and fit the final pipelines\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', rf_model)])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lr_model)])\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "rf_filtered_mse_list = []\n",
    "rf_filtered_r_squared_list = []\n",
    "rf_filtered_prop10_list = []\n",
    "rf_filtered_prop5_list = []\n",
    "\n",
    "lr_filtered_mse_list = []\n",
    "lr_filtered_r_squared_list = []\n",
    "lr_filtered_prop10_list = []\n",
    "lr_filtered_prop5_list = []\n",
    "\n",
    "    \n",
    "# Perform cross-validation for Linear Regression\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(lr_pipeline, X_test, y_test)\n",
    "    \n",
    "    lr_filtered_mse_list.append(mse)\n",
    "    lr_filtered_r_squared_list.append(r_squared)\n",
    "    lr_filtered_prop10_list.append(prop10)\n",
    "    lr_filtered_prop5_list.append(prop5)\n",
    "    \n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(lr_filtered_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(lr_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(lr_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(lr_filtered_prop5_list))\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(rf_pipeline, X_test, y_test)\n",
    "    \n",
    "    rf_filtered_mse_list.append(mse)\n",
    "    rf_filtered_r_squared_list.append(r_squared)\n",
    "    rf_filtered_prop10_list.append(prop10)\n",
    "    rf_filtered_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(rf_filtered_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(rf_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(rf_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(rf_filtered_prop5_list))\n",
    "\n",
    "CV_results2['lr-filtered-R025'] = {'mse': lr_filtered_mse_list,\n",
    "                                   'r_squared': lr_filtered_r_squared_list,\n",
    "                                   'prop10': lr_filtered_prop10_list,\n",
    "                                   'prop5': lr_filtered_prop5_list}\n",
    "\n",
    "CV_results2['rf-filtered-R025'] = {'mse': rf_filtered_mse_list,\n",
    "                                   'r_squared': rf_filtered_r_squared_list,\n",
    "                                   'prop10': rf_filtered_prop10_list,\n",
    "                                   'prop5': rf_filtered_prop5_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR & RF - horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 83.32123928904142\n",
      "Average R-squared: 0.7749070685288426\n",
      "Average prop10: 0.7662751072542902\n",
      "Average prop5: 0.47727769110764423\n",
      "Average MSE: 68.89830705247866\n",
      "Average R-squared: 0.8142857472577696\n",
      "Average prop10: 0.7804777691107644\n",
      "Average prop5: 0.5145895085803432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Tissue', 'Cell type']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "    ('scaler', RobustScaler())  # You can use other scalers as well\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply different preprocessing to numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, min_samples_split=4, min_samples_leaf=3, random_state=10)\n",
    "\n",
    "# Create a linear regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Create and fit the final pipelines\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', rf_model)])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lr_model)])\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "rf_horvath_mse_list = []\n",
    "rf_horvath_r_squared_list = []\n",
    "rf_horvath_prop10_list = []\n",
    "rf_horvath_prop5_list = []\n",
    "\n",
    "lr_horvath_mse_list = []\n",
    "lr_horvath_r_squared_list = []\n",
    "lr_horvath_prop10_list = []\n",
    "lr_horvath_prop5_list = []\n",
    "\n",
    "    \n",
    "# Perform cross-validation for Linear Regression\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(lr_pipeline, X_test, y_test)\n",
    "    \n",
    "    lr_horvath_mse_list.append(mse)\n",
    "    lr_horvath_r_squared_list.append(r_squared)\n",
    "    lr_horvath_prop10_list.append(prop10)\n",
    "    lr_horvath_prop5_list.append(prop5)\n",
    "    \n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(lr_horvath_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(lr_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(lr_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(lr_horvath_prop5_list))\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(rf_pipeline, X_test, y_test)\n",
    "    \n",
    "    rf_horvath_mse_list.append(mse)\n",
    "    rf_horvath_r_squared_list.append(r_squared)\n",
    "    rf_horvath_prop10_list.append(prop10)\n",
    "    rf_horvath_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(rf_horvath_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(rf_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(rf_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(rf_horvath_prop5_list))\n",
    "\n",
    "CV_results2['lr-horvath'] = {'mse': lr_horvath_mse_list,\n",
    "                                   'r_squared': lr_horvath_r_squared_list,\n",
    "                                   'prop10': lr_horvath_prop10_list,\n",
    "                                   'prop5': lr_horvath_prop5_list}\n",
    "\n",
    "CV_results2['rf-horvath'] = {'mse': rf_horvath_mse_list,\n",
    "                                   'r_squared': rf_horvath_r_squared_list,\n",
    "                                   'prop10': rf_horvath_prop10_list,\n",
    "                                   'prop5': rf_horvath_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR & RF - MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 325.92444578287945\n",
      "Average R-squared: 0.12069234117925723\n",
      "Average prop10: 0.4466870612324493\n",
      "Average prop5: 0.239350624024961\n",
      "Average MSE: 59.89949956877656\n",
      "Average R-squared: 0.8382924599675826\n",
      "Average prop10: 0.8103022620904836\n",
      "Average prop5: 0.5642384945397816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = MI_filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Tissue', 'Cell type']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "    ('scaler', RobustScaler())  # You can use other scalers as well\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply different preprocessing to numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, min_samples_split=4, min_samples_leaf=3, random_state=10)\n",
    "\n",
    "# Create a linear regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Create and fit the final pipelines\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', rf_model)])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lr_model)])\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "rf_MI_mse_list = []\n",
    "rf_MI_r_squared_list = []\n",
    "rf_MI_prop10_list = []\n",
    "rf_MI_prop5_list = []\n",
    "\n",
    "lr_MI_mse_list = []\n",
    "lr_MI_r_squared_list = []\n",
    "lr_MI_prop10_list = []\n",
    "lr_MI_prop5_list = []\n",
    "\n",
    "    \n",
    "# Perform cross-validation for Linear Regression\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(lr_pipeline, X_test, y_test)\n",
    "    \n",
    "    lr_MI_mse_list.append(mse)\n",
    "    lr_MI_r_squared_list.append(r_squared)\n",
    "    lr_MI_prop10_list.append(prop10)\n",
    "    lr_MI_prop5_list.append(prop5)\n",
    "    \n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(lr_MI_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(lr_MI_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(lr_MI_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(lr_MI_prop5_list))\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(rf_pipeline, X_test, y_test)\n",
    "    \n",
    "    rf_MI_mse_list.append(mse)\n",
    "    rf_MI_r_squared_list.append(r_squared)\n",
    "    rf_MI_prop10_list.append(prop10)\n",
    "    rf_MI_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(rf_MI_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(rf_MI_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(rf_MI_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(rf_MI_prop5_list))\n",
    "\n",
    "CV_results2['lr-MI'] = {'mse': lr_MI_mse_list,\n",
    "                        'r_squared': lr_MI_r_squared_list,\n",
    "                        'prop10': lr_MI_prop10_list,\n",
    "                        'prop5': lr_MI_prop5_list}\n",
    "\n",
    "CV_results2['rf-MI'] = {'mse': rf_MI_mse_list,\n",
    "                         'r_squared': rf_MI_r_squared_list,\n",
    "                         'prop10': rf_MI_prop10_list,\n",
    "                         'prop5': rf_MI_prop5_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR - filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 84.51871319424842\n",
      "Average R-squared: 0.7718912235778657\n",
      "Average prop10: 0.760805625975039\n",
      "Average prop5: 0.5064725526521061\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "svr_filtered_mse_list = []\n",
    "svr_filtered_r_squared_list = []\n",
    "svr_filtered_prop10_list = []\n",
    "svr_filtered_prop5_list = []\n",
    "\n",
    "# Perform cross-validation for SVM\n",
    "svm_model = SVR(kernel='rbf')  # You can choose a different kernel if needed\n",
    "svm_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                ('scaler', RobustScaler()),\n",
    "                                ('regressor', svm_model)])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(svm_pipeline, X_test, y_test)\n",
    "    \n",
    "    svr_filtered_mse_list.append(mse)\n",
    "    svr_filtered_r_squared_list.append(r_squared)\n",
    "    svr_filtered_prop10_list.append(prop10)\n",
    "    svr_filtered_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(svr_filtered_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(svr_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(svr_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(svr_filtered_prop5_list))\n",
    "\n",
    "CV_results2['svr-filtered'] = {'mse': svr_filtered_mse_list,\n",
    "                                'r_squared': svr_filtered_r_squared_list,\n",
    "                                'prop10': svr_filtered_prop10_list,\n",
    "                                'prop5': svr_filtered_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR - horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 86.37845644511474\n",
      "Average R-squared: 0.7673020609509977\n",
      "Average prop10: 0.7515927262090484\n",
      "Average prop5: 0.49163660296411854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "svr_horvath_mse_list = []\n",
    "svr_horvath_r_squared_list = []\n",
    "svr_horvath_prop10_list = []\n",
    "svr_horvath_prop5_list = []\n",
    "\n",
    "# Perform cross-validation for SVM\n",
    "svm_model = SVR(kernel='rbf')  # You can choose a different kernel if needed\n",
    "svm_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                ('scaler', RobustScaler()),\n",
    "                                ('regressor', svm_model)])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(svm_pipeline, X_test, y_test)\n",
    "    \n",
    "    svr_horvath_mse_list.append(mse)\n",
    "    svr_horvath_r_squared_list.append(r_squared)\n",
    "    svr_horvath_prop10_list.append(prop10)\n",
    "    svr_horvath_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(svr_horvath_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(svr_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(svr_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(svr_horvath_prop5_list))\n",
    "\n",
    "CV_results2['svr-horvath'] = {'mse': svr_horvath_mse_list,\n",
    "                                'r_squared': svr_horvath_r_squared_list,\n",
    "                                'prop10': svr_horvath_prop10_list,\n",
    "                                'prop5': svr_horvath_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR - MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 103.41684987230319\n",
      "Average R-squared: 0.7218117173801198\n",
      "Average prop10: 0.6663494539781591\n",
      "Average prop5: 0.37611130070202814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = MI_filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store MSE and R-squared for each fold\n",
    "svr_MI_mse_list = []\n",
    "svr_MI_r_squared_list = []\n",
    "svr_MI_prop10_list = []\n",
    "svr_MI_prop5_list = []\n",
    "\n",
    "# Perform cross-validation for SVM\n",
    "svm_model = SVR(kernel='rbf')  # You can choose a different kernel if needed\n",
    "svm_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                ('scaler', RobustScaler()),\n",
    "                                ('regressor', svm_model)])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(svm_pipeline, X_test, y_test)\n",
    "    \n",
    "    svr_MI_mse_list.append(mse)\n",
    "    svr_MI_r_squared_list.append(r_squared)\n",
    "    svr_MI_prop10_list.append(prop10)\n",
    "    svr_MI_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(svr_MI_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(svr_MI_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(svr_MI_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(svr_MI_prop5_list))\n",
    "\n",
    "CV_results2['svr-MI'] = {'mse': svr_MI_mse_list,\n",
    "                         'r_squared': svr_MI_r_squared_list,\n",
    "                         'prop10': svr_MI_prop10_list,\n",
    "                         'prop5': svr_MI_prop5_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 1007.8340\n",
      "Epoch 1: val_loss improved from inf to 139.37164, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 906.4651 - val_loss: 139.3716 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 153.0602\n",
      "Epoch 2: val_loss improved from 139.37164 to 111.91378, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 151.5364 - val_loss: 111.9138 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 118.5351\n",
      "Epoch 3: val_loss improved from 111.91378 to 96.86726, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 118.3970 - val_loss: 96.8673 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 100.4965\n",
      "Epoch 4: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 101.4829 - val_loss: 141.8111 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 92.7503\n",
      "Epoch 5: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 91.0439 - val_loss: 118.1830 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 70.7047\n",
      "Epoch 6: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 71.0924 - val_loss: 110.7783 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 61.4215\n",
      "Epoch 7: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 60.9386 - val_loss: 97.7169 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 53.8001\n",
      "Epoch 8: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.3468 - val_loss: 288.2088 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 83.0109\n",
      "Epoch 9: val_loss did not improve from 96.86726\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 80.1249 - val_loss: 256.1531 - lr: 0.0070\n",
      "Epoch 10/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 54.2105\n",
      "Epoch 10: val_loss improved from 96.86726 to 91.82819, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 53.3409 - val_loss: 91.8282 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 47.1618\n",
      "Epoch 11: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.5712 - val_loss: 101.1600 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 39.4570\n",
      "Epoch 12: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.7937 - val_loss: 187.9316 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 36.6891\n",
      "Epoch 13: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 36.8881 - val_loss: 106.6257 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 32.0627\n",
      "Epoch 14: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.5585 - val_loss: 140.1090 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.2262\n",
      "Epoch 15: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.3021 - val_loss: 93.4211 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 26.1634\n",
      "Epoch 16: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.7428 - val_loss: 122.6296 - lr: 0.0049\n",
      "Epoch 17/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 23.0666\n",
      "Epoch 17: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.8304 - val_loss: 96.0452 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 23.9507\n",
      "Epoch 18: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.9093 - val_loss: 96.5754 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 22.6806\n",
      "Epoch 19: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.6800 - val_loss: 97.4595 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 21.9244\n",
      "Epoch 20: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.2475 - val_loss: 123.8472 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.1683\n",
      "Epoch 21: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2545 - val_loss: 116.9840 - lr: 0.0034\n",
      "Epoch 22/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.4329\n",
      "Epoch 22: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6428 - val_loss: 96.6606 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.7333\n",
      "Epoch 23: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5719 - val_loss: 106.3436 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.7868\n",
      "Epoch 24: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8947 - val_loss: 109.0212 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.9042\n",
      "Epoch 25: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7777 - val_loss: 141.9409 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 16.4885\n",
      "Epoch 26: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.0693 - val_loss: 103.3230 - lr: 0.0024\n",
      "Epoch 27/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.1946\n",
      "Epoch 27: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.3239 - val_loss: 126.1455 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.0278\n",
      "Epoch 28: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8419 - val_loss: 111.2357 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.3012\n",
      "Epoch 29: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0907 - val_loss: 107.7491 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.8874\n",
      "Epoch 30: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9003 - val_loss: 124.0187 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.8493\n",
      "Epoch 31: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8577 - val_loss: 124.5749 - lr: 0.0017\n",
      "Epoch 32/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.9940\n",
      "Epoch 32: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8701 - val_loss: 105.5656 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.4624\n",
      "Epoch 33: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3974 - val_loss: 106.1788 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.6110\n",
      "Epoch 34: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6105 - val_loss: 120.1976 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.0047\n",
      "Epoch 35: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8498 - val_loss: 110.3602 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.8394\n",
      "Epoch 36: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7872 - val_loss: 109.0842 - lr: 0.0012\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.5639\n",
      "Epoch 37: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.5202 - val_loss: 105.3201 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.2990\n",
      "Epoch 38: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2094 - val_loss: 115.4435 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.2294\n",
      "Epoch 39: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0924 - val_loss: 96.9142 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.6499\n",
      "Epoch 40: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.4430 - val_loss: 134.0735 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.2510\n",
      "Epoch 41: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1920 - val_loss: 116.0792 - lr: 8.2354e-04\n",
      "Epoch 42/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.6811\n",
      "Epoch 42: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7103 - val_loss: 117.6041 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.4290\n",
      "Epoch 43: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2832 - val_loss: 106.9108 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.9150\n",
      "Epoch 44: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9297 - val_loss: 114.4891 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 9.7071\n",
      "Epoch 45: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7517 - val_loss: 109.9204 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.4183\n",
      "Epoch 46: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4328 - val_loss: 112.5416 - lr: 5.7648e-04\n",
      "Epoch 47/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.9592\n",
      "Epoch 47: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1331 - val_loss: 101.2599 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 9.3188\n",
      "Epoch 48: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3088 - val_loss: 116.2130 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.2323\n",
      "Epoch 49: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1979 - val_loss: 117.5710 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.6171\n",
      "Epoch 50: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5648 - val_loss: 114.1018 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.7199 \n",
      "Epoch 51: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5221 - val_loss: 113.6870 - lr: 4.0354e-04\n",
      "Epoch 52/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.9475\n",
      "Epoch 52: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0283 - val_loss: 115.7591 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.3896\n",
      "Epoch 53: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5182 - val_loss: 116.7067 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.1902\n",
      "Epoch 54: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0619 - val_loss: 109.1773 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.0592\n",
      "Epoch 55: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9598 - val_loss: 114.2378 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 8.7403\n",
      "Epoch 56: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7420 - val_loss: 113.6373 - lr: 2.8248e-04\n",
      "Epoch 57/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.5473\n",
      "Epoch 57: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6772 - val_loss: 109.6849 - lr: 2.8248e-04\n",
      "Epoch 58/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.4093\n",
      "Epoch 58: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3677 - val_loss: 112.0744 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.3433\n",
      "Epoch 59: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2024 - val_loss: 115.8334 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 7.9143\n",
      "Epoch 60: val_loss did not improve from 91.82819\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.9746 - val_loss: 106.3248 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 1930.2657\n",
      "Epoch 1: val_loss improved from inf to 186.58063, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 1930.2657 - val_loss: 186.5806 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 171.1750\n",
      "Epoch 2: val_loss improved from 186.58063 to 105.80479, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 169.2247 - val_loss: 105.8048 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 120.8587\n",
      "Epoch 3: val_loss did not improve from 105.80479\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 120.5327 - val_loss: 119.6517 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 103.0110\n",
      "Epoch 4: val_loss did not improve from 105.80479\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 104.5861 - val_loss: 206.9482 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 93.6101\n",
      "Epoch 5: val_loss did not improve from 105.80479\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 94.1845 - val_loss: 189.3181 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 103.5333\n",
      "Epoch 6: val_loss improved from 105.80479 to 90.43485, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.1076 - val_loss: 90.4349 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 77.1553\n",
      "Epoch 7: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.7920 - val_loss: 120.3111 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 75.6558\n",
      "Epoch 8: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 74.3216 - val_loss: 144.5157 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 60.0093\n",
      "Epoch 9: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 60.5094 - val_loss: 126.0585 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 67.4558\n",
      "Epoch 10: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.4498 - val_loss: 99.8140 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 53.3893\n",
      "Epoch 11: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.6795 - val_loss: 197.0187 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 48.5408\n",
      "Epoch 12: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.2928 - val_loss: 148.9834 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 41.2461\n",
      "Epoch 13: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.2416 - val_loss: 130.5491 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 42.7153\n",
      "Epoch 14: val_loss did not improve from 90.43485\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.6714 - val_loss: 100.3870 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 38.2335\n",
      "Epoch 15: val_loss improved from 90.43485 to 88.72528, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.2335 - val_loss: 88.7253 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 41.5702\n",
      "Epoch 16: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.6169 - val_loss: 89.2201 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 34.8257\n",
      "Epoch 17: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.1877 - val_loss: 117.2357 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 33.7714\n",
      "Epoch 18: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.8242 - val_loss: 99.8143 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.8659\n",
      "Epoch 19: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.8560 - val_loss: 101.0835 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 28.8892\n",
      "Epoch 20: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.7598 - val_loss: 89.7948 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 30.3296\n",
      "Epoch 21: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.3545 - val_loss: 101.4780 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 23.6533\n",
      "Epoch 22: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.9265 - val_loss: 115.8633 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 25.4963\n",
      "Epoch 23: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.6671 - val_loss: 129.7103 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 23.5142\n",
      "Epoch 24: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.2512 - val_loss: 93.8563 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 20.0917\n",
      "Epoch 25: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.1565 - val_loss: 95.1746 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 19.5189\n",
      "Epoch 26: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.2394 - val_loss: 121.3291 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.6314\n",
      "Epoch 27: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.8135 - val_loss: 100.0232 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.0098\n",
      "Epoch 28: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2425 - val_loss: 127.2577 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.5203\n",
      "Epoch 29: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2338 - val_loss: 101.1375 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 17.4300\n",
      "Epoch 30: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.1764 - val_loss: 113.4936 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.8751\n",
      "Epoch 31: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7359 - val_loss: 99.0941 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 15.5137\n",
      "Epoch 32: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6291 - val_loss: 110.8433 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.3629\n",
      "Epoch 33: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.3629 - val_loss: 110.3737 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.1307\n",
      "Epoch 34: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.1538 - val_loss: 108.7671 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.2881\n",
      "Epoch 35: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7040 - val_loss: 100.0374 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.7502\n",
      "Epoch 36: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7415 - val_loss: 99.9490 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.0774\n",
      "Epoch 37: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8210 - val_loss: 103.3295 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.5608\n",
      "Epoch 38: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5115 - val_loss: 102.4734 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 12.7819\n",
      "Epoch 39: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8652 - val_loss: 114.2263 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.6309\n",
      "Epoch 40: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7482 - val_loss: 99.1941 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.5520\n",
      "Epoch 41: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5520 - val_loss: 103.5444 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.0464\n",
      "Epoch 42: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0440 - val_loss: 104.7738 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.1327\n",
      "Epoch 43: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.1426 - val_loss: 101.5945 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.7118\n",
      "Epoch 44: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9154 - val_loss: 106.9970 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 11.7946\n",
      "Epoch 45: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7946 - val_loss: 106.9336 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.6390\n",
      "Epoch 46: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4143 - val_loss: 117.7679 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.7837\n",
      "Epoch 47: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7800 - val_loss: 114.2439 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 10.8904\n",
      "Epoch 48: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9319 - val_loss: 106.1851 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.9440\n",
      "Epoch 49: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.0390 - val_loss: 102.5474 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.0994\n",
      "Epoch 50: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0170 - val_loss: 109.8329 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.6577\n",
      "Epoch 51: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6577 - val_loss: 104.3116 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.9552\n",
      "Epoch 52: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7967 - val_loss: 102.8170 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 10.2608\n",
      "Epoch 53: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3099 - val_loss: 112.8396 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.3683\n",
      "Epoch 54: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3433 - val_loss: 115.2158 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.0170\n",
      "Epoch 55: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9460 - val_loss: 106.7656 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.8547 \n",
      "Epoch 56: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9571 - val_loss: 106.2990 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.2323\n",
      "Epoch 57: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3020 - val_loss: 105.8100 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.5239\n",
      "Epoch 58: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5897 - val_loss: 107.6450 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.7477\n",
      "Epoch 59: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6968 - val_loss: 107.4345 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.3871\n",
      "Epoch 60: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4156 - val_loss: 107.0761 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.1107\n",
      "Epoch 61: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0000 - val_loss: 113.3027 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 9.8499\n",
      "Epoch 62: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7843 - val_loss: 106.6044 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 9.4442\n",
      "Epoch 63: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4400 - val_loss: 107.0597 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.2118\n",
      "Epoch 64: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.9980 - val_loss: 106.4765 - lr: 2.8248e-04\n",
      "Epoch 65/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 10.2759\n",
      "Epoch 65: val_loss did not improve from 88.72528\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2709 - val_loss: 101.1767 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 2175.3193\n",
      "Epoch 1: val_loss improved from inf to 282.85339, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 1978.4081 - val_loss: 282.8534 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 173.2789\n",
      "Epoch 2: val_loss improved from 282.85339 to 204.33177, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 175.0314 - val_loss: 204.3318 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 140.1313\n",
      "Epoch 3: val_loss improved from 204.33177 to 111.41823, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 139.4764 - val_loss: 111.4182 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 107.4095\n",
      "Epoch 4: val_loss did not improve from 111.41823\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 107.8634 - val_loss: 117.4247 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 84.7639\n",
      "Epoch 5: val_loss did not improve from 111.41823\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 84.4304 - val_loss: 181.5333 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 76.3695\n",
      "Epoch 6: val_loss did not improve from 111.41823\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 75.7372 - val_loss: 119.1459 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 65.9483\n",
      "Epoch 7: val_loss improved from 111.41823 to 87.29147, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.3238 - val_loss: 87.2915 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 57.2799\n",
      "Epoch 8: val_loss did not improve from 87.29147\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 58.4041 - val_loss: 112.0694 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 63.7798\n",
      "Epoch 9: val_loss improved from 87.29147 to 82.70042, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 63.6490 - val_loss: 82.7004 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 62.0797\n",
      "Epoch 10: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 65.4300 - val_loss: 106.7902 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 51.1226\n",
      "Epoch 11: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.1617 - val_loss: 102.9956 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 47.5967\n",
      "Epoch 12: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 48.0224 - val_loss: 113.1268 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 41.1034\n",
      "Epoch 13: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.8099 - val_loss: 124.5530 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 44.7943\n",
      "Epoch 14: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.1471 - val_loss: 102.4285 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 35.4449\n",
      "Epoch 15: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.9846 - val_loss: 119.1180 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 29.6659\n",
      "Epoch 16: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.6499 - val_loss: 111.6699 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 29.9248\n",
      "Epoch 17: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.9362 - val_loss: 112.9818 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 28.1685\n",
      "Epoch 18: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.1482 - val_loss: 94.4460 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 25.7279\n",
      "Epoch 19: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.6581 - val_loss: 96.0161 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 22.6045\n",
      "Epoch 20: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.7686 - val_loss: 127.7844 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 24.6331\n",
      "Epoch 21: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.4617 - val_loss: 127.8123 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 20.8491\n",
      "Epoch 22: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.5591 - val_loss: 101.7112 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 21.4969\n",
      "Epoch 23: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.6192 - val_loss: 98.5200 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 21.9627\n",
      "Epoch 24: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.9627 - val_loss: 125.6092 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 17.3175\n",
      "Epoch 25: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.2345 - val_loss: 116.4607 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 18.4057\n",
      "Epoch 26: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.0484 - val_loss: 123.0001 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 16.5005\n",
      "Epoch 27: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5760 - val_loss: 110.3447 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.9240\n",
      "Epoch 28: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.9188 - val_loss: 120.6818 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.9317\n",
      "Epoch 29: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.0017 - val_loss: 111.9805 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.2587\n",
      "Epoch 30: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0419 - val_loss: 129.8329 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 13.9962\n",
      "Epoch 31: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8444 - val_loss: 121.0575 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.8001\n",
      "Epoch 32: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.6816 - val_loss: 141.5588 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 13.7449\n",
      "Epoch 33: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7706 - val_loss: 118.5578 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.8622\n",
      "Epoch 34: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8622 - val_loss: 123.5242 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 14.0222\n",
      "Epoch 35: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7102 - val_loss: 109.6380 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.6337\n",
      "Epoch 36: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4139 - val_loss: 104.4230 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.6696\n",
      "Epoch 37: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6696 - val_loss: 113.2938 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 12.3572\n",
      "Epoch 38: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7587 - val_loss: 115.2371 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.6726\n",
      "Epoch 39: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6297 - val_loss: 108.6171 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.5552\n",
      "Epoch 40: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3982 - val_loss: 131.1724 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.4901\n",
      "Epoch 41: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3923 - val_loss: 113.8545 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.5610\n",
      "Epoch 42: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7433 - val_loss: 116.0009 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.3758\n",
      "Epoch 43: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2011 - val_loss: 116.0056 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.0941\n",
      "Epoch 44: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9907 - val_loss: 110.7554 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 10.2405\n",
      "Epoch 45: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2149 - val_loss: 120.5362 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.3883\n",
      "Epoch 46: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3494 - val_loss: 114.9827 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.5721\n",
      "Epoch 47: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5721 - val_loss: 106.6504 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.1874\n",
      "Epoch 48: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0111 - val_loss: 123.2783 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.3264\n",
      "Epoch 49: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1401 - val_loss: 109.1837 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.7465\n",
      "Epoch 50: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7529 - val_loss: 120.4598 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.2596\n",
      "Epoch 51: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1538 - val_loss: 109.7556 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.6247\n",
      "Epoch 52: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7565 - val_loss: 106.8451 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.6984\n",
      "Epoch 53: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5195 - val_loss: 113.3221 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.6670\n",
      "Epoch 54: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.5073 - val_loss: 126.2442 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.0624\n",
      "Epoch 55: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0895 - val_loss: 119.8227 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.0926\n",
      "Epoch 56: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0623 - val_loss: 123.0040 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 8.9834\n",
      "Epoch 57: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7835 - val_loss: 115.6015 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.5341\n",
      "Epoch 58: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3542 - val_loss: 114.7456 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.9842\n",
      "Epoch 59: val_loss did not improve from 82.70042\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9550 - val_loss: 116.7756 - lr: 4.0354e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 784.1361\n",
      "Epoch 1: val_loss improved from inf to 305.24490, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 756.3111 - val_loss: 305.2449 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 189.2833\n",
      "Epoch 2: val_loss improved from 305.24490 to 111.06656, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 185.2358 - val_loss: 111.0666 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 114.9772\n",
      "Epoch 3: val_loss did not improve from 111.06656\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 113.2510 - val_loss: 140.5177 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 93.8411\n",
      "Epoch 4: val_loss did not improve from 111.06656\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 100.9397 - val_loss: 178.1815 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 110.8863\n",
      "Epoch 5: val_loss did not improve from 111.06656\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 110.8499 - val_loss: 171.2161 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 74.7351\n",
      "Epoch 6: val_loss improved from 111.06656 to 74.76797, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 74.9611 - val_loss: 74.7680 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 69.0592\n",
      "Epoch 7: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 68.1448 - val_loss: 84.6348 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 63.1193\n",
      "Epoch 8: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.6589 - val_loss: 150.4456 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 58.6477\n",
      "Epoch 9: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 58.5991 - val_loss: 92.1295 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 44.2519\n",
      "Epoch 10: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.9574 - val_loss: 114.3335 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 51.5367\n",
      "Epoch 11: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.5442 - val_loss: 166.1201 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 46.0392\n",
      "Epoch 12: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 44.6289 - val_loss: 84.5985 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 33.5177\n",
      "Epoch 13: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.5177 - val_loss: 98.7459 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 35.9940\n",
      "Epoch 14: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.0401 - val_loss: 96.6355 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 27.9073\n",
      "Epoch 15: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.5595 - val_loss: 132.6217 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 34.8450\n",
      "Epoch 16: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.8014 - val_loss: 96.6689 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 22.8577\n",
      "Epoch 17: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.8721 - val_loss: 108.8703 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 22.0621\n",
      "Epoch 18: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.9234 - val_loss: 99.3996 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 21.3178\n",
      "Epoch 19: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.4789 - val_loss: 100.6008 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 19.6936\n",
      "Epoch 20: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8372 - val_loss: 110.8215 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 20.0197\n",
      "Epoch 21: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.1826 - val_loss: 109.7430 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 19.9430\n",
      "Epoch 22: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.9397 - val_loss: 95.0458 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.4452\n",
      "Epoch 23: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4582 - val_loss: 109.9478 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 18.2586\n",
      "Epoch 24: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2663 - val_loss: 104.4520 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 19.6815\n",
      "Epoch 25: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.3041 - val_loss: 113.5516 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.7127\n",
      "Epoch 26: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.7592 - val_loss: 99.4368 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 18.2725\n",
      "Epoch 27: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.8430 - val_loss: 111.9167 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 13.7934\n",
      "Epoch 28: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8032 - val_loss: 111.8245 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 15.1155\n",
      "Epoch 29: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0849 - val_loss: 115.2718 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 17.1383\n",
      "Epoch 30: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5185 - val_loss: 115.0811 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 14.1691\n",
      "Epoch 31: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.9841 - val_loss: 106.0014 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.5898\n",
      "Epoch 32: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1312 - val_loss: 104.1885 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.4855\n",
      "Epoch 33: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5946 - val_loss: 104.4113 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.9575\n",
      "Epoch 34: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0114 - val_loss: 100.4915 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 13.1341\n",
      "Epoch 35: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0543 - val_loss: 100.1424 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 13.0343\n",
      "Epoch 36: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0258 - val_loss: 99.1934 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.9778\n",
      "Epoch 37: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9245 - val_loss: 103.0320 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 11.4975\n",
      "Epoch 38: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5143 - val_loss: 102.2879 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.7198\n",
      "Epoch 39: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5699 - val_loss: 115.1244 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 12.1884\n",
      "Epoch 40: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.1965 - val_loss: 103.0012 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 11.5988\n",
      "Epoch 41: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6218 - val_loss: 97.7626 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.7698\n",
      "Epoch 42: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7750 - val_loss: 105.4540 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.9106\n",
      "Epoch 43: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9207 - val_loss: 96.8635 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.2728\n",
      "Epoch 44: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3547 - val_loss: 98.9439 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.8949\n",
      "Epoch 45: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8622 - val_loss: 97.2164 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.0236\n",
      "Epoch 46: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0207 - val_loss: 98.3419 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.2980\n",
      "Epoch 47: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4422 - val_loss: 96.0957 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.7719\n",
      "Epoch 48: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.6431 - val_loss: 106.1247 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.2255\n",
      "Epoch 49: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2905 - val_loss: 100.3512 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.5389\n",
      "Epoch 50: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4590 - val_loss: 99.5311 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.5407\n",
      "Epoch 51: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5678 - val_loss: 101.5830 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.3211\n",
      "Epoch 52: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3211 - val_loss: 101.2681 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 9.4759\n",
      "Epoch 53: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3611 - val_loss: 101.9743 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.5060\n",
      "Epoch 54: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6970 - val_loss: 100.4233 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 9.2589\n",
      "Epoch 55: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2385 - val_loss: 104.6685 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.1550\n",
      "Epoch 56: val_loss did not improve from 74.76797\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1111 - val_loss: 98.5424 - lr: 4.0354e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 2639.9351\n",
      "Epoch 1: val_loss improved from inf to 190.44489, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 2501.3726 - val_loss: 190.4449 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 195.7035\n",
      "Epoch 2: val_loss improved from 190.44489 to 89.51991, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 188.9122 - val_loss: 89.5199 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 127.5904\n",
      "Epoch 3: val_loss did not improve from 89.51991\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 127.6542 - val_loss: 123.4277 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 131.7606\n",
      "Epoch 4: val_loss did not improve from 89.51991\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 129.4182 - val_loss: 91.6810 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 102.4309\n",
      "Epoch 5: val_loss did not improve from 89.51991\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.4309 - val_loss: 103.6882 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 109.4402\n",
      "Epoch 6: val_loss did not improve from 89.51991\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 108.7198 - val_loss: 189.9278 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 100.8478\n",
      "Epoch 7: val_loss did not improve from 89.51991\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 98.3826 - val_loss: 111.8652 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 72.6831\n",
      "Epoch 8: val_loss improved from 89.51991 to 69.73891, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 72.0468 - val_loss: 69.7389 - lr: 0.0070\n",
      "Epoch 9/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 57.0074\n",
      "Epoch 9: val_loss did not improve from 69.73891\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.9818 - val_loss: 118.2555 - lr: 0.0070\n",
      "Epoch 10/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 56.9721\n",
      "Epoch 10: val_loss improved from 69.73891 to 66.60030, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 56.8757 - val_loss: 66.6003 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 51.5631\n",
      "Epoch 11: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.5631 - val_loss: 138.5498 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 48.8794\n",
      "Epoch 12: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.8300 - val_loss: 83.0295 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 48.7260\n",
      "Epoch 13: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 48.2069 - val_loss: 81.8485 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 43.1569\n",
      "Epoch 14: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 43.4200 - val_loss: 72.2445 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 42.6546\n",
      "Epoch 15: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.5286 - val_loss: 82.6509 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 35.3376\n",
      "Epoch 16: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.3162 - val_loss: 84.4814 - lr: 0.0049\n",
      "Epoch 17/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 31.4840\n",
      "Epoch 17: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.6192 - val_loss: 83.4801 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 36.1888\n",
      "Epoch 18: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.6254 - val_loss: 92.0422 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 29.0964\n",
      "Epoch 19: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.1845 - val_loss: 74.9008 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 32.4335\n",
      "Epoch 20: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.8083 - val_loss: 85.6219 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 27.1379\n",
      "Epoch 21: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.0608 - val_loss: 104.9675 - lr: 0.0034\n",
      "Epoch 22/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 25.0378\n",
      "Epoch 22: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.9928 - val_loss: 78.8060 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 26.0813\n",
      "Epoch 23: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.8618 - val_loss: 93.5529 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 24.6613\n",
      "Epoch 24: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.6539 - val_loss: 84.0066 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 23.0666\n",
      "Epoch 25: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.4299 - val_loss: 103.8113 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 20.7159\n",
      "Epoch 26: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.6736 - val_loss: 113.0389 - lr: 0.0024\n",
      "Epoch 27/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 20.6821\n",
      "Epoch 27: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.3349 - val_loss: 89.7766 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 19.0111\n",
      "Epoch 28: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1292 - val_loss: 89.4817 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.0108\n",
      "Epoch 29: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1997 - val_loss: 94.0726 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.5731\n",
      "Epoch 30: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.3433 - val_loss: 91.6952 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 17.7398\n",
      "Epoch 31: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7398 - val_loss: 118.8230 - lr: 0.0017\n",
      "Epoch 32/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 17.8314\n",
      "Epoch 32: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.0133 - val_loss: 81.2162 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 18.0154\n",
      "Epoch 33: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7431 - val_loss: 103.0876 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.2219\n",
      "Epoch 34: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1796 - val_loss: 88.7756 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 16.2792\n",
      "Epoch 35: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1442 - val_loss: 100.8064 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.6922\n",
      "Epoch 36: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8096 - val_loss: 93.3559 - lr: 0.0012\n",
      "Epoch 37/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 15.0999\n",
      "Epoch 37: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0576 - val_loss: 96.1749 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.1001\n",
      "Epoch 38: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2112 - val_loss: 102.1312 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 14.8052\n",
      "Epoch 39: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7533 - val_loss: 105.6861 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.9493\n",
      "Epoch 40: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1356 - val_loss: 88.2118 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 13.7735\n",
      "Epoch 41: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7309 - val_loss: 115.2144 - lr: 8.2354e-04\n",
      "Epoch 42/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.0821\n",
      "Epoch 42: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.9242 - val_loss: 101.9839 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 14.0359\n",
      "Epoch 43: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0724 - val_loss: 89.3342 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.5300\n",
      "Epoch 44: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5346 - val_loss: 108.4672 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.7027\n",
      "Epoch 45: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8133 - val_loss: 97.3870 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.6510\n",
      "Epoch 46: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.5786 - val_loss: 97.1595 - lr: 5.7648e-04\n",
      "Epoch 47/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.5142\n",
      "Epoch 47: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5579 - val_loss: 97.4733 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.6287\n",
      "Epoch 48: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6287 - val_loss: 97.6133 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.9782\n",
      "Epoch 49: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1011 - val_loss: 98.9195 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 12.3201\n",
      "Epoch 50: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2757 - val_loss: 91.2255 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.7483\n",
      "Epoch 51: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5907 - val_loss: 94.0434 - lr: 4.0354e-04\n",
      "Epoch 52/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.8702\n",
      "Epoch 52: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1219 - val_loss: 97.3824 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 12.3451\n",
      "Epoch 53: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3246 - val_loss: 101.4336 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.9914\n",
      "Epoch 54: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9711 - val_loss: 104.4815 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 12.5271\n",
      "Epoch 55: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4260 - val_loss: 93.5240 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.5625\n",
      "Epoch 56: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5005 - val_loss: 95.8940 - lr: 2.8248e-04\n",
      "Epoch 57/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.1708\n",
      "Epoch 57: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9565 - val_loss: 100.6117 - lr: 2.8248e-04\n",
      "Epoch 58/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.6993\n",
      "Epoch 58: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6558 - val_loss: 90.3934 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.0356\n",
      "Epoch 59: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1022 - val_loss: 103.4808 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.1090\n",
      "Epoch 60: val_loss did not improve from 66.60030\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3050 - val_loss: 96.9667 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 1187.2540\n",
      "Epoch 1: val_loss improved from inf to 151.84897, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 1187.2540 - val_loss: 151.8490 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 162.6015\n",
      "Epoch 2: val_loss improved from 151.84897 to 126.81728, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 162.3593 - val_loss: 126.8173 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 132.2161\n",
      "Epoch 3: val_loss improved from 126.81728 to 114.46559, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 132.2161 - val_loss: 114.4656 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 99.5234 \n",
      "Epoch 4: val_loss improved from 114.46559 to 106.72146, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 97.5500 - val_loss: 106.7215 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 82.4072\n",
      "Epoch 5: val_loss improved from 106.72146 to 90.23104, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 82.4082 - val_loss: 90.2310 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 79.1159\n",
      "Epoch 6: val_loss did not improve from 90.23104\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.1336 - val_loss: 99.3921 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 65.3281\n",
      "Epoch 7: val_loss did not improve from 90.23104\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.3526 - val_loss: 210.6345 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 88.6889\n",
      "Epoch 8: val_loss improved from 90.23104 to 83.39391, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 86.1121 - val_loss: 83.3939 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 60.5988\n",
      "Epoch 9: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 59.6137 - val_loss: 116.5570 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 50.0405\n",
      "Epoch 10: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 50.2295 - val_loss: 106.1816 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 52.1487\n",
      "Epoch 11: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 53.2840 - val_loss: 122.5850 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 68.9110\n",
      "Epoch 12: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 64.9331 - val_loss: 109.2249 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 45.0507\n",
      "Epoch 13: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 43.9688 - val_loss: 112.5179 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 34.8672\n",
      "Epoch 14: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.8424 - val_loss: 97.4309 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 31.0039\n",
      "Epoch 15: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.5743 - val_loss: 121.2500 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 31.9603\n",
      "Epoch 16: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.9603 - val_loss: 101.5118 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 26.4634\n",
      "Epoch 17: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.4634 - val_loss: 115.1391 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 27.9169\n",
      "Epoch 18: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.3527 - val_loss: 89.5585 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 23.2402\n",
      "Epoch 19: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.1168 - val_loss: 124.5323 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 24.3005\n",
      "Epoch 20: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0772 - val_loss: 119.1350 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 21.6605\n",
      "Epoch 21: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.9467 - val_loss: 116.0656 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 22.0743\n",
      "Epoch 22: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7530 - val_loss: 121.1983 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 20.9712\n",
      "Epoch 23: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9506 - val_loss: 111.3208 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 18.3615\n",
      "Epoch 24: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.5750 - val_loss: 96.8885 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 17.9891\n",
      "Epoch 25: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7367 - val_loss: 97.1567 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 16.8095\n",
      "Epoch 26: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.8549 - val_loss: 97.5089 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 18.9314\n",
      "Epoch 27: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.8935 - val_loss: 123.7114 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 16.6222\n",
      "Epoch 28: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5731 - val_loss: 100.8713 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 16.4733\n",
      "Epoch 29: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3034 - val_loss: 118.4569 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 16.7053\n",
      "Epoch 30: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1513 - val_loss: 107.6801 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 14.8497\n",
      "Epoch 31: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.4792 - val_loss: 104.0222 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.0280\n",
      "Epoch 32: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4000 - val_loss: 103.8601 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.9326\n",
      "Epoch 33: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.7806 - val_loss: 108.3006 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 13.3526\n",
      "Epoch 34: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6322 - val_loss: 95.5221 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 13.0772\n",
      "Epoch 35: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2008 - val_loss: 106.6377 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.4266\n",
      "Epoch 36: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.3125 - val_loss: 101.7709 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.7164\n",
      "Epoch 37: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0172 - val_loss: 108.7872 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 13.1771\n",
      "Epoch 38: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9273 - val_loss: 115.4528 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 11.6969\n",
      "Epoch 39: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7416 - val_loss: 100.3535 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.8929\n",
      "Epoch 40: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0424 - val_loss: 106.2255 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 11.8408\n",
      "Epoch 41: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6679 - val_loss: 108.9800 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.5385\n",
      "Epoch 42: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5494 - val_loss: 114.3847 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.7066\n",
      "Epoch 43: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7316 - val_loss: 106.0621 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.6187\n",
      "Epoch 44: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5030 - val_loss: 108.4688 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.0581\n",
      "Epoch 45: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1483 - val_loss: 108.5738 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.0241\n",
      "Epoch 46: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.0316 - val_loss: 106.6923 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.2271\n",
      "Epoch 47: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7416 - val_loss: 103.8401 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.5958\n",
      "Epoch 48: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5797 - val_loss: 106.3580 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.4507\n",
      "Epoch 49: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4511 - val_loss: 102.0189 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.7814\n",
      "Epoch 50: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7874 - val_loss: 105.6236 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.7610\n",
      "Epoch 51: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9422 - val_loss: 104.3609 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.7399\n",
      "Epoch 52: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6268 - val_loss: 111.3592 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.7390\n",
      "Epoch 53: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.8022 - val_loss: 114.4680 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.0316\n",
      "Epoch 54: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9191 - val_loss: 111.5731 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.7759\n",
      "Epoch 55: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7272 - val_loss: 109.2646 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.4996\n",
      "Epoch 56: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4922 - val_loss: 103.8909 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 9.4860\n",
      "Epoch 57: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4860 - val_loss: 110.4171 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.5220\n",
      "Epoch 58: val_loss did not improve from 83.39391\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5268 - val_loss: 107.4667 - lr: 4.0354e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 2141.9724\n",
      "Epoch 1: val_loss improved from inf to 158.70137, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 2117.7571 - val_loss: 158.7014 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 160.6330\n",
      "Epoch 2: val_loss improved from 158.70137 to 136.21921, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 159.6947 - val_loss: 136.2192 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 127.2857\n",
      "Epoch 3: val_loss improved from 136.21921 to 112.61650, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 127.2857 - val_loss: 112.6165 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 111.6839\n",
      "Epoch 4: val_loss improved from 112.61650 to 82.29196, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 107.9553 - val_loss: 82.2920 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 77.1946\n",
      "Epoch 5: val_loss did not improve from 82.29196\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 77.3857 - val_loss: 173.2305 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 83.3808\n",
      "Epoch 6: val_loss improved from 82.29196 to 74.94497, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 82.6818 - val_loss: 74.9450 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 82.2858\n",
      "Epoch 7: val_loss did not improve from 74.94497\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.9136 - val_loss: 133.8339 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 60.0470\n",
      "Epoch 8: val_loss did not improve from 74.94497\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.0445 - val_loss: 110.5378 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 57.1019\n",
      "Epoch 9: val_loss did not improve from 74.94497\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.9394 - val_loss: 80.1611 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 63.7076\n",
      "Epoch 10: val_loss did not improve from 74.94497\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.5222 - val_loss: 106.4848 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 56.1301\n",
      "Epoch 11: val_loss did not improve from 74.94497\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.1001 - val_loss: 109.8668 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 42.0896\n",
      "Epoch 12: val_loss improved from 74.94497 to 71.39770, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 41.9387 - val_loss: 71.3977 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 38.5364\n",
      "Epoch 13: val_loss improved from 71.39770 to 69.91261, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.7335 - val_loss: 69.9126 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 42.1385\n",
      "Epoch 14: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.7674 - val_loss: 85.1248 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 34.8011\n",
      "Epoch 15: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.3537 - val_loss: 93.6779 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 33.1152\n",
      "Epoch 16: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.9143 - val_loss: 85.1491 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 38.5133\n",
      "Epoch 17: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.0791 - val_loss: 86.6444 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 33.4702\n",
      "Epoch 18: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.1252 - val_loss: 90.5714 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 27.1116\n",
      "Epoch 19: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.0051 - val_loss: 157.3859 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 26.6880\n",
      "Epoch 20: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.6923 - val_loss: 81.7933 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 23.3615\n",
      "Epoch 21: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.3028 - val_loss: 131.6048 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 28.0969\n",
      "Epoch 22: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.0932 - val_loss: 87.0986 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 24.0452\n",
      "Epoch 23: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.8904 - val_loss: 92.5492 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 21.1221\n",
      "Epoch 24: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.4792 - val_loss: 82.7947 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 21.6142\n",
      "Epoch 25: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.4590 - val_loss: 75.5556 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 21.3571\n",
      "Epoch 26: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9288 - val_loss: 81.0305 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 21.5543\n",
      "Epoch 27: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.2026 - val_loss: 122.2827 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 21.1049\n",
      "Epoch 28: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.8870 - val_loss: 92.3542 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 17.4490\n",
      "Epoch 29: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6286 - val_loss: 106.4724 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.7340\n",
      "Epoch 30: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7336 - val_loss: 96.0173 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.7200\n",
      "Epoch 31: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6510 - val_loss: 103.0777 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 16.6156\n",
      "Epoch 32: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6156 - val_loss: 95.4077 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.7441\n",
      "Epoch 33: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5604 - val_loss: 97.4844 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.9907\n",
      "Epoch 34: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.6416 - val_loss: 98.5985 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.0655\n",
      "Epoch 35: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.1014 - val_loss: 113.3195 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 13.9764\n",
      "Epoch 36: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.9766 - val_loss: 87.8713 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 15.1141\n",
      "Epoch 37: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8910 - val_loss: 87.6858 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 13.6044\n",
      "Epoch 38: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6477 - val_loss: 97.5859 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 13.6535\n",
      "Epoch 39: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5857 - val_loss: 96.7610 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 12.3870\n",
      "Epoch 40: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5304 - val_loss: 94.7876 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.5159\n",
      "Epoch 41: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6027 - val_loss: 89.5377 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 13.0294\n",
      "Epoch 42: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.0766 - val_loss: 83.3813 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 12.2110\n",
      "Epoch 43: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1934 - val_loss: 99.4444 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 12.7988\n",
      "Epoch 44: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5110 - val_loss: 92.2348 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 11.2741\n",
      "Epoch 45: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2761 - val_loss: 99.3347 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.4196\n",
      "Epoch 46: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.4053 - val_loss: 86.4897 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.6832\n",
      "Epoch 47: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.6708 - val_loss: 89.1366 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.2011\n",
      "Epoch 48: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2143 - val_loss: 99.6091 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.8357\n",
      "Epoch 49: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7715 - val_loss: 90.4135 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.1119\n",
      "Epoch 50: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9223 - val_loss: 102.6352 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.1046\n",
      "Epoch 51: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9346 - val_loss: 87.9530 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.6784\n",
      "Epoch 52: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6148 - val_loss: 97.3480 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 10.6386\n",
      "Epoch 53: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6046 - val_loss: 94.6056 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.9986\n",
      "Epoch 54: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9403 - val_loss: 96.6946 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.3057\n",
      "Epoch 55: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3711 - val_loss: 89.1711 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.6952\n",
      "Epoch 56: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6279 - val_loss: 96.5565 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.2761\n",
      "Epoch 57: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3464 - val_loss: 97.5039 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.6832\n",
      "Epoch 58: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.5786 - val_loss: 98.8384 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.2198\n",
      "Epoch 59: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1047 - val_loss: 95.2027 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.8991\n",
      "Epoch 60: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.8610 - val_loss: 95.4388 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 10.5228\n",
      "Epoch 61: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5148 - val_loss: 94.5038 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.0633\n",
      "Epoch 62: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9530 - val_loss: 93.0976 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.7703\n",
      "Epoch 63: val_loss did not improve from 69.91261\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.7889 - val_loss: 93.8352 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 2508.5205\n",
      "Epoch 1: val_loss improved from inf to 179.88351, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 2456.2439 - val_loss: 179.8835 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 179.4584\n",
      "Epoch 2: val_loss did not improve from 179.88351\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 178.1308 - val_loss: 274.5274 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 137.1720\n",
      "Epoch 3: val_loss improved from 179.88351 to 160.18919, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 132.9441 - val_loss: 160.1892 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 109.0983\n",
      "Epoch 4: val_loss improved from 160.18919 to 109.39082, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 106.3689 - val_loss: 109.3908 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 87.9237\n",
      "Epoch 5: val_loss did not improve from 109.39082\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 86.9057 - val_loss: 165.5434 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 83.4079\n",
      "Epoch 6: val_loss did not improve from 109.39082\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 83.4318 - val_loss: 150.5654 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 90.5159\n",
      "Epoch 7: val_loss did not improve from 109.39082\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 90.5159 - val_loss: 226.6133 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 78.2340\n",
      "Epoch 8: val_loss did not improve from 109.39082\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 78.3383 - val_loss: 184.0288 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 72.5506\n",
      "Epoch 9: val_loss did not improve from 109.39082\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 71.3563 - val_loss: 127.5144 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 55.6302\n",
      "Epoch 10: val_loss improved from 109.39082 to 93.92181, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 55.4229 - val_loss: 93.9218 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 46.5998\n",
      "Epoch 11: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.5998 - val_loss: 132.3323 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 44.5191\n",
      "Epoch 12: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.3498 - val_loss: 95.4266 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 49.6388\n",
      "Epoch 13: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 49.8977 - val_loss: 111.5856 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 44.1786\n",
      "Epoch 14: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 44.2922 - val_loss: 108.6282 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 45.7879\n",
      "Epoch 15: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.3981 - val_loss: 103.8287 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 32.8385\n",
      "Epoch 16: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.4934 - val_loss: 114.8432 - lr: 0.0049\n",
      "Epoch 17/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 28.7375\n",
      "Epoch 17: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.4786 - val_loss: 99.2911 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 26.8420\n",
      "Epoch 18: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.1027 - val_loss: 127.2673 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 28.6934\n",
      "Epoch 19: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.3976 - val_loss: 117.0952 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 25.3177\n",
      "Epoch 20: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.2323 - val_loss: 117.4590 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 24.2788\n",
      "Epoch 21: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0215 - val_loss: 118.0761 - lr: 0.0034\n",
      "Epoch 22/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 22.0194\n",
      "Epoch 22: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.0098 - val_loss: 97.5627 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 21.6132\n",
      "Epoch 23: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.5383 - val_loss: 107.2168 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 22.9453\n",
      "Epoch 24: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.4547 - val_loss: 128.8699 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 19.5817\n",
      "Epoch 25: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8177 - val_loss: 120.9079 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.7190\n",
      "Epoch 26: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.5342 - val_loss: 120.9784 - lr: 0.0024\n",
      "Epoch 27/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 18.3741\n",
      "Epoch 27: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.1673 - val_loss: 110.4349 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 17.3413\n",
      "Epoch 28: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.7227 - val_loss: 119.1318 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 17.3822\n",
      "Epoch 29: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4913 - val_loss: 109.0815 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.5824\n",
      "Epoch 30: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5758 - val_loss: 110.6435 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 15.2996\n",
      "Epoch 31: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0935 - val_loss: 110.4787 - lr: 0.0017\n",
      "Epoch 32/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 15.0308\n",
      "Epoch 32: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1445 - val_loss: 101.9995 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 15.8408\n",
      "Epoch 33: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7739 - val_loss: 112.6088 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 14.2394\n",
      "Epoch 34: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2113 - val_loss: 100.7753 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 15.0227\n",
      "Epoch 35: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.9535 - val_loss: 110.9793 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.3871\n",
      "Epoch 36: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5344 - val_loss: 110.7778 - lr: 0.0012\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.6850\n",
      "Epoch 37: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6156 - val_loss: 105.0974 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.7994\n",
      "Epoch 38: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6560 - val_loss: 120.5284 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.7953\n",
      "Epoch 39: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8823 - val_loss: 107.7483 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 13.3821\n",
      "Epoch 40: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.3692 - val_loss: 110.9516 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.8911\n",
      "Epoch 41: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8664 - val_loss: 115.6645 - lr: 8.2354e-04\n",
      "Epoch 42/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.6470\n",
      "Epoch 42: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6395 - val_loss: 115.0877 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 12.5663\n",
      "Epoch 43: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5245 - val_loss: 103.6153 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.0160\n",
      "Epoch 44: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8366 - val_loss: 118.8478 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 12.3918\n",
      "Epoch 45: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3210 - val_loss: 115.7208 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.1897\n",
      "Epoch 46: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9770 - val_loss: 111.0582 - lr: 5.7648e-04\n",
      "Epoch 47/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.3176\n",
      "Epoch 47: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6049 - val_loss: 112.4547 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.9198\n",
      "Epoch 48: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8909 - val_loss: 108.0625 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.0516\n",
      "Epoch 49: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.9842 - val_loss: 102.6243 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 10.8520\n",
      "Epoch 50: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7320 - val_loss: 110.2474 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.9414\n",
      "Epoch 51: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8496 - val_loss: 109.3034 - lr: 4.0354e-04\n",
      "Epoch 52/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.2883\n",
      "Epoch 52: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2120 - val_loss: 109.2563 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 11.4084\n",
      "Epoch 53: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.4348 - val_loss: 106.6040 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 10.9396\n",
      "Epoch 54: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9151 - val_loss: 111.1869 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.8192\n",
      "Epoch 55: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8925 - val_loss: 109.1646 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.0304\n",
      "Epoch 56: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9782 - val_loss: 107.9263 - lr: 2.8248e-04\n",
      "Epoch 57/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.1775\n",
      "Epoch 57: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2294 - val_loss: 111.4786 - lr: 2.8248e-04\n",
      "Epoch 58/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.5185\n",
      "Epoch 58: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4771 - val_loss: 107.4060 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.0438\n",
      "Epoch 59: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9967 - val_loss: 113.1647 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.1688\n",
      "Epoch 60: val_loss did not improve from 93.92181\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.4052 - val_loss: 110.7511 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 3882.0500\n",
      "Epoch 1: val_loss improved from inf to 217.03316, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 3673.8486 - val_loss: 217.0332 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 149.7285\n",
      "Epoch 2: val_loss improved from 217.03316 to 119.72304, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 149.9593 - val_loss: 119.7230 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 107.3287\n",
      "Epoch 3: val_loss improved from 119.72304 to 104.86798, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 107.3287 - val_loss: 104.8680 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 107.6945\n",
      "Epoch 4: val_loss did not improve from 104.86798\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 107.7612 - val_loss: 105.1987 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 79.1417\n",
      "Epoch 5: val_loss improved from 104.86798 to 104.58598, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 77.7497 - val_loss: 104.5860 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 68.0738\n",
      "Epoch 6: val_loss improved from 104.58598 to 81.17535, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 68.9973 - val_loss: 81.1754 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 64.9127\n",
      "Epoch 7: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 66.7791 - val_loss: 137.7630 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 65.2860\n",
      "Epoch 8: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.3280 - val_loss: 113.3058 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 63.0640\n",
      "Epoch 9: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.0640 - val_loss: 97.7869 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 63.1532\n",
      "Epoch 10: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 62.2883 - val_loss: 114.9281 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 49.7285\n",
      "Epoch 11: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 49.7285 - val_loss: 102.6373 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 39.5304\n",
      "Epoch 12: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.4191 - val_loss: 91.0261 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 36.4806\n",
      "Epoch 13: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 36.7809 - val_loss: 95.6378 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 37.4145\n",
      "Epoch 14: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.3898 - val_loss: 132.4246 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 35.8342\n",
      "Epoch 15: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 35.7878 - val_loss: 92.6207 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 33.1741\n",
      "Epoch 16: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 33.1115 - val_loss: 99.3588 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 26.9577\n",
      "Epoch 17: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 27.2141 - val_loss: 105.5519 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 25.6815\n",
      "Epoch 18: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.6223 - val_loss: 95.7327 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 26.2826\n",
      "Epoch 19: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.3043 - val_loss: 110.2321 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 26.9159\n",
      "Epoch 20: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.3404 - val_loss: 110.2775 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 25.0462\n",
      "Epoch 21: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.9568 - val_loss: 107.0170 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 21.6329\n",
      "Epoch 22: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.2769 - val_loss: 114.7343 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.3886\n",
      "Epoch 23: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.3823 - val_loss: 106.0589 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 19.3242\n",
      "Epoch 24: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.3159 - val_loss: 117.5077 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 19.5261\n",
      "Epoch 25: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.7629 - val_loss: 121.6240 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.7721\n",
      "Epoch 26: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.7603 - val_loss: 108.8129 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 18.5173\n",
      "Epoch 27: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.5173 - val_loss: 123.6830 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.2385\n",
      "Epoch 28: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.2414 - val_loss: 115.0947 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 17.2595\n",
      "Epoch 29: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.1303 - val_loss: 118.5840 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 17.3540\n",
      "Epoch 30: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.2101 - val_loss: 100.3539 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 17.2441\n",
      "Epoch 31: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.0241 - val_loss: 107.8269 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.5343\n",
      "Epoch 32: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6490 - val_loss: 109.1806 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 15.6087\n",
      "Epoch 33: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.4217 - val_loss: 115.3723 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.4128\n",
      "Epoch 34: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4078 - val_loss: 115.5117 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.2843\n",
      "Epoch 35: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.3702 - val_loss: 107.8905 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.4299\n",
      "Epoch 36: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7527 - val_loss: 108.2480 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.8284\n",
      "Epoch 37: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7015 - val_loss: 102.2030 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.9111\n",
      "Epoch 38: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9248 - val_loss: 101.0547 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.8224\n",
      "Epoch 39: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.7749 - val_loss: 113.6712 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.8404\n",
      "Epoch 40: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9642 - val_loss: 108.0636 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.3605\n",
      "Epoch 41: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3205 - val_loss: 115.9609 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.4666\n",
      "Epoch 42: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6051 - val_loss: 103.7091 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 9.7827\n",
      "Epoch 43: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.7569 - val_loss: 117.4204 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 9.3795\n",
      "Epoch 44: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3493 - val_loss: 101.1677 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.3888\n",
      "Epoch 45: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4526 - val_loss: 103.6280 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 8.8620\n",
      "Epoch 46: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1270 - val_loss: 100.7313 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.3354\n",
      "Epoch 47: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3542 - val_loss: 104.2057 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.7867\n",
      "Epoch 48: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8843 - val_loss: 102.7520 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.8433\n",
      "Epoch 49: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7541 - val_loss: 112.5306 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 8.6762\n",
      "Epoch 50: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6370 - val_loss: 101.0508 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 8.5320\n",
      "Epoch 51: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6640 - val_loss: 106.2935 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 8.2346\n",
      "Epoch 52: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2085 - val_loss: 104.5962 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 7.9823\n",
      "Epoch 53: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1063 - val_loss: 104.6663 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.3666\n",
      "Epoch 54: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.3994 - val_loss: 101.2898 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 7.9065\n",
      "Epoch 55: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7357 - val_loss: 104.9740 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 8.6350\n",
      "Epoch 56: val_loss did not improve from 81.17535\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.4965 - val_loss: 104.6859 - lr: 4.0354e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 3049.3220\n",
      "Epoch 1: val_loss improved from inf to 145.08705, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 2983.0137 - val_loss: 145.0871 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 147.9226\n",
      "Epoch 2: val_loss improved from 145.08705 to 107.25814, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 147.9226 - val_loss: 107.2581 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 115.6898\n",
      "Epoch 3: val_loss improved from 107.25814 to 102.08389, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 115.6898 - val_loss: 102.0839 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 96.0510\n",
      "Epoch 4: val_loss improved from 102.08389 to 101.46526, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 95.1947 - val_loss: 101.4653 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 89.0140\n",
      "Epoch 5: val_loss improved from 101.46526 to 82.57887, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 87.3779 - val_loss: 82.5789 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 72.7412\n",
      "Epoch 6: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 72.7062 - val_loss: 242.2465 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 72.4719\n",
      "Epoch 7: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 70.8052 - val_loss: 103.5500 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 63.2445\n",
      "Epoch 8: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 64.9285 - val_loss: 106.2839 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 60.8421\n",
      "Epoch 9: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 59.6995 - val_loss: 100.5087 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 55.8750\n",
      "Epoch 10: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.5347 - val_loss: 121.9972 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 46.7450\n",
      "Epoch 11: val_loss did not improve from 82.57887\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 46.0003 - val_loss: 116.9599 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 39.1476\n",
      "Epoch 12: val_loss improved from 82.57887 to 80.67855, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.9682 - val_loss: 80.6786 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 49.9469\n",
      "Epoch 13: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 49.3383 - val_loss: 118.2196 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 37.0861\n",
      "Epoch 14: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.6866 - val_loss: 94.3848 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 34.8862\n",
      "Epoch 15: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.4220 - val_loss: 158.4870 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 45.3271\n",
      "Epoch 16: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 43.2522 - val_loss: 98.0101 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 32.6207\n",
      "Epoch 17: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.5355 - val_loss: 86.6723 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 29.3463\n",
      "Epoch 18: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.0807 - val_loss: 108.5958 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 24.6269\n",
      "Epoch 19: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.3198 - val_loss: 117.8879 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 21.5250\n",
      "Epoch 20: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.6061 - val_loss: 88.1284 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 20.5517\n",
      "Epoch 21: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.7201 - val_loss: 98.3449 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 21.4911\n",
      "Epoch 22: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.7067 - val_loss: 118.8729 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.4450\n",
      "Epoch 23: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.3383 - val_loss: 112.7283 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 18.7002\n",
      "Epoch 24: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.3571 - val_loss: 119.7805 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 17.5280\n",
      "Epoch 25: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.5667 - val_loss: 115.3623 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 16.8390\n",
      "Epoch 26: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.8408 - val_loss: 120.6305 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.5435\n",
      "Epoch 27: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5696 - val_loss: 125.9269 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 15.2564\n",
      "Epoch 28: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.2557 - val_loss: 100.2434 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.1332\n",
      "Epoch 29: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2187 - val_loss: 107.3257 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.2215\n",
      "Epoch 30: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.2896 - val_loss: 117.5067 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.9375\n",
      "Epoch 31: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7278 - val_loss: 107.0073 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 12.7492\n",
      "Epoch 32: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7242 - val_loss: 104.5909 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.8350\n",
      "Epoch 33: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8354 - val_loss: 113.4765 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 12.6376\n",
      "Epoch 34: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.5581 - val_loss: 103.4458 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 12.2602\n",
      "Epoch 35: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5944 - val_loss: 116.1228 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 12.7886\n",
      "Epoch 36: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8830 - val_loss: 96.2983 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 11.9462\n",
      "Epoch 37: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9684 - val_loss: 107.5041 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 11.5808\n",
      "Epoch 38: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.5808 - val_loss: 108.2144 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.7838\n",
      "Epoch 39: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0493 - val_loss: 115.2529 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.3920\n",
      "Epoch 40: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2974 - val_loss: 110.2012 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 12.2179\n",
      "Epoch 41: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8459 - val_loss: 115.9709 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.7250\n",
      "Epoch 42: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7115 - val_loss: 122.4092 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 10.9927\n",
      "Epoch 43: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.9405 - val_loss: 108.7872 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.4145\n",
      "Epoch 44: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3982 - val_loss: 117.2422 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 9.6691\n",
      "Epoch 45: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5917 - val_loss: 116.9669 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.4046\n",
      "Epoch 46: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4149 - val_loss: 102.3372 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.1783\n",
      "Epoch 47: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1630 - val_loss: 108.6284 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 9.9035\n",
      "Epoch 48: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9035 - val_loss: 111.3765 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 9.2535\n",
      "Epoch 49: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2196 - val_loss: 112.2092 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.4953\n",
      "Epoch 50: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5666 - val_loss: 110.4729 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.8208\n",
      "Epoch 51: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.8985 - val_loss: 108.0467 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 9.2077\n",
      "Epoch 52: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.1854 - val_loss: 106.2461 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.3336\n",
      "Epoch 53: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2864 - val_loss: 100.9083 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 9.3933\n",
      "Epoch 54: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4713 - val_loss: 109.2698 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.2747\n",
      "Epoch 55: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1935 - val_loss: 107.3014 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 8.4395\n",
      "Epoch 56: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.6544 - val_loss: 108.2637 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 8.9565\n",
      "Epoch 57: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.9565 - val_loss: 111.1213 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.7144\n",
      "Epoch 58: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7009 - val_loss: 110.1723 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 8.8631\n",
      "Epoch 59: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8590 - val_loss: 113.1137 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.8426\n",
      "Epoch 60: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.8692 - val_loss: 106.8468 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 8.5722\n",
      "Epoch 61: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5576 - val_loss: 109.6368 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 8.5861\n",
      "Epoch 62: val_loss did not improve from 80.67855\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8020 - val_loss: 111.3380 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Mean MSE: 81.37043811956212\n",
      "Mean R-squared: 0.7795696592958493\n",
      "Average prop10: 0.7750204758190329\n",
      "Average prop5: 0.5275636212948518\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predict the target values\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    print(\"Shapes - y_pred:\", y_pred.shape, \"y:\", y.shape)\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    error = y_pred - y\n",
    "    mse = np.mean(error ** 2)\n",
    "    r_squared = 1 - (np.sum(error ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    \n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# Perform cross-validation for MLP\n",
    "def mlp_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    x = Dense(units=256, activation='relu')(inputs)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=50,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.7,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mlp_filtered_mse_list = []\n",
    "mlp_filtered_r_squared_list = []\n",
    "mlp_filtered_prop10_list = []\n",
    "mlp_filtered_prop5_list = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create mlp model with dropout\n",
    "    model_mlp = mlp_model_with_dropout((X_train.shape[1]), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'best_model_fold_{fold_idx}.h5',  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_mlp.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks including ModelCheckpoint\n",
    "    model_mlp.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # After training, you can load the best model using the following:\n",
    "    best_model_mlp = load_model(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(best_model_mlp, X_test_scaled, y_test)\n",
    "\n",
    "    mlp_filtered_mse_list.append(mse)\n",
    "    mlp_filtered_r_squared_list.append(r_squared)\n",
    "    mlp_filtered_prop10_list.append(prop10)\n",
    "    mlp_filtered_prop5_list.append(prop5)\n",
    "\n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(mlp_filtered_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(mlp_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(mlp_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(mlp_filtered_prop5_list))\n",
    "\n",
    "CV_results2['mlp-filtered'] = {'mse': mlp_filtered_mse_list,\n",
    "                               'r_squared': mlp_filtered_r_squared_list,\n",
    "                               'prop10': mlp_filtered_prop10_list,\n",
    "                               'prop5': mlp_filtered_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 353.1910\n",
      "Epoch 1: val_loss improved from inf to 301.06271, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 4ms/step - loss: 314.9818 - val_loss: 301.0627 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 148.2304\n",
      "Epoch 2: val_loss improved from 301.06271 to 129.91440, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 148.1364 - val_loss: 129.9144 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 119.1205\n",
      "Epoch 3: val_loss improved from 129.91440 to 128.81563, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 119.1520 - val_loss: 128.8156 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 116.9422\n",
      "Epoch 4: val_loss improved from 128.81563 to 95.81544, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 114.4809 - val_loss: 95.8154 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 91.7175\n",
      "Epoch 5: val_loss did not improve from 95.81544\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 91.7175 - val_loss: 105.5233 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 79.2907\n",
      "Epoch 6: val_loss did not improve from 95.81544\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.2406 - val_loss: 123.6604 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 70.8512\n",
      "Epoch 7: val_loss did not improve from 95.81544\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 70.6985 - val_loss: 198.9910 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 65.8830\n",
      "Epoch 8: val_loss improved from 95.81544 to 88.33872, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.0030 - val_loss: 88.3387 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 62.8532\n",
      "Epoch 9: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 62.6959 - val_loss: 147.2426 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 78.1657\n",
      "Epoch 10: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.4788 - val_loss: 126.5752 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 56.6094\n",
      "Epoch 11: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.7834 - val_loss: 129.4837 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 50.9293\n",
      "Epoch 12: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 50.7585 - val_loss: 107.3449 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 48.5389\n",
      "Epoch 13: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.4933 - val_loss: 112.3263 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 39.6624\n",
      "Epoch 14: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.2071 - val_loss: 102.6593 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 33.5529\n",
      "Epoch 15: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.8672 - val_loss: 89.9273 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 36.7693\n",
      "Epoch 16: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.7117 - val_loss: 99.6605 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.6082\n",
      "Epoch 17: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.5145 - val_loss: 91.5491 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 28.3415\n",
      "Epoch 18: val_loss did not improve from 88.33872\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.5998 - val_loss: 108.3241 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 25.1721\n",
      "Epoch 19: val_loss improved from 88.33872 to 88.17695, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.0040 - val_loss: 88.1769 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 24.4757\n",
      "Epoch 20: val_loss did not improve from 88.17695\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.1702 - val_loss: 88.7880 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 22.0062\n",
      "Epoch 21: val_loss did not improve from 88.17695\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.9185 - val_loss: 93.3669 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 21.7336\n",
      "Epoch 22: val_loss did not improve from 88.17695\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.5487 - val_loss: 108.6738 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 21.9517\n",
      "Epoch 23: val_loss improved from 88.17695 to 87.44906, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.9517 - val_loss: 87.4491 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 23.3669\n",
      "Epoch 24: val_loss did not improve from 87.44906\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.0446 - val_loss: 94.5558 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 22.2791\n",
      "Epoch 25: val_loss did not improve from 87.44906\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.0429 - val_loss: 93.8656 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 23.6486\n",
      "Epoch 26: val_loss did not improve from 87.44906\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.4522 - val_loss: 92.7378 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 19.6879\n",
      "Epoch 27: val_loss improved from 87.44906 to 87.17427, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6549 - val_loss: 87.1743 - lr: 0.0049\n",
      "Epoch 28/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 21.0539\n",
      "Epoch 28: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.3907 - val_loss: 113.3921 - lr: 0.0049\n",
      "Epoch 29/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 21.4720\n",
      "Epoch 29: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.3273 - val_loss: 93.5161 - lr: 0.0049\n",
      "Epoch 30/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 19.8955\n",
      "Epoch 30: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.4247 - val_loss: 91.8155 - lr: 0.0049\n",
      "Epoch 31/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 20.2203\n",
      "Epoch 31: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.2316 - val_loss: 88.1958 - lr: 0.0049\n",
      "Epoch 32/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 21.1598\n",
      "Epoch 32: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.6878 - val_loss: 93.4142 - lr: 0.0049\n",
      "Epoch 33/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.2560\n",
      "Epoch 33: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.2626 - val_loss: 95.0713 - lr: 0.0034\n",
      "Epoch 34/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 17.1905\n",
      "Epoch 34: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9464 - val_loss: 88.5193 - lr: 0.0034\n",
      "Epoch 35/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 17.1278\n",
      "Epoch 35: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9774 - val_loss: 90.2103 - lr: 0.0034\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.5346\n",
      "Epoch 36: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6791 - val_loss: 104.1096 - lr: 0.0034\n",
      "Epoch 37/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.2580\n",
      "Epoch 37: val_loss did not improve from 87.17427\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.0844 - val_loss: 92.3583 - lr: 0.0034\n",
      "Epoch 38/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 13.7037\n",
      "Epoch 38: val_loss improved from 87.17427 to 83.75697, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7664 - val_loss: 83.7570 - lr: 0.0024\n",
      "Epoch 39/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.0434\n",
      "Epoch 39: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.9492 - val_loss: 87.5452 - lr: 0.0024\n",
      "Epoch 40/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 12.2966\n",
      "Epoch 40: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3844 - val_loss: 96.2923 - lr: 0.0024\n",
      "Epoch 41/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.0418\n",
      "Epoch 41: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8785 - val_loss: 85.3113 - lr: 0.0024\n",
      "Epoch 42/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.3577\n",
      "Epoch 42: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3268 - val_loss: 85.4583 - lr: 0.0024\n",
      "Epoch 43/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 12.4444\n",
      "Epoch 43: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5649 - val_loss: 90.3244 - lr: 0.0024\n",
      "Epoch 44/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.6609\n",
      "Epoch 44: val_loss did not improve from 83.75697\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7313 - val_loss: 84.6513 - lr: 0.0017\n",
      "Epoch 45/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.2677\n",
      "Epoch 45: val_loss improved from 83.75697 to 83.66971, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2757 - val_loss: 83.6697 - lr: 0.0017\n",
      "Epoch 46/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.1876\n",
      "Epoch 46: val_loss did not improve from 83.66971\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0617 - val_loss: 87.5135 - lr: 0.0017\n",
      "Epoch 47/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.1810\n",
      "Epoch 47: val_loss did not improve from 83.66971\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1967 - val_loss: 86.1886 - lr: 0.0017\n",
      "Epoch 48/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.5630\n",
      "Epoch 48: val_loss did not improve from 83.66971\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.4677 - val_loss: 89.2014 - lr: 0.0017\n",
      "Epoch 49/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.8875\n",
      "Epoch 49: val_loss did not improve from 83.66971\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8750 - val_loss: 90.2407 - lr: 0.0017\n",
      "Epoch 50/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.9534\n",
      "Epoch 50: val_loss did not improve from 83.66971\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0069 - val_loss: 84.6723 - lr: 0.0017\n",
      "Epoch 51/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.0411\n",
      "Epoch 51: val_loss improved from 83.66971 to 83.33045, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.9133 - val_loss: 83.3305 - lr: 0.0012\n",
      "Epoch 52/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 10.6470\n",
      "Epoch 52: val_loss did not improve from 83.33045\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5652 - val_loss: 88.3505 - lr: 0.0012\n",
      "Epoch 53/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.4316\n",
      "Epoch 53: val_loss did not improve from 83.33045\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4666 - val_loss: 85.9264 - lr: 0.0012\n",
      "Epoch 54/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.4041\n",
      "Epoch 54: val_loss did not improve from 83.33045\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4304 - val_loss: 85.9056 - lr: 0.0012\n",
      "Epoch 55/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.5777\n",
      "Epoch 55: val_loss did not improve from 83.33045\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5689 - val_loss: 89.4582 - lr: 0.0012\n",
      "Epoch 56/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.9099\n",
      "Epoch 56: val_loss did not improve from 83.33045\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.8796 - val_loss: 83.7943 - lr: 0.0012\n",
      "Epoch 57/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 8.6146\n",
      "Epoch 57: val_loss improved from 83.33045 to 82.97669, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6683 - val_loss: 82.9767 - lr: 8.2354e-04\n",
      "Epoch 58/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 8.5684\n",
      "Epoch 58: val_loss did not improve from 82.97669\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5151 - val_loss: 84.5129 - lr: 8.2354e-04\n",
      "Epoch 59/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.3993\n",
      "Epoch 59: val_loss did not improve from 82.97669\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2964 - val_loss: 86.8670 - lr: 8.2354e-04\n",
      "Epoch 60/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.7263\n",
      "Epoch 60: val_loss did not improve from 82.97669\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6844 - val_loss: 83.2586 - lr: 8.2354e-04\n",
      "Epoch 61/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.3004\n",
      "Epoch 61: val_loss improved from 82.97669 to 82.22234, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.3017 - val_loss: 82.2223 - lr: 8.2354e-04\n",
      "Epoch 62/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 9.0123\n",
      "Epoch 62: val_loss did not improve from 82.22234\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9842 - val_loss: 84.9489 - lr: 8.2354e-04\n",
      "Epoch 63/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 8.6559\n",
      "Epoch 63: val_loss did not improve from 82.22234\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7261 - val_loss: 83.6142 - lr: 8.2354e-04\n",
      "Epoch 64/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.6188\n",
      "Epoch 64: val_loss improved from 82.22234 to 79.21899, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6292 - val_loss: 79.2190 - lr: 8.2354e-04\n",
      "Epoch 65/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 8.6146\n",
      "Epoch 65: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6843 - val_loss: 82.7317 - lr: 8.2354e-04\n",
      "Epoch 66/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.1755\n",
      "Epoch 66: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2317 - val_loss: 81.3809 - lr: 8.2354e-04\n",
      "Epoch 67/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.7982\n",
      "Epoch 67: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7986 - val_loss: 83.5786 - lr: 8.2354e-04\n",
      "Epoch 68/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 8.3257\n",
      "Epoch 68: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3503 - val_loss: 84.5875 - lr: 8.2354e-04\n",
      "Epoch 69/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 7.9925\n",
      "Epoch 69: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.9614 - val_loss: 91.3158 - lr: 8.2354e-04\n",
      "Epoch 70/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 7.8557\n",
      "Epoch 70: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7527 - val_loss: 84.8326 - lr: 5.7648e-04\n",
      "Epoch 71/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.6071\n",
      "Epoch 71: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7192 - val_loss: 90.6515 - lr: 5.7648e-04\n",
      "Epoch 72/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.4935\n",
      "Epoch 72: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.5284 - val_loss: 83.6827 - lr: 5.7648e-04\n",
      "Epoch 73/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.4762\n",
      "Epoch 73: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.5482 - val_loss: 83.3973 - lr: 5.7648e-04\n",
      "Epoch 74/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.0093\n",
      "Epoch 74: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0027 - val_loss: 88.1645 - lr: 5.7648e-04\n",
      "Epoch 75/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.0360\n",
      "Epoch 75: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.9289 - val_loss: 83.1416 - lr: 4.0354e-04\n",
      "Epoch 76/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 7.3894\n",
      "Epoch 76: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2652 - val_loss: 81.9279 - lr: 4.0354e-04\n",
      "Epoch 77/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.6427\n",
      "Epoch 77: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.5578 - val_loss: 84.6281 - lr: 4.0354e-04\n",
      "Epoch 78/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.0057\n",
      "Epoch 78: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9979 - val_loss: 89.0769 - lr: 4.0354e-04\n",
      "Epoch 79/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 7.2594\n",
      "Epoch 79: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2465 - val_loss: 84.4244 - lr: 4.0354e-04\n",
      "Epoch 80/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.7079\n",
      "Epoch 80: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7397 - val_loss: 86.8932 - lr: 2.8248e-04\n",
      "Epoch 81/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.1960\n",
      "Epoch 81: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1667 - val_loss: 85.4704 - lr: 2.8248e-04\n",
      "Epoch 82/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 7.1027\n",
      "Epoch 82: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0888 - val_loss: 82.5224 - lr: 2.8248e-04\n",
      "Epoch 83/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 7.3499\n",
      "Epoch 83: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2699 - val_loss: 85.3551 - lr: 2.8248e-04\n",
      "Epoch 84/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.9107\n",
      "Epoch 84: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8782 - val_loss: 84.6065 - lr: 2.8248e-04\n",
      "Epoch 85/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.2320\n",
      "Epoch 85: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1884 - val_loss: 83.8094 - lr: 1.9773e-04\n",
      "Epoch 86/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 6.8001\n",
      "Epoch 86: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8023 - val_loss: 83.5445 - lr: 1.9773e-04\n",
      "Epoch 87/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 6.7923\n",
      "Epoch 87: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7554 - val_loss: 82.8538 - lr: 1.9773e-04\n",
      "Epoch 88/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 6.9241\n",
      "Epoch 88: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8956 - val_loss: 85.2513 - lr: 1.9773e-04\n",
      "Epoch 89/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.8135\n",
      "Epoch 89: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7892 - val_loss: 82.5962 - lr: 1.9773e-04\n",
      "Epoch 90/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 6.5589\n",
      "Epoch 90: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5269 - val_loss: 81.8572 - lr: 1.3841e-04\n",
      "Epoch 91/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 6.6434\n",
      "Epoch 91: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7294 - val_loss: 83.7565 - lr: 1.3841e-04\n",
      "Epoch 92/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.4637\n",
      "Epoch 92: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3994 - val_loss: 82.5520 - lr: 1.3841e-04\n",
      "Epoch 93/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 6.7689\n",
      "Epoch 93: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7559 - val_loss: 83.3053 - lr: 1.3841e-04\n",
      "Epoch 94/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 6.8737\n",
      "Epoch 94: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9317 - val_loss: 82.7868 - lr: 1.3841e-04\n",
      "Epoch 95/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.5671\n",
      "Epoch 95: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6379 - val_loss: 82.5822 - lr: 9.6889e-05\n",
      "Epoch 96/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.6752\n",
      "Epoch 96: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6686 - val_loss: 85.2304 - lr: 9.6889e-05\n",
      "Epoch 97/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.7070\n",
      "Epoch 97: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6518 - val_loss: 82.8061 - lr: 9.6889e-05\n",
      "Epoch 98/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 6.3465\n",
      "Epoch 98: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3409 - val_loss: 84.2388 - lr: 9.6889e-05\n",
      "Epoch 99/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 6.6416\n",
      "Epoch 99: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5409 - val_loss: 83.1502 - lr: 9.6889e-05\n",
      "Epoch 100/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.5336\n",
      "Epoch 100: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5045 - val_loss: 84.9491 - lr: 6.7822e-05\n",
      "Epoch 101/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.4270\n",
      "Epoch 101: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.4496 - val_loss: 82.4840 - lr: 6.7822e-05\n",
      "Epoch 102/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.1092\n",
      "Epoch 102: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.1183 - val_loss: 82.5148 - lr: 6.7822e-05\n",
      "Epoch 103/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.4296\n",
      "Epoch 103: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.4568 - val_loss: 84.0614 - lr: 6.7822e-05\n",
      "Epoch 104/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.1763\n",
      "Epoch 104: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.2927 - val_loss: 84.6326 - lr: 6.7822e-05\n",
      "Epoch 105/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.2799\n",
      "Epoch 105: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.2418 - val_loss: 82.5524 - lr: 4.7476e-05\n",
      "Epoch 106/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 6.2707\n",
      "Epoch 106: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.2919 - val_loss: 83.8306 - lr: 4.7476e-05\n",
      "Epoch 107/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.3150\n",
      "Epoch 107: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3096 - val_loss: 83.0304 - lr: 4.7476e-05\n",
      "Epoch 108/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.5130\n",
      "Epoch 108: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5098 - val_loss: 83.6177 - lr: 4.7476e-05\n",
      "Epoch 109/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.3560\n",
      "Epoch 109: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3592 - val_loss: 83.9470 - lr: 4.7476e-05\n",
      "Epoch 110/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.2262\n",
      "Epoch 110: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.1903 - val_loss: 82.7877 - lr: 3.3233e-05\n",
      "Epoch 111/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.2335\n",
      "Epoch 111: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.1947 - val_loss: 84.0228 - lr: 3.3233e-05\n",
      "Epoch 112/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.2827\n",
      "Epoch 112: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.2993 - val_loss: 84.1302 - lr: 3.3233e-05\n",
      "Epoch 113/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.3723\n",
      "Epoch 113: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3515 - val_loss: 83.4108 - lr: 3.3233e-05\n",
      "Epoch 114/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 6.0479\n",
      "Epoch 114: val_loss did not improve from 79.21899\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 5.9966 - val_loss: 83.3046 - lr: 3.3233e-05\n",
      "21/21 [==============================] - 0s 985us/step\n",
      "Epoch 1/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 349.9720\n",
      "Epoch 1: val_loss improved from inf to 144.68088, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 1s 4ms/step - loss: 332.4505 - val_loss: 144.6809 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 131.3891\n",
      "Epoch 2: val_loss improved from 144.68088 to 101.76607, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 130.0369 - val_loss: 101.7661 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 115.3817\n",
      "Epoch 3: val_loss did not improve from 101.76607\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 119.6910 - val_loss: 231.1900 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 110.4943\n",
      "Epoch 4: val_loss did not improve from 101.76607\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 110.4806 - val_loss: 129.4238 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 105.6467\n",
      "Epoch 5: val_loss improved from 101.76607 to 92.94920, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 105.4114 - val_loss: 92.9492 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 96.4964\n",
      "Epoch 6: val_loss did not improve from 92.94920\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 97.9789 - val_loss: 170.4047 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 88.1825\n",
      "Epoch 7: val_loss did not improve from 92.94920\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 88.4129 - val_loss: 103.1519 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 80.7321\n",
      "Epoch 8: val_loss did not improve from 92.94920\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 80.5359 - val_loss: 214.5101 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 83.9852\n",
      "Epoch 9: val_loss did not improve from 92.94920\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 85.6872 - val_loss: 100.4712 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 96.9070 \n",
      "Epoch 10: val_loss did not improve from 92.94920\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 94.1262 - val_loss: 93.9621 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 68.2588\n",
      "Epoch 11: val_loss improved from 92.94920 to 89.71733, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 67.1783 - val_loss: 89.7173 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 54.8534\n",
      "Epoch 12: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 55.0093 - val_loss: 123.7687 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 56.2660\n",
      "Epoch 13: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.0165 - val_loss: 92.8671 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 55.8006\n",
      "Epoch 14: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.8095 - val_loss: 91.0191 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 47.1532\n",
      "Epoch 15: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.0426 - val_loss: 93.7071 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 41.7788\n",
      "Epoch 16: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.6240 - val_loss: 90.2783 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 36.2517\n",
      "Epoch 17: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.0247 - val_loss: 92.1550 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 32.7514\n",
      "Epoch 18: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.5842 - val_loss: 97.5281 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 30.1915\n",
      "Epoch 19: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.0624 - val_loss: 115.4374 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 30.7559\n",
      "Epoch 20: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.8786 - val_loss: 96.7517 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 33.1926\n",
      "Epoch 21: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.5374 - val_loss: 100.6852 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 25.9340\n",
      "Epoch 22: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.8372 - val_loss: 92.0369 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 23.4155\n",
      "Epoch 23: val_loss did not improve from 89.71733\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.5158 - val_loss: 93.0791 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 23.1543\n",
      "Epoch 24: val_loss improved from 89.71733 to 89.12491, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.8334 - val_loss: 89.1249 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 23.5920\n",
      "Epoch 25: val_loss did not improve from 89.12491\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.4144 - val_loss: 94.8901 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 22.8778\n",
      "Epoch 26: val_loss did not improve from 89.12491\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.0048 - val_loss: 95.2106 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 21.1472\n",
      "Epoch 27: val_loss did not improve from 89.12491\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9561 - val_loss: 93.2678 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 21.3115\n",
      "Epoch 28: val_loss did not improve from 89.12491\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.5499 - val_loss: 95.3980 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.7554\n",
      "Epoch 29: val_loss did not improve from 89.12491\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8684 - val_loss: 100.1315 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 18.7824\n",
      "Epoch 30: val_loss improved from 89.12491 to 88.77086, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.6441 - val_loss: 88.7709 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 17.6852\n",
      "Epoch 31: val_loss did not improve from 88.77086\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6976 - val_loss: 91.3294 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "70/91 [======================>.......] - ETA: 0s - loss: 17.3854\n",
      "Epoch 32: val_loss improved from 88.77086 to 87.35218, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.6246 - val_loss: 87.3522 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.2389\n",
      "Epoch 33: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3254 - val_loss: 90.5546 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 18.6789\n",
      "Epoch 34: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.6523 - val_loss: 92.9461 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.5226\n",
      "Epoch 35: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.5388 - val_loss: 97.9395 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 18.2533\n",
      "Epoch 36: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.9614 - val_loss: 92.3937 - lr: 0.0024\n",
      "Epoch 37/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 17.0837\n",
      "Epoch 37: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.7779 - val_loss: 95.7642 - lr: 0.0024\n",
      "Epoch 38/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 16.1882\n",
      "Epoch 38: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1383 - val_loss: 97.9046 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.6705\n",
      "Epoch 39: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.5992 - val_loss: 104.3688 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.7177\n",
      "Epoch 40: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.7060 - val_loss: 101.5793 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 15.3528\n",
      "Epoch 41: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4575 - val_loss: 97.8562 - lr: 0.0017\n",
      "Epoch 42/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.8141\n",
      "Epoch 42: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8329 - val_loss: 94.6003 - lr: 0.0017\n",
      "Epoch 43/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 13.9099\n",
      "Epoch 43: val_loss did not improve from 87.35218\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8735 - val_loss: 89.0731 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 14.3080\n",
      "Epoch 44: val_loss improved from 87.35218 to 86.53645, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2056 - val_loss: 86.5364 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 13.0155\n",
      "Epoch 45: val_loss did not improve from 86.53645\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.2005 - val_loss: 87.9570 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.6030\n",
      "Epoch 46: val_loss did not improve from 86.53645\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6053 - val_loss: 90.7445 - lr: 0.0012\n",
      "Epoch 47/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.9785\n",
      "Epoch 47: val_loss improved from 86.53645 to 85.76313, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.9838 - val_loss: 85.7631 - lr: 0.0012\n",
      "Epoch 48/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 12.7378\n",
      "Epoch 48: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7478 - val_loss: 87.9672 - lr: 0.0012\n",
      "Epoch 49/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.6527\n",
      "Epoch 49: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7000 - val_loss: 88.5716 - lr: 0.0012\n",
      "Epoch 50/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.9995\n",
      "Epoch 50: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9037 - val_loss: 86.2382 - lr: 0.0012\n",
      "Epoch 51/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.2798\n",
      "Epoch 51: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2018 - val_loss: 90.6054 - lr: 0.0012\n",
      "Epoch 52/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.5484\n",
      "Epoch 52: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6187 - val_loss: 87.0380 - lr: 0.0012\n",
      "Epoch 53/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.7400\n",
      "Epoch 53: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6689 - val_loss: 89.1396 - lr: 8.2354e-04\n",
      "Epoch 54/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.6526\n",
      "Epoch 54: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6218 - val_loss: 87.9146 - lr: 8.2354e-04\n",
      "Epoch 55/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.8636\n",
      "Epoch 55: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0350 - val_loss: 91.6242 - lr: 8.2354e-04\n",
      "Epoch 56/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 11.8609\n",
      "Epoch 56: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8378 - val_loss: 90.0184 - lr: 8.2354e-04\n",
      "Epoch 57/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.0851\n",
      "Epoch 57: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1020 - val_loss: 94.1661 - lr: 8.2354e-04\n",
      "Epoch 58/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.7175\n",
      "Epoch 58: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6766 - val_loss: 88.3825 - lr: 5.7648e-04\n",
      "Epoch 59/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.6832\n",
      "Epoch 59: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6436 - val_loss: 86.1649 - lr: 5.7648e-04\n",
      "Epoch 60/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.4238\n",
      "Epoch 60: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4484 - val_loss: 90.1282 - lr: 5.7648e-04\n",
      "Epoch 61/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.2685\n",
      "Epoch 61: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2943 - val_loss: 89.5976 - lr: 5.7648e-04\n",
      "Epoch 62/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.5762\n",
      "Epoch 62: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5255 - val_loss: 89.4960 - lr: 5.7648e-04\n",
      "Epoch 63/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.8089\n",
      "Epoch 63: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9340 - val_loss: 93.5538 - lr: 4.0354e-04\n",
      "Epoch 64/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 10.3430\n",
      "Epoch 64: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3698 - val_loss: 90.9112 - lr: 4.0354e-04\n",
      "Epoch 65/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.6303\n",
      "Epoch 65: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4522 - val_loss: 91.2071 - lr: 4.0354e-04\n",
      "Epoch 66/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.9128\n",
      "Epoch 66: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9345 - val_loss: 90.9322 - lr: 4.0354e-04\n",
      "Epoch 67/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.2598\n",
      "Epoch 67: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2834 - val_loss: 89.5374 - lr: 4.0354e-04\n",
      "Epoch 68/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.4707\n",
      "Epoch 68: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4667 - val_loss: 90.6565 - lr: 2.8248e-04\n",
      "Epoch 69/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.4818\n",
      "Epoch 69: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3866 - val_loss: 91.7827 - lr: 2.8248e-04\n",
      "Epoch 70/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.3752\n",
      "Epoch 70: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3072 - val_loss: 90.9074 - lr: 2.8248e-04\n",
      "Epoch 71/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 9.1976\n",
      "Epoch 71: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0904 - val_loss: 89.8363 - lr: 2.8248e-04\n",
      "Epoch 72/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.0454\n",
      "Epoch 72: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0061 - val_loss: 92.8391 - lr: 2.8248e-04\n",
      "Epoch 73/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.3801\n",
      "Epoch 73: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3246 - val_loss: 92.9409 - lr: 1.9773e-04\n",
      "Epoch 74/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.1735\n",
      "Epoch 74: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1174 - val_loss: 92.9095 - lr: 1.9773e-04\n",
      "Epoch 75/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.6883\n",
      "Epoch 75: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7314 - val_loss: 94.3024 - lr: 1.9773e-04\n",
      "Epoch 76/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 8.9738\n",
      "Epoch 76: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9354 - val_loss: 89.2379 - lr: 1.9773e-04\n",
      "Epoch 77/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.9753\n",
      "Epoch 77: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8541 - val_loss: 89.8683 - lr: 1.9773e-04\n",
      "Epoch 78/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 8.3971\n",
      "Epoch 78: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.4708 - val_loss: 90.3180 - lr: 1.3841e-04\n",
      "Epoch 79/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 8.0747\n",
      "Epoch 79: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.3754 - val_loss: 90.7562 - lr: 1.3841e-04\n",
      "Epoch 80/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.4609\n",
      "Epoch 80: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3952 - val_loss: 92.1351 - lr: 1.3841e-04\n",
      "Epoch 81/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 8.8692\n",
      "Epoch 81: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8000 - val_loss: 90.8058 - lr: 1.3841e-04\n",
      "Epoch 82/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.5844\n",
      "Epoch 82: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5980 - val_loss: 90.5788 - lr: 1.3841e-04\n",
      "Epoch 83/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 8.2592\n",
      "Epoch 83: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3836 - val_loss: 88.9808 - lr: 9.6889e-05\n",
      "Epoch 84/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.7062\n",
      "Epoch 84: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6487 - val_loss: 89.5182 - lr: 9.6889e-05\n",
      "Epoch 85/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.5475\n",
      "Epoch 85: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.4826 - val_loss: 91.7014 - lr: 9.6889e-05\n",
      "Epoch 86/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.5208\n",
      "Epoch 86: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5587 - val_loss: 90.2858 - lr: 9.6889e-05\n",
      "Epoch 87/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.5163\n",
      "Epoch 87: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5024 - val_loss: 89.7507 - lr: 9.6889e-05\n",
      "Epoch 88/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.4976\n",
      "Epoch 88: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.4003 - val_loss: 91.5303 - lr: 6.7822e-05\n",
      "Epoch 89/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 7.8995\n",
      "Epoch 89: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.9154 - val_loss: 91.5209 - lr: 6.7822e-05\n",
      "Epoch 90/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.9277\n",
      "Epoch 90: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.9123 - val_loss: 89.9288 - lr: 6.7822e-05\n",
      "Epoch 91/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 8.4872\n",
      "Epoch 91: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3626 - val_loss: 90.6371 - lr: 6.7822e-05\n",
      "Epoch 92/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 8.8054\n",
      "Epoch 92: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6624 - val_loss: 91.0166 - lr: 6.7822e-05\n",
      "Epoch 93/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.2513\n",
      "Epoch 93: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2688 - val_loss: 91.0132 - lr: 4.7476e-05\n",
      "Epoch 94/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.3148\n",
      "Epoch 94: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3449 - val_loss: 92.0268 - lr: 4.7476e-05\n",
      "Epoch 95/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 8.1407\n",
      "Epoch 95: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1407 - val_loss: 91.5895 - lr: 4.7476e-05\n",
      "Epoch 96/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 8.3485\n",
      "Epoch 96: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3674 - val_loss: 90.5542 - lr: 4.7476e-05\n",
      "Epoch 97/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.2716\n",
      "Epoch 97: val_loss did not improve from 85.76313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2518 - val_loss: 91.7875 - lr: 4.7476e-05\n",
      "21/21 [==============================] - 0s 969us/step\n",
      "Epoch 1/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 436.7651\n",
      "Epoch 1: val_loss improved from inf to 139.58270, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 436.6558 - val_loss: 139.5827 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 145.5508\n",
      "Epoch 2: val_loss did not improve from 139.58270\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 142.8760 - val_loss: 214.6567 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 137.0349\n",
      "Epoch 3: val_loss improved from 139.58270 to 132.90337, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 135.9489 - val_loss: 132.9034 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 110.2876\n",
      "Epoch 4: val_loss improved from 132.90337 to 112.92583, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 110.7560 - val_loss: 112.9258 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 97.7170\n",
      "Epoch 5: val_loss did not improve from 112.92583\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 98.0297 - val_loss: 144.8329 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 96.8315\n",
      "Epoch 6: val_loss improved from 112.92583 to 103.06385, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 97.0403 - val_loss: 103.0639 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 102.2182\n",
      "Epoch 7: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 100.7364 - val_loss: 132.3858 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 79.5064\n",
      "Epoch 8: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.1914 - val_loss: 109.8151 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 75.2806\n",
      "Epoch 9: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 76.7814 - val_loss: 150.0664 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 78.4661\n",
      "Epoch 10: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.5016 - val_loss: 139.3030 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 74.5300\n",
      "Epoch 11: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 77.5018 - val_loss: 114.5251 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 58.1723\n",
      "Epoch 12: val_loss did not improve from 103.06385\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 58.2060 - val_loss: 138.7721 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 53.8325\n",
      "Epoch 13: val_loss improved from 103.06385 to 92.65408, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 53.7130 - val_loss: 92.6541 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 52.9951\n",
      "Epoch 14: val_loss did not improve from 92.65408\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 52.9693 - val_loss: 96.7588 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 51.0377\n",
      "Epoch 15: val_loss improved from 92.65408 to 89.74944, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 50.8296 - val_loss: 89.7494 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 45.9841\n",
      "Epoch 16: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.0194 - val_loss: 92.0454 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 37.6446\n",
      "Epoch 17: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.2016 - val_loss: 106.0595 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 44.7004\n",
      "Epoch 18: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.6361 - val_loss: 91.8784 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 36.7742\n",
      "Epoch 19: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.0140 - val_loss: 96.3895 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 34.3789\n",
      "Epoch 20: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.9505 - val_loss: 164.1205 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 44.7244\n",
      "Epoch 21: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 43.1217 - val_loss: 100.2688 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 29.7846\n",
      "Epoch 22: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.0401 - val_loss: 91.2728 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 29.0830\n",
      "Epoch 23: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.0050 - val_loss: 106.4655 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 25.9325\n",
      "Epoch 24: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.5193 - val_loss: 98.6784 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 26.5667\n",
      "Epoch 25: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.3469 - val_loss: 105.5543 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 21.6464\n",
      "Epoch 26: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.8044 - val_loss: 94.8292 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 22.8166\n",
      "Epoch 27: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.2219 - val_loss: 99.9439 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 20.5810\n",
      "Epoch 28: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.4940 - val_loss: 102.7424 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 19.7971\n",
      "Epoch 29: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.7971 - val_loss: 101.3563 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 19.4057\n",
      "Epoch 30: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6886 - val_loss: 128.2468 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 19.2560\n",
      "Epoch 31: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1246 - val_loss: 97.6760 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.2803\n",
      "Epoch 32: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.1197 - val_loss: 94.1101 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.5319\n",
      "Epoch 33: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5319 - val_loss: 92.1551 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 17.0099\n",
      "Epoch 34: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9959 - val_loss: 95.2643 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.4157\n",
      "Epoch 35: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.9775 - val_loss: 92.8750 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.6752\n",
      "Epoch 36: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8299 - val_loss: 97.1881 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.5150\n",
      "Epoch 37: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.6299 - val_loss: 93.4802 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.0176\n",
      "Epoch 38: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0822 - val_loss: 91.7669 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 14.3429\n",
      "Epoch 39: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.4677 - val_loss: 94.2133 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.1241\n",
      "Epoch 40: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.3211 - val_loss: 97.8453 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 13.1406\n",
      "Epoch 41: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9665 - val_loss: 104.8631 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.2038\n",
      "Epoch 42: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.1109 - val_loss: 90.8909 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.4667\n",
      "Epoch 43: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5263 - val_loss: 96.9581 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.7660\n",
      "Epoch 44: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6821 - val_loss: 101.7210 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 11.9526\n",
      "Epoch 45: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9526 - val_loss: 96.9606 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.7781\n",
      "Epoch 46: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7846 - val_loss: 93.3516 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.8826\n",
      "Epoch 47: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6225 - val_loss: 97.1282 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.7056\n",
      "Epoch 48: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8809 - val_loss: 93.3511 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.0429\n",
      "Epoch 49: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9796 - val_loss: 93.3587 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.1900\n",
      "Epoch 50: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1339 - val_loss: 95.6259 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.6506\n",
      "Epoch 51: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5965 - val_loss: 92.9758 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.5049\n",
      "Epoch 52: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4403 - val_loss: 92.4627 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.2271\n",
      "Epoch 53: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1722 - val_loss: 97.3301 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 9.9760\n",
      "Epoch 54: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.9760 - val_loss: 91.8252 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.0898\n",
      "Epoch 55: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0910 - val_loss: 98.4076 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.1423\n",
      "Epoch 56: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.1551 - val_loss: 97.0809 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.1182\n",
      "Epoch 57: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9674 - val_loss: 100.0409 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.5819\n",
      "Epoch 58: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6295 - val_loss: 94.2093 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 10.2558\n",
      "Epoch 59: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1627 - val_loss: 99.2217 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.2372\n",
      "Epoch 60: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1618 - val_loss: 96.1245 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.6127\n",
      "Epoch 61: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5165 - val_loss: 94.8115 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 9.2160\n",
      "Epoch 62: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2389 - val_loss: 94.4261 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 9.3168\n",
      "Epoch 63: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3168 - val_loss: 96.2617 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 9.2122\n",
      "Epoch 64: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.1144 - val_loss: 97.7956 - lr: 2.8248e-04\n",
      "Epoch 65/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.6544\n",
      "Epoch 65: val_loss did not improve from 89.74944\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9175 - val_loss: 92.0381 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 511.8825\n",
      "Epoch 1: val_loss improved from inf to 123.73274, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 4ms/step - loss: 465.6429 - val_loss: 123.7327 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 138.8248\n",
      "Epoch 2: val_loss did not improve from 123.73274\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 138.7897 - val_loss: 213.4770 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 132.1372\n",
      "Epoch 3: val_loss did not improve from 123.73274\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 132.5702 - val_loss: 162.3671 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 137.6483\n",
      "Epoch 4: val_loss improved from 123.73274 to 107.90584, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 134.8186 - val_loss: 107.9058 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 109.3667\n",
      "Epoch 5: val_loss improved from 107.90584 to 94.28748, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 109.3304 - val_loss: 94.2875 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 86.5726\n",
      "Epoch 6: val_loss did not improve from 94.28748\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 88.1877 - val_loss: 137.9034 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 96.3431\n",
      "Epoch 7: val_loss did not improve from 94.28748\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 95.5022 - val_loss: 191.3209 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 85.9118\n",
      "Epoch 8: val_loss improved from 94.28748 to 92.14204, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 84.5667 - val_loss: 92.1420 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 78.6509\n",
      "Epoch 9: val_loss did not improve from 92.14204\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.1424 - val_loss: 169.7206 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 77.7221\n",
      "Epoch 10: val_loss improved from 92.14204 to 83.86809, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.1423 - val_loss: 83.8681 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 71.0090\n",
      "Epoch 11: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 75.2586 - val_loss: 160.5008 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 86.8642\n",
      "Epoch 12: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 83.8537 - val_loss: 116.6726 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 79.4930\n",
      "Epoch 13: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 78.5033 - val_loss: 88.8187 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 61.9247\n",
      "Epoch 14: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.8447 - val_loss: 87.4140 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 60.8645\n",
      "Epoch 15: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 60.2933 - val_loss: 96.2925 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 44.4087\n",
      "Epoch 16: val_loss did not improve from 83.86809\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.9408 - val_loss: 117.8111 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 40.4900\n",
      "Epoch 17: val_loss improved from 83.86809 to 83.85735, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.4795 - val_loss: 83.8573 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 38.5858\n",
      "Epoch 18: val_loss did not improve from 83.85735\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.3895 - val_loss: 94.6305 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 39.5191\n",
      "Epoch 19: val_loss did not improve from 83.85735\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 39.3651 - val_loss: 87.2349 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 35.6422\n",
      "Epoch 20: val_loss did not improve from 83.85735\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.6713 - val_loss: 86.5506 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 35.7973\n",
      "Epoch 21: val_loss did not improve from 83.85735\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.7876 - val_loss: 90.2016 - lr: 0.0070\n",
      "Epoch 22/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 32.3564\n",
      "Epoch 22: val_loss did not improve from 83.85735\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.3714 - val_loss: 86.2868 - lr: 0.0070\n",
      "Epoch 23/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.6355\n",
      "Epoch 23: val_loss improved from 83.85735 to 82.14568, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.6404 - val_loss: 82.1457 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 27.1336\n",
      "Epoch 24: val_loss did not improve from 82.14568\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.9522 - val_loss: 92.6607 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 25.4068\n",
      "Epoch 25: val_loss did not improve from 82.14568\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.4090 - val_loss: 104.1009 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 32.4111\n",
      "Epoch 26: val_loss improved from 82.14568 to 81.98046, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.7814 - val_loss: 81.9805 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 26.1839\n",
      "Epoch 27: val_loss did not improve from 81.98046\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.6034 - val_loss: 86.4080 - lr: 0.0049\n",
      "Epoch 28/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 25.2790\n",
      "Epoch 28: val_loss improved from 81.98046 to 81.64524, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.1749 - val_loss: 81.6452 - lr: 0.0049\n",
      "Epoch 29/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 22.6873\n",
      "Epoch 29: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.5018 - val_loss: 82.1548 - lr: 0.0049\n",
      "Epoch 30/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 21.3935\n",
      "Epoch 30: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.2825 - val_loss: 101.8094 - lr: 0.0049\n",
      "Epoch 31/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 24.0713\n",
      "Epoch 31: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.3225 - val_loss: 103.8240 - lr: 0.0049\n",
      "Epoch 32/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 24.4790\n",
      "Epoch 32: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.8795 - val_loss: 83.8970 - lr: 0.0049\n",
      "Epoch 33/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 23.2948\n",
      "Epoch 33: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.2124 - val_loss: 84.9143 - lr: 0.0049\n",
      "Epoch 34/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 19.5213\n",
      "Epoch 34: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.3986 - val_loss: 84.0785 - lr: 0.0034\n",
      "Epoch 35/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 17.9132\n",
      "Epoch 35: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.9435 - val_loss: 81.6848 - lr: 0.0034\n",
      "Epoch 36/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 18.2916\n",
      "Epoch 36: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2679 - val_loss: 81.8261 - lr: 0.0034\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 16.3512\n",
      "Epoch 37: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3012 - val_loss: 93.2156 - lr: 0.0034\n",
      "Epoch 38/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 18.9790\n",
      "Epoch 38: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9434 - val_loss: 87.4884 - lr: 0.0034\n",
      "Epoch 39/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 16.1431\n",
      "Epoch 39: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1248 - val_loss: 84.5753 - lr: 0.0024\n",
      "Epoch 40/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.6962\n",
      "Epoch 40: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.6903 - val_loss: 86.6035 - lr: 0.0024\n",
      "Epoch 41/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.2015\n",
      "Epoch 41: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2333 - val_loss: 82.0014 - lr: 0.0024\n",
      "Epoch 42/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 15.3445\n",
      "Epoch 42: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.2931 - val_loss: 82.6690 - lr: 0.0024\n",
      "Epoch 43/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 15.8999\n",
      "Epoch 43: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8986 - val_loss: 84.5013 - lr: 0.0024\n",
      "Epoch 44/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.2436\n",
      "Epoch 44: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.1242 - val_loss: 82.3589 - lr: 0.0017\n",
      "Epoch 45/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.0531\n",
      "Epoch 45: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0349 - val_loss: 84.5319 - lr: 0.0017\n",
      "Epoch 46/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.3120\n",
      "Epoch 46: val_loss did not improve from 81.64524\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2976 - val_loss: 84.3980 - lr: 0.0017\n",
      "Epoch 47/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 12.9446\n",
      "Epoch 47: val_loss improved from 81.64524 to 81.10015, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.2184 - val_loss: 81.1002 - lr: 0.0017\n",
      "Epoch 48/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 12.1663\n",
      "Epoch 48: val_loss did not improve from 81.10015\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2108 - val_loss: 91.3655 - lr: 0.0017\n",
      "Epoch 49/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.3916\n",
      "Epoch 49: val_loss improved from 81.10015 to 81.09301, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2985 - val_loss: 81.0930 - lr: 0.0017\n",
      "Epoch 50/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 11.9797\n",
      "Epoch 50: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0356 - val_loss: 98.3855 - lr: 0.0017\n",
      "Epoch 51/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.8398\n",
      "Epoch 51: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9007 - val_loss: 86.6215 - lr: 0.0017\n",
      "Epoch 52/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 13.0316\n",
      "Epoch 52: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9203 - val_loss: 83.8520 - lr: 0.0017\n",
      "Epoch 53/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.2125\n",
      "Epoch 53: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1066 - val_loss: 91.5407 - lr: 0.0017\n",
      "Epoch 54/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.8007\n",
      "Epoch 54: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6979 - val_loss: 83.7996 - lr: 0.0017\n",
      "Epoch 55/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 10.7601\n",
      "Epoch 55: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7928 - val_loss: 81.7767 - lr: 0.0012\n",
      "Epoch 56/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.0123\n",
      "Epoch 56: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9951 - val_loss: 84.2143 - lr: 0.0012\n",
      "Epoch 57/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.9589\n",
      "Epoch 57: val_loss did not improve from 81.09301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9980 - val_loss: 81.9868 - lr: 0.0012\n",
      "Epoch 58/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.6518\n",
      "Epoch 58: val_loss improved from 81.09301 to 80.61250, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6281 - val_loss: 80.6125 - lr: 0.0012\n",
      "Epoch 59/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.0341\n",
      "Epoch 59: val_loss did not improve from 80.61250\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0345 - val_loss: 83.3548 - lr: 0.0012\n",
      "Epoch 60/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.4057\n",
      "Epoch 60: val_loss did not improve from 80.61250\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3925 - val_loss: 84.8304 - lr: 0.0012\n",
      "Epoch 61/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.8222\n",
      "Epoch 61: val_loss did not improve from 80.61250\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.7579 - val_loss: 82.3699 - lr: 0.0012\n",
      "Epoch 62/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 10.6274\n",
      "Epoch 62: val_loss did not improve from 80.61250\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6088 - val_loss: 81.0698 - lr: 0.0012\n",
      "Epoch 63/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.6478\n",
      "Epoch 63: val_loss did not improve from 80.61250\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5578 - val_loss: 82.9116 - lr: 0.0012\n",
      "Epoch 64/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.6341\n",
      "Epoch 64: val_loss improved from 80.61250 to 79.60585, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6068 - val_loss: 79.6059 - lr: 8.2354e-04\n",
      "Epoch 65/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 9.7034\n",
      "Epoch 65: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7700 - val_loss: 81.7603 - lr: 8.2354e-04\n",
      "Epoch 66/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.0397\n",
      "Epoch 66: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0278 - val_loss: 81.7685 - lr: 8.2354e-04\n",
      "Epoch 67/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 9.4495\n",
      "Epoch 67: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4606 - val_loss: 82.2582 - lr: 8.2354e-04\n",
      "Epoch 68/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 8.7322\n",
      "Epoch 68: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7511 - val_loss: 84.5199 - lr: 8.2354e-04\n",
      "Epoch 69/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 9.1615\n",
      "Epoch 69: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0064 - val_loss: 82.7593 - lr: 8.2354e-04\n",
      "Epoch 70/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.5157\n",
      "Epoch 70: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5648 - val_loss: 82.6485 - lr: 5.7648e-04\n",
      "Epoch 71/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.5738\n",
      "Epoch 71: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6460 - val_loss: 81.1429 - lr: 5.7648e-04\n",
      "Epoch 72/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.5126\n",
      "Epoch 72: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5559 - val_loss: 82.9518 - lr: 5.7648e-04\n",
      "Epoch 73/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.7757\n",
      "Epoch 73: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8880 - val_loss: 87.4860 - lr: 5.7648e-04\n",
      "Epoch 74/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.0394\n",
      "Epoch 74: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0448 - val_loss: 82.6547 - lr: 5.7648e-04\n",
      "Epoch 75/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.1015\n",
      "Epoch 75: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1309 - val_loss: 85.3851 - lr: 4.0354e-04\n",
      "Epoch 76/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 8.1622\n",
      "Epoch 76: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1758 - val_loss: 85.0025 - lr: 4.0354e-04\n",
      "Epoch 77/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 7.9227\n",
      "Epoch 77: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.8995 - val_loss: 82.3418 - lr: 4.0354e-04\n",
      "Epoch 78/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.2054\n",
      "Epoch 78: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2532 - val_loss: 81.0704 - lr: 4.0354e-04\n",
      "Epoch 79/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 8.0270\n",
      "Epoch 79: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0153 - val_loss: 82.5823 - lr: 4.0354e-04\n",
      "Epoch 80/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 7.5455\n",
      "Epoch 80: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.5738 - val_loss: 85.3212 - lr: 2.8248e-04\n",
      "Epoch 81/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 7.8298\n",
      "Epoch 81: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.8533 - val_loss: 84.6291 - lr: 2.8248e-04\n",
      "Epoch 82/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.5173\n",
      "Epoch 82: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.4824 - val_loss: 84.2704 - lr: 2.8248e-04\n",
      "Epoch 83/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.2682\n",
      "Epoch 83: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.3528 - val_loss: 85.0444 - lr: 2.8248e-04\n",
      "Epoch 84/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.8421\n",
      "Epoch 84: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7720 - val_loss: 84.6770 - lr: 2.8248e-04\n",
      "Epoch 85/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 7.7727\n",
      "Epoch 85: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7186 - val_loss: 82.5794 - lr: 1.9773e-04\n",
      "Epoch 86/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 7.3665\n",
      "Epoch 86: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.4007 - val_loss: 82.2158 - lr: 1.9773e-04\n",
      "Epoch 87/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.3758\n",
      "Epoch 87: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.3234 - val_loss: 85.9162 - lr: 1.9773e-04\n",
      "Epoch 88/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 7.0589\n",
      "Epoch 88: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0501 - val_loss: 82.2802 - lr: 1.9773e-04\n",
      "Epoch 89/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 7.1555\n",
      "Epoch 89: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1539 - val_loss: 83.3077 - lr: 1.9773e-04\n",
      "Epoch 90/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.2562\n",
      "Epoch 90: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1636 - val_loss: 85.3687 - lr: 1.3841e-04\n",
      "Epoch 91/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 7.0495\n",
      "Epoch 91: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0766 - val_loss: 84.5074 - lr: 1.3841e-04\n",
      "Epoch 92/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.3694\n",
      "Epoch 92: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.4493 - val_loss: 83.7872 - lr: 1.3841e-04\n",
      "Epoch 93/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.1669\n",
      "Epoch 93: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0997 - val_loss: 82.8493 - lr: 1.3841e-04\n",
      "Epoch 94/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.8585\n",
      "Epoch 94: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9252 - val_loss: 82.7787 - lr: 1.3841e-04\n",
      "Epoch 95/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 7.0195\n",
      "Epoch 95: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0365 - val_loss: 83.6519 - lr: 9.6889e-05\n",
      "Epoch 96/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.8597\n",
      "Epoch 96: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8764 - val_loss: 84.0185 - lr: 9.6889e-05\n",
      "Epoch 97/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.0790\n",
      "Epoch 97: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9790 - val_loss: 83.8025 - lr: 9.6889e-05\n",
      "Epoch 98/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.7940\n",
      "Epoch 98: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9017 - val_loss: 84.0058 - lr: 9.6889e-05\n",
      "Epoch 99/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.2316\n",
      "Epoch 99: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2295 - val_loss: 83.4274 - lr: 9.6889e-05\n",
      "Epoch 100/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.8172\n",
      "Epoch 100: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8632 - val_loss: 84.7631 - lr: 6.7822e-05\n",
      "Epoch 101/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6.9473\n",
      "Epoch 101: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8505 - val_loss: 85.4395 - lr: 6.7822e-05\n",
      "Epoch 102/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 6.7589\n",
      "Epoch 102: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.7413 - val_loss: 82.7399 - lr: 6.7822e-05\n",
      "Epoch 103/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 6.8027\n",
      "Epoch 103: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8568 - val_loss: 83.3155 - lr: 6.7822e-05\n",
      "Epoch 104/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6.9015\n",
      "Epoch 104: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9092 - val_loss: 83.2549 - lr: 6.7822e-05\n",
      "Epoch 105/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 6.9762\n",
      "Epoch 105: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.0203 - val_loss: 84.6640 - lr: 4.7476e-05\n",
      "Epoch 106/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.7427\n",
      "Epoch 106: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7120 - val_loss: 83.9146 - lr: 4.7476e-05\n",
      "Epoch 107/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.8924\n",
      "Epoch 107: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9579 - val_loss: 83.9545 - lr: 4.7476e-05\n",
      "Epoch 108/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.8979\n",
      "Epoch 108: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9324 - val_loss: 83.3403 - lr: 4.7476e-05\n",
      "Epoch 109/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.7431\n",
      "Epoch 109: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6723 - val_loss: 83.0210 - lr: 4.7476e-05\n",
      "Epoch 110/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 6.5008\n",
      "Epoch 110: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.4724 - val_loss: 82.8115 - lr: 3.3233e-05\n",
      "Epoch 111/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 6.8156\n",
      "Epoch 111: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8131 - val_loss: 83.7671 - lr: 3.3233e-05\n",
      "Epoch 112/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 6.7931\n",
      "Epoch 112: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8167 - val_loss: 83.7784 - lr: 3.3233e-05\n",
      "Epoch 113/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.8657\n",
      "Epoch 113: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9120 - val_loss: 84.1174 - lr: 3.3233e-05\n",
      "Epoch 114/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 6.8644\n",
      "Epoch 114: val_loss did not improve from 79.60585\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8314 - val_loss: 83.8792 - lr: 3.3233e-05\n",
      "21/21 [==============================] - 0s 1000us/step\n",
      "Epoch 1/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 400.7614\n",
      "Epoch 1: val_loss improved from inf to 122.10739, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 373.7597 - val_loss: 122.1074 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 143.7822\n",
      "Epoch 2: val_loss improved from 122.10739 to 95.35526, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 142.9460 - val_loss: 95.3553 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 119.1605\n",
      "Epoch 3: val_loss did not improve from 95.35526\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 119.1605 - val_loss: 106.6532 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 102.6153\n",
      "Epoch 4: val_loss did not improve from 95.35526\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.3620 - val_loss: 253.1350 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 98.4924\n",
      "Epoch 5: val_loss did not improve from 95.35526\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 99.0611 - val_loss: 173.6838 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 92.3864\n",
      "Epoch 6: val_loss did not improve from 95.35526\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 92.4951 - val_loss: 117.8946 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 81.1480\n",
      "Epoch 7: val_loss did not improve from 95.35526\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.5647 - val_loss: 101.0825 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 74.8278\n",
      "Epoch 8: val_loss improved from 95.35526 to 85.20342, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 74.3145 - val_loss: 85.2034 - lr: 0.0070\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 63.7332\n",
      "Epoch 9: val_loss did not improve from 85.20342\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.8412 - val_loss: 118.1672 - lr: 0.0070\n",
      "Epoch 10/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 68.5597\n",
      "Epoch 10: val_loss did not improve from 85.20342\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 68.5378 - val_loss: 171.2263 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 63.6196\n",
      "Epoch 11: val_loss did not improve from 85.20342\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 62.9645 - val_loss: 155.2476 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 67.3750\n",
      "Epoch 12: val_loss improved from 85.20342 to 80.66248, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.2555 - val_loss: 80.6625 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 55.7638\n",
      "Epoch 13: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 55.8702 - val_loss: 81.0288 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 43.4624\n",
      "Epoch 14: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 43.3338 - val_loss: 86.3602 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 46.7682\n",
      "Epoch 15: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 47.3996 - val_loss: 106.3134 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 40.0414\n",
      "Epoch 16: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.8849 - val_loss: 90.3526 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 40.5113\n",
      "Epoch 17: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.2705 - val_loss: 89.9158 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 30.5195\n",
      "Epoch 18: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.4490 - val_loss: 91.5650 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 43.7603\n",
      "Epoch 19: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.6881 - val_loss: 90.0519 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.4808\n",
      "Epoch 20: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.1691 - val_loss: 89.0118 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 28.0841\n",
      "Epoch 21: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.7444 - val_loss: 96.4784 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 24.1699\n",
      "Epoch 22: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.3402 - val_loss: 88.0586 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 23.0865\n",
      "Epoch 23: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.1377 - val_loss: 104.3335 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 22.5448\n",
      "Epoch 24: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 22.5107 - val_loss: 96.0577 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 21.3181\n",
      "Epoch 25: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.2718 - val_loss: 90.5415 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 19.2780\n",
      "Epoch 26: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.2000 - val_loss: 86.9099 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 18.7228\n",
      "Epoch 27: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.6617 - val_loss: 84.0642 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 16.3533\n",
      "Epoch 28: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4677 - val_loss: 96.7897 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.5991\n",
      "Epoch 29: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.8346 - val_loss: 91.0415 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 17.4285\n",
      "Epoch 30: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.5534 - val_loss: 84.0636 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 16.5601\n",
      "Epoch 31: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4124 - val_loss: 93.8078 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.1828\n",
      "Epoch 32: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.0557 - val_loss: 87.3624 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 14.3090\n",
      "Epoch 33: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.3624 - val_loss: 87.9223 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 14.3909\n",
      "Epoch 34: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.3868 - val_loss: 83.9648 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.9839\n",
      "Epoch 35: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0706 - val_loss: 93.3227 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 14.0861\n",
      "Epoch 36: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0518 - val_loss: 90.3254 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.5672\n",
      "Epoch 37: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.4699 - val_loss: 90.6085 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 12.9857\n",
      "Epoch 38: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0942 - val_loss: 95.6360 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.7533\n",
      "Epoch 39: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6477 - val_loss: 95.5472 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.5968\n",
      "Epoch 40: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.1336 - val_loss: 87.0164 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.8347\n",
      "Epoch 41: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8313 - val_loss: 91.3881 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 12.4167\n",
      "Epoch 42: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3322 - val_loss: 86.6539 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.1711\n",
      "Epoch 43: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2919 - val_loss: 87.7956 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.4096\n",
      "Epoch 44: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.4810 - val_loss: 82.2046 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.4890\n",
      "Epoch 45: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5232 - val_loss: 85.9164 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.0953\n",
      "Epoch 46: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1510 - val_loss: 89.4547 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.7987\n",
      "Epoch 47: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.8956 - val_loss: 90.1882 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.5887\n",
      "Epoch 48: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4977 - val_loss: 90.6276 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.3679\n",
      "Epoch 49: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4355 - val_loss: 85.7073 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.5210\n",
      "Epoch 50: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4216 - val_loss: 88.5457 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.3214\n",
      "Epoch 51: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2728 - val_loss: 90.0558 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.2337\n",
      "Epoch 52: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2035 - val_loss: 89.6464 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.0509\n",
      "Epoch 53: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0255 - val_loss: 87.0983 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.7243\n",
      "Epoch 54: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.8238 - val_loss: 88.0190 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 10.2846\n",
      "Epoch 55: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2454 - val_loss: 88.3839 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.2516\n",
      "Epoch 56: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3862 - val_loss: 88.0809 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.3266\n",
      "Epoch 57: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2505 - val_loss: 87.6059 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 9.6342\n",
      "Epoch 58: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5717 - val_loss: 92.3844 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.3573\n",
      "Epoch 59: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5140 - val_loss: 90.3948 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 8.8870\n",
      "Epoch 60: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9016 - val_loss: 87.8686 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.8724\n",
      "Epoch 61: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7664 - val_loss: 89.4167 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.3284\n",
      "Epoch 62: val_loss did not improve from 80.66248\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.3502 - val_loss: 89.8239 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 474.6006\n",
      "Epoch 1: val_loss improved from inf to 145.99594, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 447.3967 - val_loss: 145.9959 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 158.1119\n",
      "Epoch 2: val_loss improved from 145.99594 to 129.22583, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 152.2751 - val_loss: 129.2258 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 109.8648\n",
      "Epoch 3: val_loss improved from 129.22583 to 118.61381, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 110.0233 - val_loss: 118.6138 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 106.1609\n",
      "Epoch 4: val_loss did not improve from 118.61381\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 105.8773 - val_loss: 129.3794 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 109.4997\n",
      "Epoch 5: val_loss improved from 118.61381 to 107.55373, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 107.3425 - val_loss: 107.5537 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 83.2293\n",
      "Epoch 6: val_loss improved from 107.55373 to 106.46244, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 83.4617 - val_loss: 106.4624 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 98.7024 \n",
      "Epoch 7: val_loss did not improve from 106.46244\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 98.7415 - val_loss: 106.8344 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 83.7309\n",
      "Epoch 8: val_loss did not improve from 106.46244\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 84.5378 - val_loss: 106.7500 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 74.5605\n",
      "Epoch 9: val_loss did not improve from 106.46244\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 73.0802 - val_loss: 146.2148 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 71.1422\n",
      "Epoch 10: val_loss did not improve from 106.46244\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 69.4595 - val_loss: 144.5478 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 77.9109\n",
      "Epoch 11: val_loss did not improve from 106.46244\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 76.5673 - val_loss: 139.2810 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 54.6170\n",
      "Epoch 12: val_loss improved from 106.46244 to 106.24103, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.5074 - val_loss: 106.2410 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 49.6226\n",
      "Epoch 13: val_loss did not improve from 106.24103\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 49.5129 - val_loss: 155.3993 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 44.6555\n",
      "Epoch 14: val_loss did not improve from 106.24103\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.0252 - val_loss: 122.7146 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 41.3812\n",
      "Epoch 15: val_loss did not improve from 106.24103\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.3012 - val_loss: 151.1724 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 47.4588\n",
      "Epoch 16: val_loss improved from 106.24103 to 105.66134, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.8241 - val_loss: 105.6613 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 38.1438\n",
      "Epoch 17: val_loss improved from 105.66134 to 99.15472, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.2836 - val_loss: 99.1547 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 39.5388\n",
      "Epoch 18: val_loss did not improve from 99.15472\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.5721 - val_loss: 150.5358 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 34.9353\n",
      "Epoch 19: val_loss did not improve from 99.15472\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.8795 - val_loss: 107.5879 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 34.0592\n",
      "Epoch 20: val_loss improved from 99.15472 to 98.38289, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.6562 - val_loss: 98.3829 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.3478\n",
      "Epoch 21: val_loss did not improve from 98.38289\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.1479 - val_loss: 103.8929 - lr: 0.0070\n",
      "Epoch 22/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 29.8162\n",
      "Epoch 22: val_loss did not improve from 98.38289\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.0391 - val_loss: 106.5294 - lr: 0.0070\n",
      "Epoch 23/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 28.8257\n",
      "Epoch 23: val_loss did not improve from 98.38289\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.3307 - val_loss: 142.7235 - lr: 0.0070\n",
      "Epoch 24/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 28.2811\n",
      "Epoch 24: val_loss did not improve from 98.38289\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.3302 - val_loss: 134.8027 - lr: 0.0070\n",
      "Epoch 25/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 29.1040\n",
      "Epoch 25: val_loss did not improve from 98.38289\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.3459 - val_loss: 103.6996 - lr: 0.0070\n",
      "Epoch 26/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 22.7624\n",
      "Epoch 26: val_loss improved from 98.38289 to 94.33965, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.5812 - val_loss: 94.3397 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 19.6005\n",
      "Epoch 27: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.7830 - val_loss: 98.7174 - lr: 0.0049\n",
      "Epoch 28/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 21.6214\n",
      "Epoch 28: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.6447 - val_loss: 97.1914 - lr: 0.0049\n",
      "Epoch 29/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 20.7679\n",
      "Epoch 29: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.1390 - val_loss: 96.1113 - lr: 0.0049\n",
      "Epoch 30/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 19.9209\n",
      "Epoch 30: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6270 - val_loss: 95.8580 - lr: 0.0049\n",
      "Epoch 31/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 20.1270\n",
      "Epoch 31: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8909 - val_loss: 100.0906 - lr: 0.0049\n",
      "Epoch 32/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 17.5218\n",
      "Epoch 32: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4966 - val_loss: 99.1947 - lr: 0.0034\n",
      "Epoch 33/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.2746\n",
      "Epoch 33: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.3857 - val_loss: 103.6578 - lr: 0.0034\n",
      "Epoch 34/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.5487\n",
      "Epoch 34: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4629 - val_loss: 102.3429 - lr: 0.0034\n",
      "Epoch 35/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 17.2167\n",
      "Epoch 35: val_loss did not improve from 94.33965\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6258 - val_loss: 96.8279 - lr: 0.0034\n",
      "Epoch 36/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 14.8481\n",
      "Epoch 36: val_loss improved from 94.33965 to 92.25415, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8685 - val_loss: 92.2542 - lr: 0.0034\n",
      "Epoch 37/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 15.2475\n",
      "Epoch 37: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1574 - val_loss: 93.2066 - lr: 0.0034\n",
      "Epoch 38/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.5369\n",
      "Epoch 38: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4812 - val_loss: 94.8582 - lr: 0.0034\n",
      "Epoch 39/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 14.5108\n",
      "Epoch 39: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.4836 - val_loss: 97.2290 - lr: 0.0034\n",
      "Epoch 40/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 14.1221\n",
      "Epoch 40: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2438 - val_loss: 101.8497 - lr: 0.0034\n",
      "Epoch 41/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.7026\n",
      "Epoch 41: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6740 - val_loss: 94.0585 - lr: 0.0034\n",
      "Epoch 42/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 12.9496\n",
      "Epoch 42: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.8907 - val_loss: 97.1441 - lr: 0.0024\n",
      "Epoch 43/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.0976\n",
      "Epoch 43: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9678 - val_loss: 93.7755 - lr: 0.0024\n",
      "Epoch 44/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 11.7310\n",
      "Epoch 44: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.6850 - val_loss: 97.0535 - lr: 0.0024\n",
      "Epoch 45/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.6543\n",
      "Epoch 45: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6687 - val_loss: 94.9985 - lr: 0.0024\n",
      "Epoch 46/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.3371\n",
      "Epoch 46: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3274 - val_loss: 99.1466 - lr: 0.0024\n",
      "Epoch 47/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.1149\n",
      "Epoch 47: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0082 - val_loss: 99.0659 - lr: 0.0017\n",
      "Epoch 48/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 10.6075\n",
      "Epoch 48: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6724 - val_loss: 95.6372 - lr: 0.0017\n",
      "Epoch 49/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.4227\n",
      "Epoch 49: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.4276 - val_loss: 94.9645 - lr: 0.0017\n",
      "Epoch 50/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.3205\n",
      "Epoch 50: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3312 - val_loss: 99.9368 - lr: 0.0017\n",
      "Epoch 51/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.5780\n",
      "Epoch 51: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5647 - val_loss: 99.8103 - lr: 0.0017\n",
      "Epoch 52/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.4968\n",
      "Epoch 52: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.4856 - val_loss: 96.9375 - lr: 0.0012\n",
      "Epoch 53/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.5246\n",
      "Epoch 53: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.5271 - val_loss: 99.6089 - lr: 0.0012\n",
      "Epoch 54/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.1966\n",
      "Epoch 54: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1378 - val_loss: 97.0173 - lr: 0.0012\n",
      "Epoch 55/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.4819\n",
      "Epoch 55: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.6615 - val_loss: 96.8545 - lr: 0.0012\n",
      "Epoch 56/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.2294\n",
      "Epoch 56: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4315 - val_loss: 96.9013 - lr: 0.0012\n",
      "Epoch 57/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.9805\n",
      "Epoch 57: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0057 - val_loss: 92.5625 - lr: 8.2354e-04\n",
      "Epoch 58/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.4952\n",
      "Epoch 58: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4538 - val_loss: 97.9012 - lr: 8.2354e-04\n",
      "Epoch 59/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 8.6679\n",
      "Epoch 59: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7005 - val_loss: 99.7385 - lr: 8.2354e-04\n",
      "Epoch 60/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.6575\n",
      "Epoch 60: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7575 - val_loss: 96.5919 - lr: 8.2354e-04\n",
      "Epoch 61/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.9385\n",
      "Epoch 61: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8257 - val_loss: 97.9822 - lr: 8.2354e-04\n",
      "Epoch 62/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.5649\n",
      "Epoch 62: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.5292 - val_loss: 94.3264 - lr: 5.7648e-04\n",
      "Epoch 63/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.7449\n",
      "Epoch 63: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.7139 - val_loss: 100.3479 - lr: 5.7648e-04\n",
      "Epoch 64/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 8.2518\n",
      "Epoch 64: val_loss did not improve from 92.25415\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2485 - val_loss: 93.3858 - lr: 5.7648e-04\n",
      "Epoch 65/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 7.9696\n",
      "Epoch 65: val_loss improved from 92.25415 to 91.85985, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0172 - val_loss: 91.8598 - lr: 5.7648e-04\n",
      "Epoch 66/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 8.4204\n",
      "Epoch 66: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3589 - val_loss: 94.0199 - lr: 5.7648e-04\n",
      "Epoch 67/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.1547\n",
      "Epoch 67: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1353 - val_loss: 98.2446 - lr: 5.7648e-04\n",
      "Epoch 68/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.0236\n",
      "Epoch 68: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.8742 - val_loss: 97.5668 - lr: 5.7648e-04\n",
      "Epoch 69/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 8.0611\n",
      "Epoch 69: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0738 - val_loss: 97.9395 - lr: 5.7648e-04\n",
      "Epoch 70/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.3612\n",
      "Epoch 70: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2806 - val_loss: 96.7777 - lr: 5.7648e-04\n",
      "Epoch 71/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 7.7453\n",
      "Epoch 71: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.7897 - val_loss: 95.4464 - lr: 4.0354e-04\n",
      "Epoch 72/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.5489\n",
      "Epoch 72: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.5657 - val_loss: 94.4514 - lr: 4.0354e-04\n",
      "Epoch 73/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.6429\n",
      "Epoch 73: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.6170 - val_loss: 93.2688 - lr: 4.0354e-04\n",
      "Epoch 74/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 7.2981\n",
      "Epoch 74: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2981 - val_loss: 97.5165 - lr: 4.0354e-04\n",
      "Epoch 75/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.7620\n",
      "Epoch 75: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.6901 - val_loss: 93.8075 - lr: 4.0354e-04\n",
      "Epoch 76/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.4304\n",
      "Epoch 76: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.4692 - val_loss: 99.8583 - lr: 2.8248e-04\n",
      "Epoch 77/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 7.1892\n",
      "Epoch 77: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1591 - val_loss: 93.7760 - lr: 2.8248e-04\n",
      "Epoch 78/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 7.1948\n",
      "Epoch 78: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2539 - val_loss: 96.4700 - lr: 2.8248e-04\n",
      "Epoch 79/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 6.9249\n",
      "Epoch 79: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9040 - val_loss: 95.2797 - lr: 2.8248e-04\n",
      "Epoch 80/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.2301\n",
      "Epoch 80: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1539 - val_loss: 95.4757 - lr: 2.8248e-04\n",
      "Epoch 81/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.2162\n",
      "Epoch 81: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2058 - val_loss: 94.5971 - lr: 1.9773e-04\n",
      "Epoch 82/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 6.8715\n",
      "Epoch 82: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8478 - val_loss: 96.6520 - lr: 1.9773e-04\n",
      "Epoch 83/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.5323\n",
      "Epoch 83: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5728 - val_loss: 94.6476 - lr: 1.9773e-04\n",
      "Epoch 84/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 7.0267\n",
      "Epoch 84: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9942 - val_loss: 95.8883 - lr: 1.9773e-04\n",
      "Epoch 85/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 7.2855\n",
      "Epoch 85: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.2100 - val_loss: 95.7987 - lr: 1.9773e-04\n",
      "Epoch 86/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 6.7229\n",
      "Epoch 86: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6356 - val_loss: 95.6856 - lr: 1.3841e-04\n",
      "Epoch 87/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 6.7292\n",
      "Epoch 87: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7243 - val_loss: 96.2189 - lr: 1.3841e-04\n",
      "Epoch 88/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.7802\n",
      "Epoch 88: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6790 - val_loss: 97.3132 - lr: 1.3841e-04\n",
      "Epoch 89/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.1666\n",
      "Epoch 89: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.1600 - val_loss: 95.5272 - lr: 1.3841e-04\n",
      "Epoch 90/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 6.9441\n",
      "Epoch 90: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.9314 - val_loss: 94.1636 - lr: 1.3841e-04\n",
      "Epoch 91/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 6.5155\n",
      "Epoch 91: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5487 - val_loss: 94.8726 - lr: 9.6889e-05\n",
      "Epoch 92/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.7658\n",
      "Epoch 92: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7652 - val_loss: 97.2298 - lr: 9.6889e-05\n",
      "Epoch 93/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 6.7648\n",
      "Epoch 93: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8105 - val_loss: 96.3148 - lr: 9.6889e-05\n",
      "Epoch 94/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 6.6480\n",
      "Epoch 94: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6480 - val_loss: 94.6512 - lr: 9.6889e-05\n",
      "Epoch 95/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6.8033\n",
      "Epoch 95: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7885 - val_loss: 96.2895 - lr: 9.6889e-05\n",
      "Epoch 96/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.2633\n",
      "Epoch 96: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 7.1947 - val_loss: 96.0171 - lr: 6.7822e-05\n",
      "Epoch 97/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.7777\n",
      "Epoch 97: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7787 - val_loss: 95.2145 - lr: 6.7822e-05\n",
      "Epoch 98/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6.6290\n",
      "Epoch 98: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.5745 - val_loss: 96.3966 - lr: 6.7822e-05\n",
      "Epoch 99/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 6.6164\n",
      "Epoch 99: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6089 - val_loss: 95.1337 - lr: 6.7822e-05\n",
      "Epoch 100/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6.8026\n",
      "Epoch 100: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8170 - val_loss: 95.1548 - lr: 6.7822e-05\n",
      "Epoch 101/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.6314\n",
      "Epoch 101: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5559 - val_loss: 95.3690 - lr: 4.7476e-05\n",
      "Epoch 102/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.6487\n",
      "Epoch 102: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6903 - val_loss: 96.2462 - lr: 4.7476e-05\n",
      "Epoch 103/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.7082\n",
      "Epoch 103: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.6512 - val_loss: 94.6849 - lr: 4.7476e-05\n",
      "Epoch 104/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.8584\n",
      "Epoch 104: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.8222 - val_loss: 96.4862 - lr: 4.7476e-05\n",
      "Epoch 105/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 6.2787\n",
      "Epoch 105: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3350 - val_loss: 95.3487 - lr: 4.7476e-05\n",
      "Epoch 106/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.5873\n",
      "Epoch 106: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5989 - val_loss: 95.0323 - lr: 3.3233e-05\n",
      "Epoch 107/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.7201\n",
      "Epoch 107: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.7668 - val_loss: 95.3845 - lr: 3.3233e-05\n",
      "Epoch 108/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 6.6580\n",
      "Epoch 108: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.6580 - val_loss: 94.7718 - lr: 3.3233e-05\n",
      "Epoch 109/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 6.5624\n",
      "Epoch 109: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5850 - val_loss: 96.0130 - lr: 3.3233e-05\n",
      "Epoch 110/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 6.4952\n",
      "Epoch 110: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5012 - val_loss: 95.5632 - lr: 3.3233e-05\n",
      "Epoch 111/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 6.6528\n",
      "Epoch 111: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.4942 - val_loss: 95.9878 - lr: 2.3263e-05\n",
      "Epoch 112/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 6.2387\n",
      "Epoch 112: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3124 - val_loss: 95.0436 - lr: 2.3263e-05\n",
      "Epoch 113/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 6.3364\n",
      "Epoch 113: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.3990 - val_loss: 95.7018 - lr: 2.3263e-05\n",
      "Epoch 114/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 6.5407\n",
      "Epoch 114: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.5586 - val_loss: 96.0423 - lr: 2.3263e-05\n",
      "Epoch 115/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 6.3369\n",
      "Epoch 115: val_loss did not improve from 91.85985\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 6.3405 - val_loss: 95.1196 - lr: 2.3263e-05\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 628.2098\n",
      "Epoch 1: val_loss improved from inf to 151.71312, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 4ms/step - loss: 570.7687 - val_loss: 151.7131 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 162.5887\n",
      "Epoch 2: val_loss improved from 151.71312 to 103.20685, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 161.5365 - val_loss: 103.2068 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 125.2896\n",
      "Epoch 3: val_loss improved from 103.20685 to 90.09459, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 125.0259 - val_loss: 90.0946 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 115.1634\n",
      "Epoch 4: val_loss did not improve from 90.09459\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 114.6711 - val_loss: 95.4915 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 110.6334\n",
      "Epoch 5: val_loss improved from 90.09459 to 85.43939, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 109.5394 - val_loss: 85.4394 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 94.6635\n",
      "Epoch 6: val_loss did not improve from 85.43939\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 94.2489 - val_loss: 97.9222 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 86.9430\n",
      "Epoch 7: val_loss did not improve from 85.43939\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 87.7891 - val_loss: 106.1201 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 89.4037\n",
      "Epoch 8: val_loss did not improve from 85.43939\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 88.7010 - val_loss: 87.7417 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 77.2953\n",
      "Epoch 9: val_loss improved from 85.43939 to 80.25880, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 76.3979 - val_loss: 80.2588 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 73.8694\n",
      "Epoch 10: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 73.8369 - val_loss: 104.8877 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 65.8425\n",
      "Epoch 11: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.0763 - val_loss: 133.2164 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 75.8271\n",
      "Epoch 12: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 75.9161 - val_loss: 97.5388 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 61.7394\n",
      "Epoch 13: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 62.1506 - val_loss: 83.7795 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 64.7787\n",
      "Epoch 14: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 64.5914 - val_loss: 82.4703 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 46.1918\n",
      "Epoch 15: val_loss did not improve from 80.25880\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 46.3995 - val_loss: 103.9521 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 48.8954\n",
      "Epoch 16: val_loss improved from 80.25880 to 79.99936, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 49.1267 - val_loss: 79.9994 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 45.5412\n",
      "Epoch 17: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.8960 - val_loss: 82.2590 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 41.7236\n",
      "Epoch 18: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.8948 - val_loss: 114.4627 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 36.9023\n",
      "Epoch 19: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.9159 - val_loss: 100.3681 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 39.9808\n",
      "Epoch 20: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.7708 - val_loss: 97.3416 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 39.3225\n",
      "Epoch 21: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.9269 - val_loss: 91.6463 - lr: 0.0070\n",
      "Epoch 22/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 31.1962\n",
      "Epoch 22: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.8271 - val_loss: 99.3697 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 26.6580\n",
      "Epoch 23: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.1800 - val_loss: 123.2891 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 35.3792\n",
      "Epoch 24: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.1231 - val_loss: 87.2464 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 27.6494\n",
      "Epoch 25: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.5500 - val_loss: 106.0293 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 24.0891\n",
      "Epoch 26: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0478 - val_loss: 88.5768 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 21.1476\n",
      "Epoch 27: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.3102 - val_loss: 94.7854 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 19.1163\n",
      "Epoch 28: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.2951 - val_loss: 91.1751 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 19.2955\n",
      "Epoch 29: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.5421 - val_loss: 96.5572 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 17.9885\n",
      "Epoch 30: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.1113 - val_loss: 86.6332 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.7734\n",
      "Epoch 31: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.6802 - val_loss: 86.1286 - lr: 0.0034\n",
      "Epoch 32/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 17.7438\n",
      "Epoch 32: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.5527 - val_loss: 87.2494 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 16.2436\n",
      "Epoch 33: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.3885 - val_loss: 81.6606 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 16.7210\n",
      "Epoch 34: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.8233 - val_loss: 85.4696 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 16.0033\n",
      "Epoch 35: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.2381 - val_loss: 97.1139 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 15.6465\n",
      "Epoch 36: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6851 - val_loss: 92.3577 - lr: 0.0024\n",
      "Epoch 37/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 14.1355\n",
      "Epoch 37: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.1414 - val_loss: 87.9891 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.0946\n",
      "Epoch 38: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.1459 - val_loss: 90.0449 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 13.7062\n",
      "Epoch 39: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.8411 - val_loss: 89.2158 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.7895\n",
      "Epoch 40: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7515 - val_loss: 82.2178 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.0130\n",
      "Epoch 41: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0952 - val_loss: 84.4775 - lr: 0.0017\n",
      "Epoch 42/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.5003\n",
      "Epoch 42: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2661 - val_loss: 83.1394 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.4257\n",
      "Epoch 43: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.3026 - val_loss: 91.3591 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.5413\n",
      "Epoch 44: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5885 - val_loss: 84.2581 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.8237\n",
      "Epoch 45: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9323 - val_loss: 89.4668 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.8311\n",
      "Epoch 46: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8388 - val_loss: 95.4419 - lr: 0.0012\n",
      "Epoch 47/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.9311\n",
      "Epoch 47: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7513 - val_loss: 90.5214 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.1885\n",
      "Epoch 48: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1885 - val_loss: 89.1705 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.3768\n",
      "Epoch 49: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3653 - val_loss: 89.3448 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.9095\n",
      "Epoch 50: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9879 - val_loss: 84.1538 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.7706\n",
      "Epoch 51: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8040 - val_loss: 89.0577 - lr: 8.2354e-04\n",
      "Epoch 52/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.5595\n",
      "Epoch 52: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5466 - val_loss: 84.3914 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.0496\n",
      "Epoch 53: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0253 - val_loss: 87.0655 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.3312\n",
      "Epoch 54: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.3163 - val_loss: 93.6431 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.9410\n",
      "Epoch 55: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8330 - val_loss: 83.7965 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.5031\n",
      "Epoch 56: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5567 - val_loss: 82.5731 - lr: 5.7648e-04\n",
      "Epoch 57/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.9810\n",
      "Epoch 57: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9691 - val_loss: 85.7368 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.9099\n",
      "Epoch 58: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.9099 - val_loss: 88.7226 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.5092\n",
      "Epoch 59: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5360 - val_loss: 88.1525 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.5889\n",
      "Epoch 60: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5377 - val_loss: 89.6658 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.1059\n",
      "Epoch 61: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1349 - val_loss: 86.6109 - lr: 4.0354e-04\n",
      "Epoch 62/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 9.9502\n",
      "Epoch 62: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.9784 - val_loss: 85.3868 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 9.7817\n",
      "Epoch 63: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9420 - val_loss: 84.9292 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 10.0515\n",
      "Epoch 64: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9478 - val_loss: 86.6986 - lr: 2.8248e-04\n",
      "Epoch 65/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.1882\n",
      "Epoch 65: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2745 - val_loss: 84.4728 - lr: 2.8248e-04\n",
      "Epoch 66/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.1757\n",
      "Epoch 66: val_loss did not improve from 79.99936\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1803 - val_loss: 87.3566 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 931us/step\n",
      "Epoch 1/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 591.4784\n",
      "Epoch 1: val_loss improved from inf to 231.77628, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 576.8171 - val_loss: 231.7763 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 168.8612\n",
      "Epoch 2: val_loss improved from 231.77628 to 143.49348, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 164.6252 - val_loss: 143.4935 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 117.2702\n",
      "Epoch 3: val_loss improved from 143.49348 to 108.69668, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 117.3732 - val_loss: 108.6967 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 111.2798\n",
      "Epoch 4: val_loss did not improve from 108.69668\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 111.5296 - val_loss: 111.0095 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 99.2343 \n",
      "Epoch 5: val_loss did not improve from 108.69668\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 98.0949 - val_loss: 146.9009 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 89.6226\n",
      "Epoch 6: val_loss improved from 108.69668 to 98.94976, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 89.6163 - val_loss: 98.9498 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 95.9208\n",
      "Epoch 7: val_loss improved from 98.94976 to 98.88147, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 95.7876 - val_loss: 98.8815 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 86.6660\n",
      "Epoch 8: val_loss did not improve from 98.88147\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 87.6056 - val_loss: 102.7215 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 78.9089\n",
      "Epoch 9: val_loss did not improve from 98.88147\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.1952 - val_loss: 108.7500 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 72.7343\n",
      "Epoch 10: val_loss did not improve from 98.88147\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 73.2944 - val_loss: 276.6642 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 76.7448\n",
      "Epoch 11: val_loss improved from 98.88147 to 90.16315, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 76.3862 - val_loss: 90.1632 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 63.6431\n",
      "Epoch 12: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.2463 - val_loss: 95.7179 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 67.2523\n",
      "Epoch 13: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 67.6327 - val_loss: 100.4995 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 72.5332\n",
      "Epoch 14: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 71.1314 - val_loss: 109.3065 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 59.7023\n",
      "Epoch 15: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 59.6656 - val_loss: 104.0829 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 53.8638\n",
      "Epoch 16: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.7944 - val_loss: 93.7046 - lr: 0.0100\n",
      "Epoch 17/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 43.8814\n",
      "Epoch 17: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 43.6371 - val_loss: 93.1866 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 44.3194\n",
      "Epoch 18: val_loss did not improve from 90.16315\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.1614 - val_loss: 103.6509 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 37.8168\n",
      "Epoch 19: val_loss improved from 90.16315 to 89.63105, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.8168 - val_loss: 89.6311 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 34.9052\n",
      "Epoch 20: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.4279 - val_loss: 94.5014 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 32.5664\n",
      "Epoch 21: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.0785 - val_loss: 99.0301 - lr: 0.0070\n",
      "Epoch 22/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 37.4661\n",
      "Epoch 22: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.0850 - val_loss: 118.2412 - lr: 0.0070\n",
      "Epoch 23/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 36.6812\n",
      "Epoch 23: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.6642 - val_loss: 95.9972 - lr: 0.0070\n",
      "Epoch 24/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 33.5068\n",
      "Epoch 24: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.2957 - val_loss: 115.5407 - lr: 0.0070\n",
      "Epoch 25/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 27.1815\n",
      "Epoch 25: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.0739 - val_loss: 101.0643 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 25.1004\n",
      "Epoch 26: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.8682 - val_loss: 92.4430 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 24.7172\n",
      "Epoch 27: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.6716 - val_loss: 102.3875 - lr: 0.0049\n",
      "Epoch 28/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 23.0136\n",
      "Epoch 28: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.9010 - val_loss: 100.0354 - lr: 0.0049\n",
      "Epoch 29/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 23.4455\n",
      "Epoch 29: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.7282 - val_loss: 129.2861 - lr: 0.0049\n",
      "Epoch 30/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 21.3173\n",
      "Epoch 30: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.2022 - val_loss: 93.7699 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.1915\n",
      "Epoch 31: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1083 - val_loss: 95.2198 - lr: 0.0034\n",
      "Epoch 32/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.9976\n",
      "Epoch 32: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9107 - val_loss: 92.4082 - lr: 0.0034\n",
      "Epoch 33/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 18.8077\n",
      "Epoch 33: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.8733 - val_loss: 91.6985 - lr: 0.0034\n",
      "Epoch 34/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.5843\n",
      "Epoch 34: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7117 - val_loss: 92.5946 - lr: 0.0034\n",
      "Epoch 35/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.8073\n",
      "Epoch 35: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.8020 - val_loss: 101.2823 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 15.8446\n",
      "Epoch 36: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8291 - val_loss: 95.7827 - lr: 0.0024\n",
      "Epoch 37/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 16.4523\n",
      "Epoch 37: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1168 - val_loss: 95.4399 - lr: 0.0024\n",
      "Epoch 38/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 15.8967\n",
      "Epoch 38: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8517 - val_loss: 102.5837 - lr: 0.0024\n",
      "Epoch 39/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 15.6008\n",
      "Epoch 39: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5565 - val_loss: 98.8173 - lr: 0.0024\n",
      "Epoch 40/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 14.2965\n",
      "Epoch 40: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0232 - val_loss: 94.8610 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 13.1667\n",
      "Epoch 41: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2664 - val_loss: 96.8405 - lr: 0.0017\n",
      "Epoch 42/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 13.1961\n",
      "Epoch 42: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.4626 - val_loss: 98.8882 - lr: 0.0017\n",
      "Epoch 43/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.7194\n",
      "Epoch 43: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7193 - val_loss: 96.5497 - lr: 0.0017\n",
      "Epoch 44/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 13.5664\n",
      "Epoch 44: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5579 - val_loss: 95.5655 - lr: 0.0017\n",
      "Epoch 45/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 12.4520\n",
      "Epoch 45: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4244 - val_loss: 93.9541 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 12.3431\n",
      "Epoch 46: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3335 - val_loss: 98.9102 - lr: 0.0012\n",
      "Epoch 47/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 12.7165\n",
      "Epoch 47: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5899 - val_loss: 92.2229 - lr: 0.0012\n",
      "Epoch 48/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 13.0918\n",
      "Epoch 48: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0400 - val_loss: 93.1575 - lr: 0.0012\n",
      "Epoch 49/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.9374\n",
      "Epoch 49: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0340 - val_loss: 93.6474 - lr: 0.0012\n",
      "Epoch 50/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.3586\n",
      "Epoch 50: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2989 - val_loss: 96.8697 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.5850\n",
      "Epoch 51: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5044 - val_loss: 93.7848 - lr: 8.2354e-04\n",
      "Epoch 52/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.5847\n",
      "Epoch 52: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5452 - val_loss: 95.6668 - lr: 8.2354e-04\n",
      "Epoch 53/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.2475\n",
      "Epoch 53: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3277 - val_loss: 95.3397 - lr: 8.2354e-04\n",
      "Epoch 54/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.3912\n",
      "Epoch 54: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3983 - val_loss: 90.7998 - lr: 8.2354e-04\n",
      "Epoch 55/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.6116\n",
      "Epoch 55: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7588 - val_loss: 97.8444 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 11.2271\n",
      "Epoch 56: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.1684 - val_loss: 100.7019 - lr: 5.7648e-04\n",
      "Epoch 57/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.3815\n",
      "Epoch 57: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3808 - val_loss: 92.6483 - lr: 5.7648e-04\n",
      "Epoch 58/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.9112\n",
      "Epoch 58: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8591 - val_loss: 99.2631 - lr: 5.7648e-04\n",
      "Epoch 59/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.1198\n",
      "Epoch 59: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0806 - val_loss: 94.7970 - lr: 5.7648e-04\n",
      "Epoch 60/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 10.1770\n",
      "Epoch 60: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2419 - val_loss: 95.3899 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.9866\n",
      "Epoch 61: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0187 - val_loss: 94.3632 - lr: 4.0354e-04\n",
      "Epoch 62/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.0188\n",
      "Epoch 62: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1468 - val_loss: 98.5183 - lr: 4.0354e-04\n",
      "Epoch 63/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 9.8894\n",
      "Epoch 63: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.8579 - val_loss: 97.3746 - lr: 4.0354e-04\n",
      "Epoch 64/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.7236\n",
      "Epoch 64: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7668 - val_loss: 98.7239 - lr: 4.0354e-04\n",
      "Epoch 65/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 10.0509\n",
      "Epoch 65: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0020 - val_loss: 96.4002 - lr: 2.8248e-04\n",
      "Epoch 66/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.3949\n",
      "Epoch 66: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4146 - val_loss: 94.2457 - lr: 2.8248e-04\n",
      "Epoch 67/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.6795\n",
      "Epoch 67: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7565 - val_loss: 93.7402 - lr: 2.8248e-04\n",
      "Epoch 68/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 10.1968\n",
      "Epoch 68: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0970 - val_loss: 95.3666 - lr: 2.8248e-04\n",
      "Epoch 69/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 9.9769\n",
      "Epoch 69: val_loss did not improve from 89.63105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.0196 - val_loss: 100.6540 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 536.2537\n",
      "Epoch 1: val_loss improved from inf to 279.19513, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 508.8789 - val_loss: 279.1951 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 172.6346\n",
      "Epoch 2: val_loss improved from 279.19513 to 131.73889, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 170.9662 - val_loss: 131.7389 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 119.8533\n",
      "Epoch 3: val_loss improved from 131.73889 to 95.20412, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 120.7166 - val_loss: 95.2041 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 104.8877\n",
      "Epoch 4: val_loss did not improve from 95.20412\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 105.8777 - val_loss: 110.8041 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 98.0289\n",
      "Epoch 5: val_loss did not improve from 95.20412\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 99.0104 - val_loss: 107.4103 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 94.3090\n",
      "Epoch 6: val_loss improved from 95.20412 to 94.44784, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 94.0028 - val_loss: 94.4478 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 104.6373\n",
      "Epoch 7: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 104.4422 - val_loss: 107.4890 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 85.7780\n",
      "Epoch 8: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 85.8155 - val_loss: 133.4989 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 75.0421\n",
      "Epoch 9: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 74.9855 - val_loss: 118.3135 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 70.3103\n",
      "Epoch 10: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 71.1252 - val_loss: 215.6256 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 81.5565\n",
      "Epoch 11: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 81.0910 - val_loss: 94.7658 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 61.3727\n",
      "Epoch 12: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.4048 - val_loss: 104.0523 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 57.9149\n",
      "Epoch 13: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 58.1262 - val_loss: 110.8167 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 55.0898\n",
      "Epoch 14: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.1778 - val_loss: 101.0896 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 49.3268\n",
      "Epoch 15: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.5376 - val_loss: 125.3114 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 52.6655\n",
      "Epoch 16: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 52.6286 - val_loss: 98.7321 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 37.1980\n",
      "Epoch 17: val_loss did not improve from 94.44784\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.1812 - val_loss: 98.5475 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 36.4405\n",
      "Epoch 18: val_loss improved from 94.44784 to 93.66675, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.7902 - val_loss: 93.6667 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 34.5420\n",
      "Epoch 19: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.4907 - val_loss: 99.0071 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 30.9802\n",
      "Epoch 20: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.9561 - val_loss: 107.6374 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 34.4716\n",
      "Epoch 21: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.1426 - val_loss: 99.5014 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 29.7316\n",
      "Epoch 22: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.8737 - val_loss: 104.1091 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 34.5479\n",
      "Epoch 23: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.5741 - val_loss: 118.1238 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 28.3699\n",
      "Epoch 24: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.0350 - val_loss: 103.3755 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 24.1187\n",
      "Epoch 25: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0070 - val_loss: 99.2252 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 23.8660\n",
      "Epoch 26: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.5482 - val_loss: 96.9427 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 22.2628\n",
      "Epoch 27: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.1930 - val_loss: 104.0655 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 23.7584\n",
      "Epoch 28: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.6575 - val_loss: 98.9688 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 20.7624\n",
      "Epoch 29: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.8057 - val_loss: 101.6193 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 20.2281\n",
      "Epoch 30: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.2145 - val_loss: 113.8483 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 18.3063\n",
      "Epoch 31: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.6504 - val_loss: 94.6589 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 18.9661\n",
      "Epoch 32: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9661 - val_loss: 98.4149 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 17.7423\n",
      "Epoch 33: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.8112 - val_loss: 96.1078 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 17.0110\n",
      "Epoch 34: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.2271 - val_loss: 97.9749 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.9218\n",
      "Epoch 35: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8913 - val_loss: 93.7084 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 15.5474\n",
      "Epoch 36: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6171 - val_loss: 103.8805 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 15.5251\n",
      "Epoch 37: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5054 - val_loss: 96.8737 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 14.6755\n",
      "Epoch 38: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.7299 - val_loss: 98.5406 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 14.7203\n",
      "Epoch 39: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.6923 - val_loss: 97.9247 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.2950\n",
      "Epoch 40: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.3160 - val_loss: 104.5703 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 14.6812\n",
      "Epoch 41: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.6198 - val_loss: 98.4260 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.2882\n",
      "Epoch 42: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.1210 - val_loss: 100.5652 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 13.3144\n",
      "Epoch 43: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5287 - val_loss: 95.3291 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 13.5523\n",
      "Epoch 44: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5459 - val_loss: 99.2802 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.9325\n",
      "Epoch 45: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9727 - val_loss: 100.0210 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.4148\n",
      "Epoch 46: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3313 - val_loss: 99.6731 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 12.8594\n",
      "Epoch 47: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7980 - val_loss: 101.2954 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.4563\n",
      "Epoch 48: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4456 - val_loss: 98.7422 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.9117\n",
      "Epoch 49: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9442 - val_loss: 103.0592 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.0945\n",
      "Epoch 50: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1207 - val_loss: 97.1591 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.7137\n",
      "Epoch 51: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8818 - val_loss: 97.3765 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.9596\n",
      "Epoch 52: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9953 - val_loss: 100.1778 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.0588\n",
      "Epoch 53: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.9932 - val_loss: 100.2826 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 11.5578\n",
      "Epoch 54: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5578 - val_loss: 100.6008 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.3145\n",
      "Epoch 55: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.0930 - val_loss: 100.2357 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.2179\n",
      "Epoch 56: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1873 - val_loss: 97.9419 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.1399\n",
      "Epoch 57: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.1804 - val_loss: 97.3634 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.2469\n",
      "Epoch 58: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.2052 - val_loss: 98.2607 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.9223\n",
      "Epoch 59: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8496 - val_loss: 97.0191 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 10.8263\n",
      "Epoch 60: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8705 - val_loss: 98.7736 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.5200\n",
      "Epoch 61: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6116 - val_loss: 103.7615 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.6196\n",
      "Epoch 62: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5809 - val_loss: 98.8422 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.9579\n",
      "Epoch 63: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.9579 - val_loss: 100.1121 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 10.5351\n",
      "Epoch 64: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.6780 - val_loss: 97.9345 - lr: 1.9773e-04\n",
      "Epoch 65/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.9572\n",
      "Epoch 65: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.0936 - val_loss: 97.7357 - lr: 1.9773e-04\n",
      "Epoch 66/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 10.2583\n",
      "Epoch 66: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.3867 - val_loss: 102.1614 - lr: 1.9773e-04\n",
      "Epoch 67/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 10.4990\n",
      "Epoch 67: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.5930 - val_loss: 98.9615 - lr: 1.9773e-04\n",
      "Epoch 68/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.9401\n",
      "Epoch 68: val_loss did not improve from 93.66675\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9416 - val_loss: 97.6325 - lr: 1.9773e-04\n",
      "20/20 [==============================] - 0s 999us/step\n",
      "Epoch 1/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 379.8478\n",
      "Epoch 1: val_loss improved from inf to 180.37192, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 356.5278 - val_loss: 180.3719 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 139.1745\n",
      "Epoch 2: val_loss improved from 180.37192 to 109.13519, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 139.3050 - val_loss: 109.1352 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 115.2922\n",
      "Epoch 3: val_loss improved from 109.13519 to 93.42706, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 114.9379 - val_loss: 93.4271 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 105.1735\n",
      "Epoch 4: val_loss did not improve from 93.42706\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 104.9104 - val_loss: 101.0137 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 98.1653\n",
      "Epoch 5: val_loss improved from 93.42706 to 84.56731, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 98.2089 - val_loss: 84.5673 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 90.3699\n",
      "Epoch 6: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 89.4855 - val_loss: 89.8487 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 82.2138\n",
      "Epoch 7: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 81.7082 - val_loss: 112.3620 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 79.5628\n",
      "Epoch 8: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.1422 - val_loss: 91.0147 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 87.7231\n",
      "Epoch 9: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 87.8832 - val_loss: 127.4759 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 63.7117\n",
      "Epoch 10: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.9776 - val_loss: 121.4040 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 57.0722\n",
      "Epoch 11: val_loss did not improve from 84.56731\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.4029 - val_loss: 89.1637 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 51.1497\n",
      "Epoch 12: val_loss improved from 84.56731 to 84.15096, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 51.1739 - val_loss: 84.1510 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 44.9182\n",
      "Epoch 13: val_loss did not improve from 84.15096\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.7559 - val_loss: 107.4373 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 42.4941\n",
      "Epoch 14: val_loss did not improve from 84.15096\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.5526 - val_loss: 136.7789 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 41.9601\n",
      "Epoch 15: val_loss did not improve from 84.15096\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.8924 - val_loss: 115.9705 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 39.0067\n",
      "Epoch 16: val_loss did not improve from 84.15096\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.7098 - val_loss: 90.9414 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 38.9780\n",
      "Epoch 17: val_loss did not improve from 84.15096\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.6798 - val_loss: 84.5302 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 32.1054\n",
      "Epoch 18: val_loss improved from 84.15096 to 83.20885, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.3033 - val_loss: 83.2089 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 28.0434\n",
      "Epoch 19: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.3634 - val_loss: 93.0541 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 25.0144\n",
      "Epoch 20: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.0553 - val_loss: 111.4989 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 22.8500\n",
      "Epoch 21: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.0601 - val_loss: 84.9272 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 23.6488\n",
      "Epoch 22: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.6831 - val_loss: 83.5794 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 23.9346\n",
      "Epoch 23: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.6628 - val_loss: 89.1177 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 20.3127\n",
      "Epoch 24: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.3647 - val_loss: 103.3479 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 18.5164\n",
      "Epoch 25: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.4741 - val_loss: 90.5768 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.3466\n",
      "Epoch 26: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4797 - val_loss: 104.4710 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 18.9511\n",
      "Epoch 27: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.7473 - val_loss: 85.9016 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 17.3659\n",
      "Epoch 28: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6222 - val_loss: 99.9039 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.3647\n",
      "Epoch 29: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4641 - val_loss: 89.5775 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.9690\n",
      "Epoch 30: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7930 - val_loss: 83.5712 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.0427\n",
      "Epoch 31: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0353 - val_loss: 96.0602 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.5635\n",
      "Epoch 32: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.6498 - val_loss: 83.6860 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.1726\n",
      "Epoch 33: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.3966 - val_loss: 89.1975 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.0234\n",
      "Epoch 34: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9338 - val_loss: 84.0691 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.6213\n",
      "Epoch 35: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.3721 - val_loss: 87.1415 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.0234\n",
      "Epoch 36: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.0540 - val_loss: 97.3805 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 12.4841\n",
      "Epoch 37: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.3992 - val_loss: 90.6875 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 11.9799\n",
      "Epoch 38: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1253 - val_loss: 88.8981 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.9252\n",
      "Epoch 39: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0275 - val_loss: 86.7327 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.1486\n",
      "Epoch 40: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.8947 - val_loss: 91.1046 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.4995\n",
      "Epoch 41: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.5445 - val_loss: 91.7826 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 10.9642\n",
      "Epoch 42: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8976 - val_loss: 89.4696 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 11.0296\n",
      "Epoch 43: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8729 - val_loss: 89.0051 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 10.7460\n",
      "Epoch 44: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.8015 - val_loss: 88.9409 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.9664 \n",
      "Epoch 45: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.9417 - val_loss: 92.2800 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.3296\n",
      "Epoch 46: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.2520 - val_loss: 84.2214 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 10.0852\n",
      "Epoch 47: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.1076 - val_loss: 94.7818 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.3345\n",
      "Epoch 48: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 10.2994 - val_loss: 89.3267 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.4039\n",
      "Epoch 49: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.4199 - val_loss: 92.3111 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.5810\n",
      "Epoch 50: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7591 - val_loss: 91.8331 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.7851\n",
      "Epoch 51: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8319 - val_loss: 93.0193 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 9.2709\n",
      "Epoch 52: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2050 - val_loss: 93.9023 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "71/91 [======================>.......] - ETA: 0s - loss: 9.6549\n",
      "Epoch 53: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.7402 - val_loss: 92.8620 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9.1155\n",
      "Epoch 54: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.0261 - val_loss: 92.9972 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 9.2188\n",
      "Epoch 55: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2064 - val_loss: 95.9440 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.3020\n",
      "Epoch 56: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 9.2742 - val_loss: 91.5050 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.8091\n",
      "Epoch 57: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.8372 - val_loss: 88.6033 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 8.6158\n",
      "Epoch 58: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.6380 - val_loss: 92.3645 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 8.8925\n",
      "Epoch 59: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.9205 - val_loss: 90.1010 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 8.1530\n",
      "Epoch 60: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1446 - val_loss: 96.0388 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 8.1724\n",
      "Epoch 61: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1046 - val_loss: 91.1426 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 8.1843\n",
      "Epoch 62: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.1296 - val_loss: 92.2322 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.6865\n",
      "Epoch 63: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.7028 - val_loss: 92.7284 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 8.1385\n",
      "Epoch 64: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.0700 - val_loss: 91.6603 - lr: 1.9773e-04\n",
      "Epoch 65/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.4275\n",
      "Epoch 65: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3853 - val_loss: 90.6388 - lr: 1.9773e-04\n",
      "Epoch 66/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.2619\n",
      "Epoch 66: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.2870 - val_loss: 91.9344 - lr: 1.9773e-04\n",
      "Epoch 67/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 8.3401\n",
      "Epoch 67: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3377 - val_loss: 93.3834 - lr: 1.9773e-04\n",
      "Epoch 68/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 8.2455\n",
      "Epoch 68: val_loss did not improve from 83.20885\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 8.3153 - val_loss: 90.6242 - lr: 1.9773e-04\n",
      "20/20 [==============================] - 0s 961us/step\n",
      "Mean MSE: 85.33657395131158\n",
      "Mean R-squared: 0.7696146953280352\n",
      "Average prop10: 0.7553376072542902\n",
      "Average prop5: 0.5124007897815913\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "'''\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predict the target values\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    print(\"Shapes - y_pred:\", y_pred.shape, \"y:\", y.shape)\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    error = y_pred - y\n",
    "    mse = np.mean(error ** 2)\n",
    "    r_squared = 1 - (np.sum(error ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    \n",
    "    return mse, r_squared, prop10, prop5\n",
    "'''\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# Perform cross-validation for MLP\n",
    "def mlp_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    x = Dense(units=256, activation='relu')(inputs)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=50,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.7,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mlp_horvath_mse_list = []\n",
    "mlp_horvath_r_squared_list = []\n",
    "mlp_horvath_prop10_list = []\n",
    "mlp_horvath_prop5_list = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create mlp model with dropout\n",
    "    model_mlp = mlp_model_with_dropout((X_train.shape[1]), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'best_model_fold_{fold_idx}.h5',  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_mlp.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks including ModelCheckpoint\n",
    "    model_mlp.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # After training, you can load the best model using the following:\n",
    "    best_model_mlp = load_model(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(best_model_mlp, X_test_scaled, y_test)\n",
    "\n",
    "    mlp_horvath_mse_list.append(mse)\n",
    "    mlp_horvath_r_squared_list.append(r_squared)\n",
    "    mlp_horvath_prop10_list.append(prop10)\n",
    "    mlp_horvath_prop5_list.append(prop5)\n",
    "\n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(mlp_horvath_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(mlp_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(mlp_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(mlp_horvath_prop5_list))\n",
    "\n",
    "CV_results2['mlp-horvath'] = {'mse': mlp_horvath_mse_list,\n",
    "                               'r_squared': mlp_horvath_r_squared_list,\n",
    "                               'prop10': mlp_horvath_prop10_list,\n",
    "                               'prop5': mlp_horvath_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 6782.5996\n",
      "Epoch 1: val_loss improved from inf to 168.61703, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 6127.9487 - val_loss: 168.6170 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 165.9261\n",
      "Epoch 2: val_loss improved from 168.61703 to 149.05870, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 164.0616 - val_loss: 149.0587 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 127.1387\n",
      "Epoch 3: val_loss improved from 149.05870 to 92.75224, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 125.0240 - val_loss: 92.7522 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 116.0194\n",
      "Epoch 4: val_loss improved from 92.75224 to 84.46534, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 115.3622 - val_loss: 84.4653 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 95.2760\n",
      "Epoch 5: val_loss did not improve from 84.46534\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 95.5169 - val_loss: 156.9343 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 102.7783\n",
      "Epoch 6: val_loss improved from 84.46534 to 80.70976, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 98.8047 - val_loss: 80.7098 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 79.5602\n",
      "Epoch 7: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 80.1223 - val_loss: 88.2382 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 67.9667\n",
      "Epoch 8: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 67.2560 - val_loss: 85.1829 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 65.9625\n",
      "Epoch 9: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.4028 - val_loss: 104.3026 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 65.4977\n",
      "Epoch 10: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 64.8639 - val_loss: 255.5630 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 79.3996\n",
      "Epoch 11: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 77.0443 - val_loss: 122.4013 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 48.9084\n",
      "Epoch 12: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.8670 - val_loss: 93.3308 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 46.0751\n",
      "Epoch 13: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 47.3185 - val_loss: 87.0596 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 46.1727\n",
      "Epoch 14: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.1563 - val_loss: 85.0432 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 42.4225\n",
      "Epoch 15: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.1667 - val_loss: 85.4202 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 42.2701\n",
      "Epoch 16: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.4617 - val_loss: 121.0448 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 36.2578\n",
      "Epoch 17: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.2328 - val_loss: 100.6475 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 35.3275\n",
      "Epoch 18: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.3237 - val_loss: 89.3979 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 31.2383\n",
      "Epoch 19: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.3193 - val_loss: 88.9515 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 30.9591\n",
      "Epoch 20: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.5729 - val_loss: 102.5690 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 33.4080\n",
      "Epoch 21: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.4080 - val_loss: 96.0733 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 28.0169\n",
      "Epoch 22: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.2577 - val_loss: 107.5535 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 27.3092\n",
      "Epoch 23: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.1287 - val_loss: 112.3868 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 24.5647\n",
      "Epoch 24: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.5702 - val_loss: 101.4609 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 25.7746\n",
      "Epoch 25: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.8086 - val_loss: 134.1325 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 25.3039\n",
      "Epoch 26: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.8934 - val_loss: 100.5804 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 26.3312\n",
      "Epoch 27: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.3312 - val_loss: 113.2223 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 23.5547\n",
      "Epoch 28: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.4118 - val_loss: 126.6054 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 23.4430\n",
      "Epoch 29: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.6237 - val_loss: 85.7506 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 26.6260\n",
      "Epoch 30: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.5885 - val_loss: 93.8833 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 22.9169\n",
      "Epoch 31: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 22.8940 - val_loss: 92.5570 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 20.9693\n",
      "Epoch 32: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.7895 - val_loss: 108.2361 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 21.2876\n",
      "Epoch 33: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.9882 - val_loss: 126.4897 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 22.1322\n",
      "Epoch 34: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7603 - val_loss: 111.0856 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 20.7298\n",
      "Epoch 35: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.8180 - val_loss: 116.3749 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 20.0414\n",
      "Epoch 36: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.1192 - val_loss: 100.1980 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 19.9721\n",
      "Epoch 37: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.9607 - val_loss: 108.9951 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.2797\n",
      "Epoch 38: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.2576 - val_loss: 96.4797 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.4868\n",
      "Epoch 39: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.5714 - val_loss: 109.9333 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 20.5486\n",
      "Epoch 40: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.1488 - val_loss: 97.3273 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 18.2568\n",
      "Epoch 41: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2693 - val_loss: 106.2355 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.8485\n",
      "Epoch 42: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.7534 - val_loss: 116.3188 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 18.9106\n",
      "Epoch 43: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1560 - val_loss: 108.1779 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.9638\n",
      "Epoch 44: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7624 - val_loss: 111.8628 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 18.3000\n",
      "Epoch 45: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.2342 - val_loss: 100.4395 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.0881\n",
      "Epoch 46: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.1546 - val_loss: 100.4026 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 18.7026\n",
      "Epoch 47: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.3486 - val_loss: 111.4446 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 17.3240\n",
      "Epoch 48: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.3351 - val_loss: 108.8749 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.0890\n",
      "Epoch 49: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.1443 - val_loss: 103.4934 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 16.8228\n",
      "Epoch 50: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6435 - val_loss: 105.3004 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 17.5110\n",
      "Epoch 51: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.5110 - val_loss: 106.0845 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 16.5771\n",
      "Epoch 52: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4039 - val_loss: 108.2273 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.7223\n",
      "Epoch 53: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6870 - val_loss: 110.8971 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 16.5794\n",
      "Epoch 54: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5794 - val_loss: 103.3541 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 16.3912\n",
      "Epoch 55: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5329 - val_loss: 109.5626 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.3500\n",
      "Epoch 56: val_loss did not improve from 80.70976\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.1389 - val_loss: 104.1368 - lr: 4.0354e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 11408.6621\n",
      "Epoch 1: val_loss improved from inf to 176.16008, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 9159.6777 - val_loss: 176.1601 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 177.7231\n",
      "Epoch 2: val_loss improved from 176.16008 to 126.31921, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 177.6532 - val_loss: 126.3192 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 128.1648\n",
      "Epoch 3: val_loss improved from 126.31921 to 108.40794, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 127.9557 - val_loss: 108.4079 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 118.4699\n",
      "Epoch 4: val_loss did not improve from 108.40794\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 114.3991 - val_loss: 125.5379 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 96.6451\n",
      "Epoch 5: val_loss did not improve from 108.40794\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 96.0623 - val_loss: 110.3764 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 88.1183\n",
      "Epoch 6: val_loss improved from 108.40794 to 72.93301, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 89.7102 - val_loss: 72.9330 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 84.4946\n",
      "Epoch 7: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 85.9053 - val_loss: 88.2025 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 69.1972\n",
      "Epoch 8: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 69.4459 - val_loss: 143.3022 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 82.7237\n",
      "Epoch 9: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 79.8197 - val_loss: 119.7585 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 63.1125\n",
      "Epoch 10: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 62.3287 - val_loss: 129.7046 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 56.5203\n",
      "Epoch 11: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.1475 - val_loss: 148.3829 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 44.4271\n",
      "Epoch 12: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 44.2706 - val_loss: 86.8083 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 39.6447\n",
      "Epoch 13: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 40.1605 - val_loss: 103.2586 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 43.2902\n",
      "Epoch 14: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.3561 - val_loss: 110.3797 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 39.1652\n",
      "Epoch 15: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.2439 - val_loss: 100.9135 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 39.3802\n",
      "Epoch 16: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.3595 - val_loss: 119.3103 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 33.0658\n",
      "Epoch 17: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.1814 - val_loss: 88.8955 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 29.9337\n",
      "Epoch 18: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.6606 - val_loss: 99.1375 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 28.5665\n",
      "Epoch 19: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.4643 - val_loss: 103.3866 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 25.6579\n",
      "Epoch 20: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.0696 - val_loss: 103.8533 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 27.2739\n",
      "Epoch 21: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.2739 - val_loss: 99.5867 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 24.0994\n",
      "Epoch 22: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0994 - val_loss: 92.2989 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 22.4123\n",
      "Epoch 23: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.3995 - val_loss: 108.5538 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 22.1828\n",
      "Epoch 24: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.2981 - val_loss: 86.9709 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 22.7920\n",
      "Epoch 25: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.8561 - val_loss: 94.7372 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 22.2737\n",
      "Epoch 26: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.1142 - val_loss: 88.8197 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 19.9141\n",
      "Epoch 27: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6053 - val_loss: 98.1003 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 18.1998\n",
      "Epoch 28: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.9984 - val_loss: 142.2159 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 20.1802\n",
      "Epoch 29: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.1905 - val_loss: 108.6001 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.8156\n",
      "Epoch 30: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.4947 - val_loss: 98.5011 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 18.3488\n",
      "Epoch 31: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.3659 - val_loss: 95.7150 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 17.6574\n",
      "Epoch 32: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6574 - val_loss: 117.9276 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 16.8822\n",
      "Epoch 33: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.8813 - val_loss: 106.8673 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.8850\n",
      "Epoch 34: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.8823 - val_loss: 91.1136 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.4707\n",
      "Epoch 35: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3438 - val_loss: 114.2026 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 15.2385\n",
      "Epoch 36: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.2944 - val_loss: 109.0687 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 14.8132\n",
      "Epoch 37: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8327 - val_loss: 101.5814 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.2595\n",
      "Epoch 38: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.2597 - val_loss: 105.2262 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 14.3694\n",
      "Epoch 39: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.5060 - val_loss: 103.2258 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.4531\n",
      "Epoch 40: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.4524 - val_loss: 106.3080 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.4624\n",
      "Epoch 41: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.3713 - val_loss: 99.0756 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 13.6615\n",
      "Epoch 42: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6615 - val_loss: 109.4838 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.3997\n",
      "Epoch 43: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.3531 - val_loss: 105.7637 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.9232\n",
      "Epoch 44: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.9578 - val_loss: 99.8575 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 13.0561\n",
      "Epoch 45: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.9580 - val_loss: 101.4424 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 13.4627\n",
      "Epoch 46: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.4627 - val_loss: 107.2968 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 13.1671\n",
      "Epoch 47: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2617 - val_loss: 99.9830 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 13.1100\n",
      "Epoch 48: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.1812 - val_loss: 101.9477 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.7147\n",
      "Epoch 49: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.7156 - val_loss: 106.6850 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 13.1504\n",
      "Epoch 50: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.1511 - val_loss: 99.6695 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 11.9526\n",
      "Epoch 51: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.2589 - val_loss: 107.2992 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 12.2608\n",
      "Epoch 52: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.1216 - val_loss: 103.9090 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.3783\n",
      "Epoch 53: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.2859 - val_loss: 108.9080 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 12.4581\n",
      "Epoch 54: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.4394 - val_loss: 106.7858 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.9548\n",
      "Epoch 55: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.0552 - val_loss: 105.6180 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 11.8139\n",
      "Epoch 56: val_loss did not improve from 72.93301\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.7755 - val_loss: 107.5554 - lr: 4.0354e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 5450.3857\n",
      "Epoch 1: val_loss improved from inf to 322.19470, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 5450.3857 - val_loss: 322.1947 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 192.6818\n",
      "Epoch 2: val_loss improved from 322.19470 to 137.50157, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 191.2211 - val_loss: 137.5016 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 123.0037\n",
      "Epoch 3: val_loss improved from 137.50157 to 127.72237, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 122.7277 - val_loss: 127.7224 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 148.1048\n",
      "Epoch 4: val_loss did not improve from 127.72237\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 146.2279 - val_loss: 188.5425 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 107.1504\n",
      "Epoch 5: val_loss improved from 127.72237 to 97.10654, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 106.5353 - val_loss: 97.1065 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 88.5741\n",
      "Epoch 6: val_loss did not improve from 97.10654\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 88.5689 - val_loss: 133.8973 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 84.4503\n",
      "Epoch 7: val_loss improved from 97.10654 to 87.71825, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 84.9041 - val_loss: 87.7183 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 151.2115\n",
      "Epoch 8: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 151.5664 - val_loss: 101.0658 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 103.0628\n",
      "Epoch 9: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.0428 - val_loss: 92.5776 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 84.7538\n",
      "Epoch 10: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 82.8667 - val_loss: 102.8820 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 80.2460\n",
      "Epoch 11: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 79.6205 - val_loss: 130.0550 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 72.6162\n",
      "Epoch 12: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 72.5426 - val_loss: 127.1547 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 65.1189\n",
      "Epoch 13: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 64.9255 - val_loss: 120.5555 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 59.4756\n",
      "Epoch 14: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 58.4580 - val_loss: 88.5965 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 62.0865\n",
      "Epoch 15: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 61.5990 - val_loss: 91.9309 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 51.5407\n",
      "Epoch 16: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.7971 - val_loss: 107.6546 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 52.7464\n",
      "Epoch 17: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.2497 - val_loss: 103.7608 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 58.4588\n",
      "Epoch 18: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.6737 - val_loss: 135.6809 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 44.8812\n",
      "Epoch 19: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.3881 - val_loss: 95.3020 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 42.6564\n",
      "Epoch 20: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.5462 - val_loss: 94.7888 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 41.7223\n",
      "Epoch 21: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 41.7402 - val_loss: 93.8086 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 41.5258\n",
      "Epoch 22: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.9693 - val_loss: 98.6722 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 37.8798\n",
      "Epoch 23: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.9934 - val_loss: 100.3331 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 35.8364\n",
      "Epoch 24: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.5841 - val_loss: 94.6637 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 35.1979\n",
      "Epoch 25: val_loss did not improve from 87.71825\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 34.8153 - val_loss: 98.0639 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 37.7429\n",
      "Epoch 26: val_loss improved from 87.71825 to 87.39552, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.4817 - val_loss: 87.3955 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 34.3229\n",
      "Epoch 27: val_loss did not improve from 87.39552\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.6061 - val_loss: 90.8271 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 33.0505\n",
      "Epoch 28: val_loss did not improve from 87.39552\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.5203 - val_loss: 95.6358 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 32.5299\n",
      "Epoch 29: val_loss did not improve from 87.39552\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.5587 - val_loss: 90.8489 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 36.2449\n",
      "Epoch 30: val_loss did not improve from 87.39552\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 36.3094 - val_loss: 97.6689 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 35.0429\n",
      "Epoch 31: val_loss did not improve from 87.39552\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.0429 - val_loss: 111.8553 - lr: 0.0034\n",
      "Epoch 32/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 31.5595\n",
      "Epoch 32: val_loss improved from 87.39552 to 85.65881, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 31.5854 - val_loss: 85.6588 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 35.5565\n",
      "Epoch 33: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 34.5827 - val_loss: 89.0203 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 27.5944\n",
      "Epoch 34: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.8596 - val_loss: 103.4192 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 34.8718\n",
      "Epoch 35: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.5958 - val_loss: 92.2817 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 30.4128\n",
      "Epoch 36: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.4772 - val_loss: 88.2293 - lr: 0.0024\n",
      "Epoch 37/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 30.3805\n",
      "Epoch 37: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.3825 - val_loss: 95.3192 - lr: 0.0024\n",
      "Epoch 38/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 30.0688\n",
      "Epoch 38: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.9628 - val_loss: 88.3790 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 30.9297\n",
      "Epoch 39: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.9189 - val_loss: 89.4396 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 26.4081\n",
      "Epoch 40: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.8995 - val_loss: 96.2612 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 27.2595\n",
      "Epoch 41: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 28.2430 - val_loss: 87.2007 - lr: 0.0017\n",
      "Epoch 42/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 31.3291\n",
      "Epoch 42: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.3413 - val_loss: 93.0997 - lr: 0.0017\n",
      "Epoch 43/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 25.9181\n",
      "Epoch 43: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.1021 - val_loss: 92.1137 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 29.5250\n",
      "Epoch 44: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.1292 - val_loss: 91.3935 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 28.0145\n",
      "Epoch 45: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.8489 - val_loss: 93.5707 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 28.4204\n",
      "Epoch 46: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 28.0037 - val_loss: 96.3721 - lr: 0.0012\n",
      "Epoch 47/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 23.6030\n",
      "Epoch 47: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.5209 - val_loss: 100.3849 - lr: 0.0012\n",
      "Epoch 48/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 23.2583\n",
      "Epoch 48: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.1155 - val_loss: 95.7163 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 22.2913\n",
      "Epoch 49: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 22.2913 - val_loss: 97.4109 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 21.2773\n",
      "Epoch 50: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.2798 - val_loss: 91.7564 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 21.7438\n",
      "Epoch 51: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7925 - val_loss: 87.5929 - lr: 8.2354e-04\n",
      "Epoch 52/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 24.6929\n",
      "Epoch 52: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.5786 - val_loss: 98.4011 - lr: 8.2354e-04\n",
      "Epoch 53/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 23.3030\n",
      "Epoch 53: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.2123 - val_loss: 94.0375 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 21.5760\n",
      "Epoch 54: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7070 - val_loss: 91.9161 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 24.1990\n",
      "Epoch 55: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.0206 - val_loss: 96.3273 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 20.9048\n",
      "Epoch 56: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9487 - val_loss: 98.4558 - lr: 5.7648e-04\n",
      "Epoch 57/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 21.4503\n",
      "Epoch 57: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.5777 - val_loss: 92.9112 - lr: 5.7648e-04\n",
      "Epoch 58/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 22.4424\n",
      "Epoch 58: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.4534 - val_loss: 91.1395 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 21.8306\n",
      "Epoch 59: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.8410 - val_loss: 100.5729 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 21.7828\n",
      "Epoch 60: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7828 - val_loss: 91.2931 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 20.8483\n",
      "Epoch 61: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.7468 - val_loss: 93.4812 - lr: 4.0354e-04\n",
      "Epoch 62/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 19.8033\n",
      "Epoch 62: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8419 - val_loss: 93.1702 - lr: 4.0354e-04\n",
      "Epoch 63/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 20.6307\n",
      "Epoch 63: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 20.6229 - val_loss: 91.7948 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 21.5545\n",
      "Epoch 64: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.4832 - val_loss: 92.8261 - lr: 2.8248e-04\n",
      "Epoch 65/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 20.8699\n",
      "Epoch 65: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9049 - val_loss: 96.2938 - lr: 2.8248e-04\n",
      "Epoch 66/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 20.3151\n",
      "Epoch 66: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.4223 - val_loss: 96.2072 - lr: 2.8248e-04\n",
      "Epoch 67/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 20.3058\n",
      "Epoch 67: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.2127 - val_loss: 96.6713 - lr: 2.8248e-04\n",
      "Epoch 68/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 21.2976\n",
      "Epoch 68: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.3023 - val_loss: 93.0350 - lr: 1.9773e-04\n",
      "Epoch 69/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 21.2594\n",
      "Epoch 69: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.1990 - val_loss: 94.4110 - lr: 1.9773e-04\n",
      "Epoch 70/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 20.1776\n",
      "Epoch 70: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.1533 - val_loss: 95.4977 - lr: 1.9773e-04\n",
      "Epoch 71/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 20.6313\n",
      "Epoch 71: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.6676 - val_loss: 90.1770 - lr: 1.9773e-04\n",
      "Epoch 72/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 21.8728\n",
      "Epoch 72: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7659 - val_loss: 91.4610 - lr: 1.9773e-04\n",
      "Epoch 73/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 19.6091\n",
      "Epoch 73: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6280 - val_loss: 97.4668 - lr: 1.3841e-04\n",
      "Epoch 74/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 20.3067\n",
      "Epoch 74: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.3176 - val_loss: 96.0986 - lr: 1.3841e-04\n",
      "Epoch 75/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 19.8634\n",
      "Epoch 75: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.0112 - val_loss: 94.9295 - lr: 1.3841e-04\n",
      "Epoch 76/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 20.0080\n",
      "Epoch 76: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.0299 - val_loss: 93.5472 - lr: 1.3841e-04\n",
      "Epoch 77/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 20.0274\n",
      "Epoch 77: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.0274 - val_loss: 94.5564 - lr: 1.3841e-04\n",
      "Epoch 78/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.9781\n",
      "Epoch 78: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.8816 - val_loss: 94.8261 - lr: 9.6889e-05\n",
      "Epoch 79/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 19.4389\n",
      "Epoch 79: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.6468 - val_loss: 96.9680 - lr: 9.6889e-05\n",
      "Epoch 80/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.2567\n",
      "Epoch 80: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.2471 - val_loss: 95.3482 - lr: 9.6889e-05\n",
      "Epoch 81/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.5510\n",
      "Epoch 81: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.5890 - val_loss: 95.3426 - lr: 9.6889e-05\n",
      "Epoch 82/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 19.7686\n",
      "Epoch 82: val_loss did not improve from 85.65881\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.6732 - val_loss: 93.1123 - lr: 9.6889e-05\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 2991.5303\n",
      "Epoch 1: val_loss improved from inf to 230.32465, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 2774.0146 - val_loss: 230.3246 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 202.1817\n",
      "Epoch 2: val_loss improved from 230.32465 to 113.13967, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 193.8464 - val_loss: 113.1397 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 129.6763\n",
      "Epoch 3: val_loss did not improve from 113.13967\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 126.6083 - val_loss: 142.8490 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 145.2172\n",
      "Epoch 4: val_loss improved from 113.13967 to 81.27281, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 144.0665 - val_loss: 81.2728 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 124.5551\n",
      "Epoch 5: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 124.3392 - val_loss: 102.1287 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 100.7757\n",
      "Epoch 6: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.8955 - val_loss: 89.5997 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 90.0073\n",
      "Epoch 7: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 88.3576 - val_loss: 197.2030 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 85.6571\n",
      "Epoch 8: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 84.0864 - val_loss: 120.7824 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 70.8097\n",
      "Epoch 9: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 70.9742 - val_loss: 107.0197 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 63.8777\n",
      "Epoch 10: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 63.4266 - val_loss: 95.9022 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 57.3337\n",
      "Epoch 11: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 56.3427 - val_loss: 125.1967 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 54.3807\n",
      "Epoch 12: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 53.8556 - val_loss: 90.1652 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 46.2626\n",
      "Epoch 13: val_loss did not improve from 81.27281\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.3823 - val_loss: 88.0896 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 47.1898\n",
      "Epoch 14: val_loss improved from 81.27281 to 79.33677, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 47.0243 - val_loss: 79.3368 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 49.4290\n",
      "Epoch 15: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 49.5121 - val_loss: 120.5115 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 43.4760\n",
      "Epoch 16: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 43.4586 - val_loss: 93.8675 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 38.6323\n",
      "Epoch 17: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 38.6461 - val_loss: 92.4047 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 45.6776\n",
      "Epoch 18: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.3665 - val_loss: 80.7951 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 36.9327\n",
      "Epoch 19: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 36.7246 - val_loss: 87.8003 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 31.4546\n",
      "Epoch 20: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 31.1457 - val_loss: 80.5367 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 29.5789\n",
      "Epoch 21: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.6194 - val_loss: 87.0921 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 26.7737\n",
      "Epoch 22: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.1310 - val_loss: 121.6294 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 27.1114\n",
      "Epoch 23: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.9290 - val_loss: 94.0621 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 28.9705\n",
      "Epoch 24: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.8357 - val_loss: 88.3183 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 25.6779\n",
      "Epoch 25: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.5754 - val_loss: 81.0889 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 24.3978\n",
      "Epoch 26: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.3874 - val_loss: 98.0015 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 22.5318\n",
      "Epoch 27: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 22.7042 - val_loss: 85.8972 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 24.4725\n",
      "Epoch 28: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.4573 - val_loss: 114.0307 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 23.4364\n",
      "Epoch 29: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.1353 - val_loss: 99.6390 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.1968\n",
      "Epoch 30: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.3379 - val_loss: 88.8458 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 21.3135\n",
      "Epoch 31: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 20.9680 - val_loss: 95.9216 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 20.8484\n",
      "Epoch 32: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.8825 - val_loss: 93.7455 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.7242\n",
      "Epoch 33: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6791 - val_loss: 103.5745 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 19.7949\n",
      "Epoch 34: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.0090 - val_loss: 82.8673 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 19.1959\n",
      "Epoch 35: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1959 - val_loss: 92.5890 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.6619\n",
      "Epoch 36: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.7312 - val_loss: 94.9186 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.9063\n",
      "Epoch 37: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9134 - val_loss: 118.4705 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 19.9998\n",
      "Epoch 38: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.9998 - val_loss: 109.4550 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.7688\n",
      "Epoch 39: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.7025 - val_loss: 95.3320 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 16.9408\n",
      "Epoch 40: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.2698 - val_loss: 98.0290 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 17.5421\n",
      "Epoch 41: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.5019 - val_loss: 100.6254 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.9193\n",
      "Epoch 42: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9419 - val_loss: 86.4395 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.8292\n",
      "Epoch 43: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.8357 - val_loss: 95.4481 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 17.0419\n",
      "Epoch 44: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9954 - val_loss: 99.2717 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 16.5853\n",
      "Epoch 45: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.5950 - val_loss: 91.2632 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.5550\n",
      "Epoch 46: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.5834 - val_loss: 101.7189 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 16.6233\n",
      "Epoch 47: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6233 - val_loss: 97.2921 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 16.2689\n",
      "Epoch 48: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3652 - val_loss: 89.1183 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.9602\n",
      "Epoch 49: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.0363 - val_loss: 98.1907 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.8445\n",
      "Epoch 50: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8391 - val_loss: 89.9994 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 15.8420\n",
      "Epoch 51: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.7986 - val_loss: 94.0561 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.7535\n",
      "Epoch 52: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6761 - val_loss: 101.0920 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.7331\n",
      "Epoch 53: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7237 - val_loss: 91.6166 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 16.9103\n",
      "Epoch 54: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9103 - val_loss: 94.5014 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 16.3512\n",
      "Epoch 55: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1121 - val_loss: 92.3469 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.7157\n",
      "Epoch 56: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7157 - val_loss: 93.2831 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.5264\n",
      "Epoch 57: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.3709 - val_loss: 96.8782 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.2455\n",
      "Epoch 58: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.2822 - val_loss: 96.6880 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.9714\n",
      "Epoch 59: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8133 - val_loss: 94.0420 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.2812\n",
      "Epoch 60: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.2812 - val_loss: 92.3554 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 15.0741\n",
      "Epoch 61: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0273 - val_loss: 90.1121 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.2258\n",
      "Epoch 62: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.1601 - val_loss: 96.2232 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.4269\n",
      "Epoch 63: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.4633 - val_loss: 91.4533 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 15.1224\n",
      "Epoch 64: val_loss did not improve from 79.33677\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1235 - val_loss: 93.2112 - lr: 2.8248e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 9122.1211 \n",
      "Epoch 1: val_loss improved from inf to 130.72005, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 7926.1846 - val_loss: 130.7200 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 158.4383\n",
      "Epoch 2: val_loss improved from 130.72005 to 124.59649, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 158.3439 - val_loss: 124.5965 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 133.5656\n",
      "Epoch 3: val_loss improved from 124.59649 to 88.80688, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 134.7605 - val_loss: 88.8069 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 110.8768\n",
      "Epoch 4: val_loss improved from 88.80688 to 79.62677, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 110.8131 - val_loss: 79.6268 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 100.6487\n",
      "Epoch 5: val_loss improved from 79.62677 to 79.02592, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 100.6705 - val_loss: 79.0259 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 84.4178\n",
      "Epoch 6: val_loss did not improve from 79.02592\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 84.2780 - val_loss: 83.2125 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 73.2580\n",
      "Epoch 7: val_loss improved from 79.02592 to 77.96255, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 72.4654 - val_loss: 77.9625 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 68.0103\n",
      "Epoch 8: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 67.8962 - val_loss: 81.0017 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 64.2688\n",
      "Epoch 9: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 63.8786 - val_loss: 95.2608 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 61.7381\n",
      "Epoch 10: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 63.1278 - val_loss: 170.6700 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 73.1066\n",
      "Epoch 11: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 69.3699 - val_loss: 84.7726 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 53.6037\n",
      "Epoch 12: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 53.8880 - val_loss: 81.8846 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 46.4354\n",
      "Epoch 13: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.9194 - val_loss: 130.9905 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 48.4095\n",
      "Epoch 14: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 48.2234 - val_loss: 106.9831 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 37.4255\n",
      "Epoch 15: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.7834 - val_loss: 98.6385 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 38.3051\n",
      "Epoch 16: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.9575 - val_loss: 129.9082 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 32.7849\n",
      "Epoch 17: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.8531 - val_loss: 137.6906 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 32.0433\n",
      "Epoch 18: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.9523 - val_loss: 105.3136 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 31.6599\n",
      "Epoch 19: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 31.6883 - val_loss: 89.8269 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 28.8045\n",
      "Epoch 20: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.9333 - val_loss: 98.9558 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 33.0958\n",
      "Epoch 21: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 32.9177 - val_loss: 137.5170 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 28.7349\n",
      "Epoch 22: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.7349 - val_loss: 109.5741 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 25.6213\n",
      "Epoch 23: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 25.6478 - val_loss: 108.5349 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 23.9024\n",
      "Epoch 24: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.6337 - val_loss: 106.0439 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 23.0761\n",
      "Epoch 25: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.2380 - val_loss: 91.8848 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 26.4731\n",
      "Epoch 26: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.0919 - val_loss: 111.1732 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 23.7794\n",
      "Epoch 27: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.6597 - val_loss: 124.4553 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 22.5604\n",
      "Epoch 28: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 22.4278 - val_loss: 106.7052 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 21.8654\n",
      "Epoch 29: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.8654 - val_loss: 103.6482 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 22.1525\n",
      "Epoch 30: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.1525 - val_loss: 101.0031 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 21.9526\n",
      "Epoch 31: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.7854 - val_loss: 115.1193 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 21.0037\n",
      "Epoch 32: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.7582 - val_loss: 114.2460 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 20.1840\n",
      "Epoch 33: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.0376 - val_loss: 105.8600 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.2868\n",
      "Epoch 34: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.3274 - val_loss: 107.3496 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 18.9080\n",
      "Epoch 35: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9080 - val_loss: 98.2531 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 18.3179\n",
      "Epoch 36: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.3495 - val_loss: 106.7481 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 19.3032\n",
      "Epoch 37: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.3079 - val_loss: 101.9573 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 17.7117\n",
      "Epoch 38: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.7790 - val_loss: 111.0598 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.8765\n",
      "Epoch 39: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.9135 - val_loss: 118.8304 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 19.5975\n",
      "Epoch 40: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6564 - val_loss: 112.2197 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 17.3628\n",
      "Epoch 41: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.4510 - val_loss: 112.3799 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 17.6331\n",
      "Epoch 42: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6947 - val_loss: 108.9850 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 16.6874\n",
      "Epoch 43: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6938 - val_loss: 104.9397 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.5186\n",
      "Epoch 44: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4408 - val_loss: 113.2941 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 17.3655\n",
      "Epoch 45: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.0107 - val_loss: 110.4400 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 16.7305\n",
      "Epoch 46: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.9729 - val_loss: 105.7585 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.6816\n",
      "Epoch 47: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6754 - val_loss: 105.0791 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.5926\n",
      "Epoch 48: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5890 - val_loss: 106.7342 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 15.8808\n",
      "Epoch 49: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8416 - val_loss: 105.9846 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.0189\n",
      "Epoch 50: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.0573 - val_loss: 100.8463 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.4320\n",
      "Epoch 51: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4320 - val_loss: 102.3523 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.0044\n",
      "Epoch 52: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.9498 - val_loss: 101.9650 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 16.1180\n",
      "Epoch 53: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.1956 - val_loss: 103.0563 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.3069\n",
      "Epoch 54: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.6130 - val_loss: 98.5780 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.3873\n",
      "Epoch 55: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3161 - val_loss: 105.3027 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.3789\n",
      "Epoch 56: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3541 - val_loss: 111.6201 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.1495\n",
      "Epoch 57: val_loss did not improve from 77.96255\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1750 - val_loss: 106.0322 - lr: 4.0354e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (641,) y: (641,)\n",
      "Epoch 1/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 3610.8032\n",
      "Epoch 1: val_loss improved from inf to 190.11465, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 3039.9885 - val_loss: 190.1147 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 161.5639\n",
      "Epoch 2: val_loss improved from 190.11465 to 185.44173, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 161.3957 - val_loss: 185.4417 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 136.7316\n",
      "Epoch 3: val_loss did not improve from 185.44173\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 136.7316 - val_loss: 293.5517 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 134.8617\n",
      "Epoch 4: val_loss improved from 185.44173 to 94.28154, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 134.8617 - val_loss: 94.2815 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 102.6505\n",
      "Epoch 5: val_loss did not improve from 94.28154\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 102.5320 - val_loss: 133.5769 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 89.8624\n",
      "Epoch 6: val_loss improved from 94.28154 to 89.50838, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 89.8624 - val_loss: 89.5084 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 88.4729\n",
      "Epoch 7: val_loss did not improve from 89.50838\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 88.4729 - val_loss: 108.5969 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 73.7370\n",
      "Epoch 8: val_loss did not improve from 89.50838\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 73.6013 - val_loss: 100.4393 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 61.5438\n",
      "Epoch 9: val_loss did not improve from 89.50838\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.3113 - val_loss: 101.5576 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 62.9819\n",
      "Epoch 10: val_loss improved from 89.50838 to 84.74324, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 62.9819 - val_loss: 84.7432 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 63.5396\n",
      "Epoch 11: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 63.2127 - val_loss: 93.4904 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 55.2785\n",
      "Epoch 12: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 55.0273 - val_loss: 107.4157 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 61.5832\n",
      "Epoch 13: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 61.7891 - val_loss: 165.8970 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 60.7640\n",
      "Epoch 14: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 60.7647 - val_loss: 113.9963 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 48.2778\n",
      "Epoch 15: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.1227 - val_loss: 94.9451 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 39.4441\n",
      "Epoch 16: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 39.3416 - val_loss: 94.2599 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 36.4507\n",
      "Epoch 17: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.4306 - val_loss: 133.9461 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 34.3706\n",
      "Epoch 18: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 34.0887 - val_loss: 138.8788 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 33.3315\n",
      "Epoch 19: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.3648 - val_loss: 109.5487 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 32.0906\n",
      "Epoch 20: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.7821 - val_loss: 101.8452 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 29.0680\n",
      "Epoch 21: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.9755 - val_loss: 93.5027 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 29.5348\n",
      "Epoch 22: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.2959 - val_loss: 97.5852 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 26.7052\n",
      "Epoch 23: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.8811 - val_loss: 114.1014 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 29.7720\n",
      "Epoch 24: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.7647 - val_loss: 90.1256 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 26.9417\n",
      "Epoch 25: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.7751 - val_loss: 112.6746 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 24.3512\n",
      "Epoch 26: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.3255 - val_loss: 93.8774 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 22.5568\n",
      "Epoch 27: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.2259 - val_loss: 115.4017 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 22.5847\n",
      "Epoch 28: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.6231 - val_loss: 113.7088 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 20.8353\n",
      "Epoch 29: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.8639 - val_loss: 113.9281 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 26.8264\n",
      "Epoch 30: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.8050 - val_loss: 91.0698 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 23.5938\n",
      "Epoch 31: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.9513 - val_loss: 112.1905 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 21.2443\n",
      "Epoch 32: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.5109 - val_loss: 97.7064 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 20.8401\n",
      "Epoch 33: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.1617 - val_loss: 105.9264 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 20.5818\n",
      "Epoch 34: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.0191 - val_loss: 116.2168 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 20.3411\n",
      "Epoch 35: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.9424 - val_loss: 96.5897 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.5604\n",
      "Epoch 36: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.5090 - val_loss: 106.4664 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 19.7980\n",
      "Epoch 37: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.4912 - val_loss: 104.4314 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 19.0333\n",
      "Epoch 38: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.0623 - val_loss: 106.3221 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 18.1499\n",
      "Epoch 39: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.0797 - val_loss: 103.3206 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.0490\n",
      "Epoch 40: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.1306 - val_loss: 110.0133 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 17.3809\n",
      "Epoch 41: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.3741 - val_loss: 124.6633 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 18.5178\n",
      "Epoch 42: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.3805 - val_loss: 98.8131 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 17.6654\n",
      "Epoch 43: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.6817 - val_loss: 92.2786 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 17.7464\n",
      "Epoch 44: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.6340 - val_loss: 111.2951 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 17.4548\n",
      "Epoch 45: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4371 - val_loss: 104.8827 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 16.5691\n",
      "Epoch 46: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.6633 - val_loss: 102.0997 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 16.0758\n",
      "Epoch 47: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.1444 - val_loss: 109.2922 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 16.3394\n",
      "Epoch 48: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3546 - val_loss: 104.0925 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 15.6155\n",
      "Epoch 49: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5906 - val_loss: 104.6288 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.8688\n",
      "Epoch 50: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.8688 - val_loss: 103.9655 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 16.4060\n",
      "Epoch 51: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.2081 - val_loss: 107.0992 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.0805\n",
      "Epoch 52: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.9507 - val_loss: 109.3992 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 15.6481\n",
      "Epoch 53: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.6762 - val_loss: 101.3935 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.5842\n",
      "Epoch 54: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7346 - val_loss: 100.0855 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 15.3595\n",
      "Epoch 55: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.3632 - val_loss: 103.4011 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 15.3058\n",
      "Epoch 56: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.3882 - val_loss: 107.6694 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.3869\n",
      "Epoch 57: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.3869 - val_loss: 101.4582 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 14.8179\n",
      "Epoch 58: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.8303 - val_loss: 102.7754 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 15.7648\n",
      "Epoch 59: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.7163 - val_loss: 102.4738 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.3123\n",
      "Epoch 60: val_loss did not improve from 84.74324\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1418 - val_loss: 105.0194 - lr: 4.0354e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 18925.9180\n",
      "Epoch 1: val_loss improved from inf to 120.87105, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 17452.2832 - val_loss: 120.8710 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 141.9414\n",
      "Epoch 2: val_loss did not improve from 120.87105\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 139.1466 - val_loss: 218.6842 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 123.4115\n",
      "Epoch 3: val_loss did not improve from 120.87105\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 124.2985 - val_loss: 199.8529 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 103.7970\n",
      "Epoch 4: val_loss improved from 120.87105 to 85.67136, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 102.0101 - val_loss: 85.6714 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 104.1302\n",
      "Epoch 5: val_loss improved from 85.67136 to 82.46188, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 103.4055 - val_loss: 82.4619 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 100.1275\n",
      "Epoch 6: val_loss improved from 82.46188 to 76.38062, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 102.9753 - val_loss: 76.3806 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 83.0891\n",
      "Epoch 7: val_loss did not improve from 76.38062\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 82.0598 - val_loss: 149.5747 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 67.1675\n",
      "Epoch 8: val_loss did not improve from 76.38062\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 70.6646 - val_loss: 84.7451 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 64.1515\n",
      "Epoch 9: val_loss improved from 76.38062 to 73.55952, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 64.1163 - val_loss: 73.5595 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 61.0486\n",
      "Epoch 10: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 60.9588 - val_loss: 105.8494 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 57.1097\n",
      "Epoch 11: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.2248 - val_loss: 101.5964 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 51.4054\n",
      "Epoch 12: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.8955 - val_loss: 75.0407 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 52.7899\n",
      "Epoch 13: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 52.8204 - val_loss: 116.3253 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 70.3463\n",
      "Epoch 14: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 69.7316 - val_loss: 92.6138 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 48.3206\n",
      "Epoch 15: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 48.1978 - val_loss: 93.4451 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 42.1915\n",
      "Epoch 16: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 42.0143 - val_loss: 132.0271 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 37.4875\n",
      "Epoch 17: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.6275 - val_loss: 83.6730 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 37.0338\n",
      "Epoch 18: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.1068 - val_loss: 86.0237 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 37.1808\n",
      "Epoch 19: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.2665 - val_loss: 108.3077 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 28.6424\n",
      "Epoch 20: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 28.4641 - val_loss: 96.7595 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 37.7454\n",
      "Epoch 21: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 37.7730 - val_loss: 80.7066 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 31.3330\n",
      "Epoch 22: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 31.3393 - val_loss: 108.9912 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 27.3084\n",
      "Epoch 23: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.4727 - val_loss: 86.8770 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 26.1108\n",
      "Epoch 24: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.5348 - val_loss: 83.6423 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 44.2064\n",
      "Epoch 25: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 43.2231 - val_loss: 127.2115 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 30.6474\n",
      "Epoch 26: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.6474 - val_loss: 82.5110 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 36.1768\n",
      "Epoch 27: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 36.1768 - val_loss: 109.6078 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 37.1674\n",
      "Epoch 28: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 36.9798 - val_loss: 101.7742 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 28.1268\n",
      "Epoch 29: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 27.9114 - val_loss: 82.7258 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 26.6307\n",
      "Epoch 30: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.6990 - val_loss: 92.4266 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 24.6883\n",
      "Epoch 31: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.6883 - val_loss: 87.9682 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 22.7896\n",
      "Epoch 32: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.7837 - val_loss: 91.7414 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 23.5994\n",
      "Epoch 33: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.4764 - val_loss: 100.4078 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 20.5356\n",
      "Epoch 34: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.5604 - val_loss: 97.2759 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.4617\n",
      "Epoch 35: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.5138 - val_loss: 99.0579 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.9494\n",
      "Epoch 36: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.8893 - val_loss: 101.9230 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 17.7386\n",
      "Epoch 37: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.7386 - val_loss: 97.2557 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 18.2031\n",
      "Epoch 38: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.0336 - val_loss: 101.6569 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 16.9961\n",
      "Epoch 39: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9848 - val_loss: 93.1424 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 15.7193\n",
      "Epoch 40: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.5840 - val_loss: 93.2199 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.6777\n",
      "Epoch 41: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.6659 - val_loss: 99.8097 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 15.3244\n",
      "Epoch 42: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5387 - val_loss: 88.5620 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.2510\n",
      "Epoch 43: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.2938 - val_loss: 88.5770 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 15.1781\n",
      "Epoch 44: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.1772 - val_loss: 97.8953 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.8924\n",
      "Epoch 45: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.7106 - val_loss: 91.4241 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 14.8360\n",
      "Epoch 46: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.9061 - val_loss: 85.5678 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 14.4276\n",
      "Epoch 47: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.3531 - val_loss: 95.7450 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.8279\n",
      "Epoch 48: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.8183 - val_loss: 99.0026 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 14.1148\n",
      "Epoch 49: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.2465 - val_loss: 97.0573 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 13.2852\n",
      "Epoch 50: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2911 - val_loss: 97.0900 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 13.5824\n",
      "Epoch 51: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.7079 - val_loss: 88.3091 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 13.4031\n",
      "Epoch 52: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.4227 - val_loss: 94.2630 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.2640\n",
      "Epoch 53: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.2541 - val_loss: 87.3457 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 13.7434\n",
      "Epoch 54: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.6367 - val_loss: 97.7416 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 13.4490\n",
      "Epoch 55: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.4282 - val_loss: 96.0530 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.4572\n",
      "Epoch 56: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.4084 - val_loss: 93.8619 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 13.8159\n",
      "Epoch 57: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.5633 - val_loss: 90.0816 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.4374\n",
      "Epoch 58: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.5289 - val_loss: 89.7808 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 13.3468\n",
      "Epoch 59: val_loss did not improve from 73.55952\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 13.2050 - val_loss: 98.4097 - lr: 4.0354e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 3003.0046\n",
      "Epoch 1: val_loss improved from inf to 131.33057, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 2811.1370 - val_loss: 131.3306 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 156.1852\n",
      "Epoch 2: val_loss improved from 131.33057 to 128.69325, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 156.1852 - val_loss: 128.6933 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 125.7903\n",
      "Epoch 3: val_loss improved from 128.69325 to 123.42503, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 125.9206 - val_loss: 123.4250 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 107.3616\n",
      "Epoch 4: val_loss improved from 123.42503 to 86.62326, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 105.5956 - val_loss: 86.6233 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 88.4487\n",
      "Epoch 5: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 88.0745 - val_loss: 88.5750 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 89.9060\n",
      "Epoch 6: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 89.8438 - val_loss: 132.9220 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 92.6248\n",
      "Epoch 7: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 92.3783 - val_loss: 249.3524 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 88.6393\n",
      "Epoch 8: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 88.5875 - val_loss: 95.9322 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 71.5434\n",
      "Epoch 9: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 71.8753 - val_loss: 121.6371 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 59.4650\n",
      "Epoch 10: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 59.7495 - val_loss: 108.6504 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 57.8906\n",
      "Epoch 11: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 58.0488 - val_loss: 92.8490 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 53.1392\n",
      "Epoch 12: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 51.7781 - val_loss: 89.1060 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 50.8190\n",
      "Epoch 13: val_loss did not improve from 86.62326\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 50.2618 - val_loss: 110.9893 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 57.6681\n",
      "Epoch 14: val_loss improved from 86.62326 to 77.67580, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 56.7238 - val_loss: 77.6758 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 45.2559\n",
      "Epoch 15: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 45.1704 - val_loss: 83.9092 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 44.3731\n",
      "Epoch 16: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.3587 - val_loss: 83.4674 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 40.9365\n",
      "Epoch 17: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.0992 - val_loss: 109.3235 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 37.1857\n",
      "Epoch 18: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.4326 - val_loss: 82.7872 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 33.7239\n",
      "Epoch 19: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.7184 - val_loss: 109.1842 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 46.0934\n",
      "Epoch 20: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 45.8410 - val_loss: 95.7601 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 33.6185\n",
      "Epoch 21: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 33.2630 - val_loss: 88.4660 - lr: 0.0049\n",
      "Epoch 22/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 34.1713\n",
      "Epoch 22: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.2547 - val_loss: 147.1592 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 33.3223\n",
      "Epoch 23: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 32.9809 - val_loss: 90.6908 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 27.1664\n",
      "Epoch 24: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 27.1664 - val_loss: 89.3980 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 29.2948\n",
      "Epoch 25: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.3536 - val_loss: 119.5949 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 30.2822\n",
      "Epoch 26: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.2804 - val_loss: 83.7334 - lr: 0.0034\n",
      "Epoch 27/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 25.8317\n",
      "Epoch 27: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.9194 - val_loss: 92.2364 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 30.2440\n",
      "Epoch 28: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.0539 - val_loss: 85.0045 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 25.1525\n",
      "Epoch 29: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.0593 - val_loss: 87.3306 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 25.4473\n",
      "Epoch 30: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.1078 - val_loss: 87.5200 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 23.0828\n",
      "Epoch 31: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 23.0828 - val_loss: 84.8900 - lr: 0.0024\n",
      "Epoch 32/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 22.7831\n",
      "Epoch 32: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.3406 - val_loss: 87.8146 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 20.7112\n",
      "Epoch 33: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.0069 - val_loss: 95.7765 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 21.1155\n",
      "Epoch 34: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 21.1078 - val_loss: 86.7847 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 19.0787\n",
      "Epoch 35: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.9574 - val_loss: 93.6373 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 19.5882\n",
      "Epoch 36: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.6017 - val_loss: 96.6204 - lr: 0.0017\n",
      "Epoch 37/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.9516\n",
      "Epoch 37: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.9775 - val_loss: 97.2108 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 19.5716\n",
      "Epoch 38: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.0646 - val_loss: 89.5379 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 18.4677\n",
      "Epoch 39: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 18.5650 - val_loss: 95.6402 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 17.6260\n",
      "Epoch 40: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.6265 - val_loss: 84.7497 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.6285\n",
      "Epoch 41: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.5553 - val_loss: 88.8529 - lr: 0.0012\n",
      "Epoch 42/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 17.4569\n",
      "Epoch 42: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.5917 - val_loss: 96.8735 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 16.6772\n",
      "Epoch 43: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.7109 - val_loss: 88.4329 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 17.6684\n",
      "Epoch 44: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.5828 - val_loss: 97.2484 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 17.0886\n",
      "Epoch 45: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.9486 - val_loss: 95.7129 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 17.4185\n",
      "Epoch 46: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4092 - val_loss: 91.4616 - lr: 8.2354e-04\n",
      "Epoch 47/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 17.0631\n",
      "Epoch 47: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.4664 - val_loss: 97.1264 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 17.4273\n",
      "Epoch 48: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.3112 - val_loss: 89.8239 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 16.4702\n",
      "Epoch 49: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.4365 - val_loss: 91.4401 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 15.9198\n",
      "Epoch 50: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.8960 - val_loss: 91.9616 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 15.2426\n",
      "Epoch 51: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.3499 - val_loss: 94.5432 - lr: 5.7648e-04\n",
      "Epoch 52/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 16.7504\n",
      "Epoch 52: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.7081 - val_loss: 93.9158 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 17.8365\n",
      "Epoch 53: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.7709 - val_loss: 89.1791 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 16.7634\n",
      "Epoch 54: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.7429 - val_loss: 88.1269 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.3572\n",
      "Epoch 55: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.3825 - val_loss: 100.3245 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 16.0529\n",
      "Epoch 56: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.0529 - val_loss: 101.0028 - lr: 4.0354e-04\n",
      "Epoch 57/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.6048\n",
      "Epoch 57: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.4577 - val_loss: 93.5428 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 16.5958\n",
      "Epoch 58: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 16.5450 - val_loss: 90.3207 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.4772\n",
      "Epoch 59: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 15.4772 - val_loss: 93.8402 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 15.5858\n",
      "Epoch 60: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.5686 - val_loss: 92.9818 - lr: 2.8248e-04\n",
      "Epoch 61/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 15.4831\n",
      "Epoch 61: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.5228 - val_loss: 96.4963 - lr: 2.8248e-04\n",
      "Epoch 62/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.9809\n",
      "Epoch 62: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.9199 - val_loss: 97.1407 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 15.0420\n",
      "Epoch 63: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.0136 - val_loss: 95.8517 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.3242\n",
      "Epoch 64: val_loss did not improve from 77.67580\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.3759 - val_loss: 92.0151 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 3775.3455\n",
      "Epoch 1: val_loss improved from inf to 149.32484, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 3732.7666 - val_loss: 149.3248 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 157.3486\n",
      "Epoch 2: val_loss improved from 149.32484 to 125.76376, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 155.6847 - val_loss: 125.7638 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 127.4388\n",
      "Epoch 3: val_loss improved from 125.76376 to 93.05619, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 127.6303 - val_loss: 93.0562 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 121.6697\n",
      "Epoch 4: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 120.6911 - val_loss: 123.1086 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 100.2615\n",
      "Epoch 5: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 98.9492 - val_loss: 102.0614 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 87.9703\n",
      "Epoch 6: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 87.9250 - val_loss: 121.4968 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 82.8097\n",
      "Epoch 7: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 82.3138 - val_loss: 200.8400 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 82.0618\n",
      "Epoch 8: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 81.7054 - val_loss: 199.5850 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 68.4383\n",
      "Epoch 9: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 68.4520 - val_loss: 100.8333 - lr: 0.0070\n",
      "Epoch 10/500\n",
      "72/91 [======================>.......] - ETA: 0s - loss: 57.9326\n",
      "Epoch 10: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.6040 - val_loss: 98.7675 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 56.7913\n",
      "Epoch 11: val_loss did not improve from 93.05619\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 56.6200 - val_loss: 115.9252 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 57.7474\n",
      "Epoch 12: val_loss improved from 93.05619 to 81.09313, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.5661 - val_loss: 81.0931 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 54.3581\n",
      "Epoch 13: val_loss did not improve from 81.09313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 54.3581 - val_loss: 93.4569 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 42.1928\n",
      "Epoch 14: val_loss did not improve from 81.09313\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 42.4142 - val_loss: 103.8452 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 42.4568\n",
      "Epoch 15: val_loss did not improve from 81.09313\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.5214 - val_loss: 95.5762 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 40.0111\n",
      "Epoch 16: val_loss improved from 81.09313 to 80.49686, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 40.0111 - val_loss: 80.4969 - lr: 0.0070\n",
      "Epoch 17/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 40.4929\n",
      "Epoch 17: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 40.8826 - val_loss: 124.3962 - lr: 0.0070\n",
      "Epoch 18/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 34.7114\n",
      "Epoch 18: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.8244 - val_loss: 115.2046 - lr: 0.0070\n",
      "Epoch 19/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 247.0628\n",
      "Epoch 19: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 253.6233 - val_loss: 199.7757 - lr: 0.0070\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 738.8129\n",
      "Epoch 20: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 710.2252 - val_loss: 142.3748 - lr: 0.0070\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 92.2167\n",
      "Epoch 21: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 97.5416 - val_loss: 113.2104 - lr: 0.0070\n",
      "Epoch 22/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 72.1974\n",
      "Epoch 22: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 70.2219 - val_loss: 113.2712 - lr: 0.0049\n",
      "Epoch 23/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 57.5258\n",
      "Epoch 23: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 57.0720 - val_loss: 92.9218 - lr: 0.0049\n",
      "Epoch 24/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 53.2348\n",
      "Epoch 24: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 53.0608 - val_loss: 85.3418 - lr: 0.0049\n",
      "Epoch 25/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 49.6577\n",
      "Epoch 25: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 48.6811 - val_loss: 115.4687 - lr: 0.0049\n",
      "Epoch 26/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 121.4233\n",
      "Epoch 26: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 115.4956 - val_loss: 100.8784 - lr: 0.0049\n",
      "Epoch 27/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 60.0798\n",
      "Epoch 27: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 59.8945 - val_loss: 85.6587 - lr: 0.0034\n",
      "Epoch 28/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 90.1170\n",
      "Epoch 28: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 90.1170 - val_loss: 119.1963 - lr: 0.0034\n",
      "Epoch 29/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 67.4932\n",
      "Epoch 29: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 66.2088 - val_loss: 94.2948 - lr: 0.0034\n",
      "Epoch 30/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 51.6423\n",
      "Epoch 30: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 51.9028 - val_loss: 82.4822 - lr: 0.0034\n",
      "Epoch 31/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 43.9157\n",
      "Epoch 31: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 43.2150 - val_loss: 118.0138 - lr: 0.0034\n",
      "Epoch 32/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 37.7923\n",
      "Epoch 32: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.8600 - val_loss: 119.0608 - lr: 0.0024\n",
      "Epoch 33/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 44.6335\n",
      "Epoch 33: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 44.6360 - val_loss: 94.7343 - lr: 0.0024\n",
      "Epoch 34/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 35.4887\n",
      "Epoch 34: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 35.5028 - val_loss: 93.4782 - lr: 0.0024\n",
      "Epoch 35/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 35.0315\n",
      "Epoch 35: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 34.9339 - val_loss: 94.2205 - lr: 0.0024\n",
      "Epoch 36/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 92.5720\n",
      "Epoch 36: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 91.8253 - val_loss: 114.8147 - lr: 0.0024\n",
      "Epoch 37/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 240.7270\n",
      "Epoch 37: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 231.5643 - val_loss: 92.2791 - lr: 0.0017\n",
      "Epoch 38/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 47.3003\n",
      "Epoch 38: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 46.2186 - val_loss: 90.1443 - lr: 0.0017\n",
      "Epoch 39/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 37.6693\n",
      "Epoch 39: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.2442 - val_loss: 88.6042 - lr: 0.0017\n",
      "Epoch 40/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 35.8046\n",
      "Epoch 40: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 35.9459 - val_loss: 80.8551 - lr: 0.0017\n",
      "Epoch 41/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 35.2044\n",
      "Epoch 41: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 35.1858 - val_loss: 99.5445 - lr: 0.0017\n",
      "Epoch 42/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 33.9368\n",
      "Epoch 42: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.9111 - val_loss: 92.2474 - lr: 0.0012\n",
      "Epoch 43/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 31.8507\n",
      "Epoch 43: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 32.0201 - val_loss: 86.4045 - lr: 0.0012\n",
      "Epoch 44/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 31.0374\n",
      "Epoch 44: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.9351 - val_loss: 92.4687 - lr: 0.0012\n",
      "Epoch 45/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 31.1422\n",
      "Epoch 45: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.7436 - val_loss: 101.2585 - lr: 0.0012\n",
      "Epoch 46/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 29.7363\n",
      "Epoch 46: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.6997 - val_loss: 88.6407 - lr: 0.0012\n",
      "Epoch 47/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 30.8056\n",
      "Epoch 47: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 30.6982 - val_loss: 92.0313 - lr: 8.2354e-04\n",
      "Epoch 48/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 29.3838\n",
      "Epoch 48: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 29.2782 - val_loss: 83.1329 - lr: 8.2354e-04\n",
      "Epoch 49/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 29.4982\n",
      "Epoch 49: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.2819 - val_loss: 104.9164 - lr: 8.2354e-04\n",
      "Epoch 50/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 27.2635\n",
      "Epoch 50: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.2550 - val_loss: 94.6828 - lr: 8.2354e-04\n",
      "Epoch 51/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 28.2421\n",
      "Epoch 51: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 28.3458 - val_loss: 90.2375 - lr: 8.2354e-04\n",
      "Epoch 52/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 27.3300\n",
      "Epoch 52: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 27.3300 - val_loss: 91.6368 - lr: 5.7648e-04\n",
      "Epoch 53/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 26.8361\n",
      "Epoch 53: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.8262 - val_loss: 95.1590 - lr: 5.7648e-04\n",
      "Epoch 54/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 26.9238\n",
      "Epoch 54: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.8037 - val_loss: 93.2944 - lr: 5.7648e-04\n",
      "Epoch 55/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 26.9939\n",
      "Epoch 55: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 26.9220 - val_loss: 90.2039 - lr: 5.7648e-04\n",
      "Epoch 56/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 26.8882\n",
      "Epoch 56: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.8837 - val_loss: 91.1747 - lr: 5.7648e-04\n",
      "Epoch 57/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 26.6394\n",
      "Epoch 57: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.7702 - val_loss: 90.7732 - lr: 4.0354e-04\n",
      "Epoch 58/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 25.8600\n",
      "Epoch 58: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.1500 - val_loss: 92.8767 - lr: 4.0354e-04\n",
      "Epoch 59/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 26.0547\n",
      "Epoch 59: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.9855 - val_loss: 94.0889 - lr: 4.0354e-04\n",
      "Epoch 60/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 25.3581\n",
      "Epoch 60: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.3581 - val_loss: 93.9713 - lr: 4.0354e-04\n",
      "Epoch 61/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 24.5228\n",
      "Epoch 61: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.5344 - val_loss: 91.4986 - lr: 4.0354e-04\n",
      "Epoch 62/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 25.4436\n",
      "Epoch 62: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.4436 - val_loss: 91.2720 - lr: 2.8248e-04\n",
      "Epoch 63/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 24.1859\n",
      "Epoch 63: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.4566 - val_loss: 89.2523 - lr: 2.8248e-04\n",
      "Epoch 64/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 24.6745\n",
      "Epoch 64: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.6402 - val_loss: 94.5398 - lr: 2.8248e-04\n",
      "Epoch 65/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 23.8622\n",
      "Epoch 65: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.8874 - val_loss: 92.2174 - lr: 2.8248e-04\n",
      "Epoch 66/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 25.0373\n",
      "Epoch 66: val_loss did not improve from 80.49686\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 25.0373 - val_loss: 94.8210 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Epoch 1/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 15354.9434\n",
      "Epoch 1: val_loss improved from inf to 209.32401, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 13995.3877 - val_loss: 209.3240 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 150.6913\n",
      "Epoch 2: val_loss improved from 209.32401 to 94.87111, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 146.3732 - val_loss: 94.8711 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 115.0250\n",
      "Epoch 3: val_loss did not improve from 94.87111\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 116.4001 - val_loss: 131.0912 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 117.1073\n",
      "Epoch 4: val_loss improved from 94.87111 to 79.97276, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 117.0872 - val_loss: 79.9728 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 97.9802 \n",
      "Epoch 5: val_loss did not improve from 79.97276\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 97.9047 - val_loss: 87.9556 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 81.0931\n",
      "Epoch 6: val_loss did not improve from 79.97276\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 84.9488 - val_loss: 100.5116 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 79.6874\n",
      "Epoch 7: val_loss did not improve from 79.97276\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 79.3072 - val_loss: 96.3357 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 66.1874\n",
      "Epoch 8: val_loss did not improve from 79.97276\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 66.1684 - val_loss: 110.9473 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 59.2657\n",
      "Epoch 9: val_loss did not improve from 79.97276\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 59.6031 - val_loss: 156.7370 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 56.4586\n",
      "Epoch 10: val_loss improved from 79.97276 to 74.47807, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 56.6561 - val_loss: 74.4781 - lr: 0.0070\n",
      "Epoch 11/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 50.3757\n",
      "Epoch 11: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 50.4692 - val_loss: 79.4478 - lr: 0.0070\n",
      "Epoch 12/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 46.8652\n",
      "Epoch 12: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 46.4722 - val_loss: 124.9365 - lr: 0.0070\n",
      "Epoch 13/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 42.3593\n",
      "Epoch 13: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.4688 - val_loss: 115.3547 - lr: 0.0070\n",
      "Epoch 14/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 43.1504\n",
      "Epoch 14: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 43.1617 - val_loss: 74.8243 - lr: 0.0070\n",
      "Epoch 15/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 42.1249\n",
      "Epoch 15: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 42.0190 - val_loss: 88.4828 - lr: 0.0070\n",
      "Epoch 16/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 34.0582\n",
      "Epoch 16: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 33.9777 - val_loss: 100.8274 - lr: 0.0049\n",
      "Epoch 17/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 30.0517\n",
      "Epoch 17: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.3930 - val_loss: 82.4476 - lr: 0.0049\n",
      "Epoch 18/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 28.3343\n",
      "Epoch 18: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 28.0924 - val_loss: 86.8816 - lr: 0.0049\n",
      "Epoch 19/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 26.9987\n",
      "Epoch 19: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 26.8637 - val_loss: 106.1498 - lr: 0.0049\n",
      "Epoch 20/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 26.3836\n",
      "Epoch 20: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 27.1108 - val_loss: 76.3678 - lr: 0.0049\n",
      "Epoch 21/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 24.4852\n",
      "Epoch 21: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.3829 - val_loss: 88.9067 - lr: 0.0034\n",
      "Epoch 22/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 24.7769\n",
      "Epoch 22: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 24.3724 - val_loss: 97.6123 - lr: 0.0034\n",
      "Epoch 23/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 22.3964\n",
      "Epoch 23: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 22.3964 - val_loss: 105.0259 - lr: 0.0034\n",
      "Epoch 24/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 20.7974\n",
      "Epoch 24: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.7846 - val_loss: 94.8783 - lr: 0.0034\n",
      "Epoch 25/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 20.9867\n",
      "Epoch 25: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 20.9464 - val_loss: 82.8802 - lr: 0.0034\n",
      "Epoch 26/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 21.5352\n",
      "Epoch 26: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 21.4948 - val_loss: 91.3452 - lr: 0.0024\n",
      "Epoch 27/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 18.1948\n",
      "Epoch 27: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.3298 - val_loss: 87.9652 - lr: 0.0024\n",
      "Epoch 28/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 17.8003\n",
      "Epoch 28: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.0907 - val_loss: 88.2110 - lr: 0.0024\n",
      "Epoch 29/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.2668\n",
      "Epoch 29: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 19.1782 - val_loss: 96.2369 - lr: 0.0024\n",
      "Epoch 30/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 18.8882\n",
      "Epoch 30: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.0009 - val_loss: 111.5994 - lr: 0.0024\n",
      "Epoch 31/500\n",
      "75/91 [=======================>......] - ETA: 0s - loss: 17.4676\n",
      "Epoch 31: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 17.3199 - val_loss: 90.9949 - lr: 0.0017\n",
      "Epoch 32/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 15.9547\n",
      "Epoch 32: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 15.9725 - val_loss: 120.6589 - lr: 0.0017\n",
      "Epoch 33/500\n",
      "77/91 [========================>.....] - ETA: 0s - loss: 15.7837\n",
      "Epoch 33: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.8617 - val_loss: 110.5448 - lr: 0.0017\n",
      "Epoch 34/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 15.7811\n",
      "Epoch 34: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.6875 - val_loss: 89.9032 - lr: 0.0017\n",
      "Epoch 35/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 15.5602\n",
      "Epoch 35: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.4661 - val_loss: 96.7208 - lr: 0.0017\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.1454\n",
      "Epoch 36: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.4074 - val_loss: 110.4945 - lr: 0.0012\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 14.7694\n",
      "Epoch 37: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.7601 - val_loss: 105.3509 - lr: 0.0012\n",
      "Epoch 38/500\n",
      "74/91 [=======================>......] - ETA: 0s - loss: 14.9234\n",
      "Epoch 38: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.9275 - val_loss: 103.6588 - lr: 0.0012\n",
      "Epoch 39/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 14.4293\n",
      "Epoch 39: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.4307 - val_loss: 98.3199 - lr: 0.0012\n",
      "Epoch 40/500\n",
      "73/91 [=======================>......] - ETA: 0s - loss: 13.8713\n",
      "Epoch 40: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0322 - val_loss: 113.6277 - lr: 0.0012\n",
      "Epoch 41/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.9539\n",
      "Epoch 41: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.0360 - val_loss: 114.3714 - lr: 8.2354e-04\n",
      "Epoch 42/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.4144\n",
      "Epoch 42: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.4498 - val_loss: 89.3153 - lr: 8.2354e-04\n",
      "Epoch 43/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 13.3419\n",
      "Epoch 43: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.3197 - val_loss: 102.6731 - lr: 8.2354e-04\n",
      "Epoch 44/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.0308\n",
      "Epoch 44: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 14.0231 - val_loss: 87.7793 - lr: 8.2354e-04\n",
      "Epoch 45/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 13.0414\n",
      "Epoch 45: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.0532 - val_loss: 99.2415 - lr: 8.2354e-04\n",
      "Epoch 46/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 12.7266\n",
      "Epoch 46: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.6759 - val_loss: 99.9700 - lr: 5.7648e-04\n",
      "Epoch 47/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.6837\n",
      "Epoch 47: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.7084 - val_loss: 100.5247 - lr: 5.7648e-04\n",
      "Epoch 48/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 12.9058\n",
      "Epoch 48: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.7979 - val_loss: 107.7594 - lr: 5.7648e-04\n",
      "Epoch 49/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.2732\n",
      "Epoch 49: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.4841 - val_loss: 105.7561 - lr: 5.7648e-04\n",
      "Epoch 50/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 12.9342\n",
      "Epoch 50: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.9187 - val_loss: 105.5990 - lr: 5.7648e-04\n",
      "Epoch 51/500\n",
      "76/91 [========================>.....] - ETA: 0s - loss: 12.5229\n",
      "Epoch 51: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.5959 - val_loss: 107.6964 - lr: 4.0354e-04\n",
      "Epoch 52/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 12.5591\n",
      "Epoch 52: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.5591 - val_loss: 109.8883 - lr: 4.0354e-04\n",
      "Epoch 53/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.8782\n",
      "Epoch 53: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.8722 - val_loss: 104.5668 - lr: 4.0354e-04\n",
      "Epoch 54/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 11.9287\n",
      "Epoch 54: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.8989 - val_loss: 95.7508 - lr: 4.0354e-04\n",
      "Epoch 55/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.9345\n",
      "Epoch 55: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.0920 - val_loss: 104.0136 - lr: 4.0354e-04\n",
      "Epoch 56/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.8190\n",
      "Epoch 56: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.9380 - val_loss: 97.7835 - lr: 2.8248e-04\n",
      "Epoch 57/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 11.4527\n",
      "Epoch 57: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.3961 - val_loss: 95.6452 - lr: 2.8248e-04\n",
      "Epoch 58/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.0750\n",
      "Epoch 58: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 12.0666 - val_loss: 103.0416 - lr: 2.8248e-04\n",
      "Epoch 59/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.2636\n",
      "Epoch 59: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 3ms/step - loss: 11.3141 - val_loss: 104.7553 - lr: 2.8248e-04\n",
      "Epoch 60/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 11.2172\n",
      "Epoch 60: val_loss did not improve from 74.47807\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.1182 - val_loss: 98.7180 - lr: 2.8248e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Shapes - y_pred: (640,) y: (640,)\n",
      "Mean MSE: 78.75543845579838\n",
      "Mean R-squared: 0.7874894448834455\n",
      "Average prop10: 0.7751769695787831\n",
      "Average prop5: 0.4969561719968799\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predict the target values\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    print(\"Shapes - y_pred:\", y_pred.shape, \"y:\", y.shape)\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    error = y_pred - y\n",
    "    mse = np.mean(error ** 2)\n",
    "    r_squared = 1 - (np.sum(error ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    \n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = MI_filtered_pivoted_data.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# Perform cross-validation for MLP\n",
    "def mlp_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    x = Dense(units=256, activation='relu')(inputs)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=50,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.7,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mlp_MI_mse_list = []\n",
    "mlp_MI_r_squared_list = []\n",
    "mlp_MI_prop10_list = []\n",
    "mlp_MI_prop5_list = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create mlp model with dropout\n",
    "    model_mlp = mlp_model_with_dropout((X_train.shape[1]), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'best_model_fold_{fold_idx}.h5',  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_mlp.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks including ModelCheckpoint\n",
    "    model_mlp.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # After training, you can load the best model using the following:\n",
    "    best_model_mlp = load_model(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(best_model_mlp, X_test_scaled, y_test)\n",
    "\n",
    "    mlp_MI_mse_list.append(mse)\n",
    "    mlp_MI_r_squared_list.append(r_squared)\n",
    "    mlp_MI_prop10_list.append(prop10)\n",
    "    mlp_MI_prop5_list.append(prop5)\n",
    "\n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(mlp_MI_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(mlp_MI_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(mlp_MI_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(mlp_MI_prop5_list))\n",
    "\n",
    "CV_results2['mlp-MI'] = {'mse': mlp_MI_mse_list,\n",
    "                         'r_squared': mlp_MI_r_squared_list,\n",
    "                         'prop10': mlp_MI_prop10_list,\n",
    "                         'prop5': mlp_MI_prop5_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:13:34] WARNING: D:\\bld\\xgboost-split_1703077804448\\work\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 59.19932039744748\n",
      "Average R-squared: 0.8401362642351794\n",
      "Average prop10: 0.8160788806552262\n",
      "Average prop5: 0.5787551189547582\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for GBDT\n",
    "gbdt_model = XGBRegressor(n_estimators=100,\n",
    "                          learning_rate=0.05,\n",
    "                          max_depth=10,\n",
    "                          subsample= 0.9,\n",
    "                          min_child_weight= 2,\n",
    "                          random_state=10,\n",
    "                          tree_method='hist',\n",
    "                          device='cuda')\n",
    "\n",
    "gbdt_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                  ('regressor', gbdt_model)])\n",
    "\n",
    "gbdt_filtered_mse_list = []\n",
    "gbdt_filtered_r_squared_list = []\n",
    "gbdt_filtered_prop10_list = []\n",
    "gbdt_filtered_prop5_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    gbdt_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(gbdt_pipeline, X_test, y_test)\n",
    "    \n",
    "    gbdt_filtered_mse_list.append(mse)\n",
    "    gbdt_filtered_r_squared_list.append(r_squared)\n",
    "    gbdt_filtered_prop10_list.append(prop10)\n",
    "    gbdt_filtered_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(gbdt_filtered_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(gbdt_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(gbdt_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(gbdt_filtered_prop5_list))\n",
    "\n",
    "CV_results2['gbdt-filtered'] = {'mse': gbdt_filtered_mse_list,\n",
    "                               'r_squared': gbdt_filtered_r_squared_list,\n",
    "                               'prop10': gbdt_filtered_prop10_list,\n",
    "                               'prop5': gbdt_filtered_prop5_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT-horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 67.15795029470992\n",
      "Average R-squared: 0.8190221258072062\n",
      "Average prop10: 0.7895334438377535\n",
      "Average prop5: 0.5336386017940717\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for GBDT\n",
    "gbdt_model = XGBRegressor(n_estimators=100,\n",
    "                          learning_rate=0.05,\n",
    "                          max_depth=10,\n",
    "                          subsample= 0.9,\n",
    "                          min_child_weight= 2,\n",
    "                          random_state=10,\n",
    "                          tree_method='hist',\n",
    "                          device='cuda')\n",
    "\n",
    "gbdt_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                  ('regressor', gbdt_model)])\n",
    "\n",
    "gbdt_horvath_mse_list = []\n",
    "gbdt_horvath_r_squared_list = []\n",
    "gbdt_horvath_prop10_list = []\n",
    "gbdt_horvath_prop5_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    gbdt_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(gbdt_pipeline, X_test, y_test)\n",
    "    \n",
    "    gbdt_horvath_mse_list.append(mse)\n",
    "    gbdt_horvath_r_squared_list.append(r_squared)\n",
    "    gbdt_horvath_prop10_list.append(prop10)\n",
    "    gbdt_horvath_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(gbdt_horvath_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(gbdt_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(gbdt_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(gbdt_horvath_prop5_list))\n",
    "\n",
    "CV_results2['gbdt-horvath'] = {'mse': gbdt_horvath_mse_list,\n",
    "                               'r_squared': gbdt_horvath_r_squared_list,\n",
    "                               'prop10': gbdt_horvath_prop10_list,\n",
    "                               'prop5': gbdt_horvath_prop5_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT - MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = MI_filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:46:08] WARNING: D:\\bld\\xgboost-split_1703077804448\\work\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 57.96833289648056\n",
      "Average R-squared: 0.8435105342593447\n",
      "Average prop10: 0.8179477866614665\n",
      "Average prop5: 0.5825048751950077\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for GBDT\n",
    "gbdt_model = XGBRegressor(n_estimators=100,\n",
    "                          learning_rate=0.05,\n",
    "                          max_depth=10,\n",
    "                          subsample= 0.9,\n",
    "                          min_child_weight= 2,\n",
    "                          random_state=10,\n",
    "                          tree_method='hist',\n",
    "                          device='cuda')\n",
    "\n",
    "gbdt_pipeline = Pipeline(steps=[('imputer', KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')),\n",
    "                                  ('regressor', gbdt_model)])\n",
    "\n",
    "gbdt_MI_mse_list = []\n",
    "gbdt_MI_r_squared_list = []\n",
    "gbdt_MI_prop10_list = []\n",
    "gbdt_MI_prop5_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    gbdt_pipeline.fit(X_train, y_train)\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(gbdt_pipeline, X_test, y_test)\n",
    "    \n",
    "    gbdt_MI_mse_list.append(mse)\n",
    "    gbdt_MI_r_squared_list.append(r_squared)\n",
    "    gbdt_MI_prop10_list.append(prop10)\n",
    "    gbdt_MI_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(gbdt_MI_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(gbdt_MI_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(gbdt_MI_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(gbdt_MI_prop5_list))\n",
    "\n",
    "CV_results2['gbdt-MI'] = {'mse': gbdt_MI_mse_list,\n",
    "                          'r_squared': gbdt_MI_r_squared_list,\n",
    "                          'prop10': gbdt_MI_prop10_list,\n",
    "                          'prop5': gbdt_MI_prop5_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBRF-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import xgboost as xgb\n",
    "\n",
    "## Perform cross-validation for GBRF\n",
    "# Define the hyperparameters\n",
    "best_params = {'learning_rate': 0.05,\n",
    "               'max_depth': 10,\n",
    "               'subsample': 0.7,\n",
    "               'min_child_weight': 3,\n",
    "               'num_parallel_tree': 200,\n",
    "               'objective': 'reg:squarederror',\n",
    "               'booster': 'gbtree',\n",
    "               'random_state': 10,\n",
    "               'tree_method': 'hist',\n",
    "               'device': 'cuda'}\n",
    "\n",
    "# Lists to store MSE and R-squared values\n",
    "gbrf_filtered_mse_list = []\n",
    "gbrf_filtered_r_squared_list = []\n",
    "gbrf_filtered_prop10_list = []\n",
    "gbrf_filtered_prop5_list = []\n",
    "\n",
    "    \n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Set up the XGBoost model with the best hyperparameters\n",
    "    gbrf_model = xgb.train(best_params, dtrain, num_boost_round=100)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = gbrf_model.predict(dtest)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Append results to lists\n",
    "    gbrf_filtered_mse_list.append(mse)\n",
    "    gbrf_filtered_r_squared_list.append(r_squared)\n",
    "    gbrf_filtered_prop10_list.append(prop10)\n",
    "    gbrf_filtered_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(gbrf_filtered_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(gbrf_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(gbrf_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(gbrf_filtered_prop5_list))\n",
    "\n",
    "CV_results2['gbrf-filtered'] = {'mse': gbrf_filtered_mse_list,\n",
    "                                'r_squared': gbrf_filtered_r_squared_list,\n",
    "                                'prop10': gbrf_filtered_prop10_list,\n",
    "                                'prop5': gbrf_filtered_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBRF-horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import xgboost as xgb\n",
    "\n",
    "## Perform cross-validation for GBRF\n",
    "# Define the hyperparameters\n",
    "best_params = {'learning_rate': 0.05,\n",
    "               'max_depth': 10,\n",
    "               'subsample': 0.7,\n",
    "               'min_child_weight': 3,\n",
    "               'num_parallel_tree': 200,\n",
    "               'objective': 'reg:squarederror',\n",
    "               'booster': 'gbtree',\n",
    "               'random_state': 10,\n",
    "               'tree_method': 'hist',\n",
    "               'device': 'cuda'}\n",
    "\n",
    "# Lists to store MSE and R-squared values\n",
    "gbrf_horvath_mse_list = []\n",
    "gbrf_horvath_r_squared_list = []\n",
    "gbrf_horvath_prop10_list = []\n",
    "gbrf_horvath_prop5_list = []\n",
    "\n",
    "    \n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Set up the XGBoost model with the best hyperparameters\n",
    "    gbrf_model = xgb.train(best_params, dtrain, num_boost_round=100)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = gbrf_model.predict(dtest)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Append results to lists\n",
    "    gbrf_horvath_mse_list.append(mse)\n",
    "    gbrf_horvath_r_squared_list.append(r_squared)\n",
    "    gbrf_horvath_prop10_list.append(prop10)\n",
    "    gbrf_horvath_prop5_list.append(prop5)\n",
    "\n",
    "# Print the average MSE and R-squared values\n",
    "print(\"Average MSE:\", np.mean(gbrf_horvath_mse_list))\n",
    "print(\"Average R-squared:\", np.mean(gbrf_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(gbrf_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(gbrf_horvath_prop5_list))\n",
    "\n",
    "CV_results2['gbrf-horvath'] = {'mse': gbrf_horvath_mse_list,\n",
    "                                'r_squared': gbrf_horvath_r_squared_list,\n",
    "                                'prop10': gbrf_horvath_prop10_list,\n",
    "                                'prop5': gbrf_horvath_prop5_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Create CNN model with dropout\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m model_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model_with_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Define ModelCheckpoint callback for each fold\u001b[39;00m\n\u001b[0;32m    111\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m    112\u001b[0m     filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Unique filename for each fold\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    117\u001b[0m )\n",
      "Cell \u001b[1;32mIn[8], line 58\u001b[0m, in \u001b[0;36mcnn_model_with_dropout\u001b[1;34m(input_shape, dropout_rate)\u001b[0m\n\u001b[0;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Dense Layers with Dropout\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(rate\u001b[38;5;241m=\u001b[39mdropout_rate)(x)\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "#from tensorflow.keras import mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "# expression values for the filtered probes\n",
    "X = filtered_pivoted_data.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float16')\n",
    "y = y.astype('float16')\n",
    "\n",
    "# Perform cross-validation for transformer+CNN\n",
    "def cnn_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    \n",
    "    # MaxPooling Layer\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # LayerNormalization\n",
    "    x = LayerNormalization(epsilon=1e-10)(x)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=50,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.8,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=10,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "cnn_filtered_mse_list = []\n",
    "cnn_filtered_r_squared_list = []\n",
    "cnn_filtered_prop10_list = []\n",
    "cnn_filtered_prop5_list = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create CNN model with dropout\n",
    "    model_cnn = cnn_model_with_dropout((X_train.shape[1], 1), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f\"best_model_fold_{fold_idx}.h5\",  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_cnn.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks including ModelCheckpoint\n",
    "    model_cnn.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # After training, you can load the best model using the following:\n",
    "    best_model_cnn = load_model(f\"best_model_fold_{fold_idx}.h5\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(best_model_cnn, X_test_scaled, y_test)\n",
    "    \n",
    "    cnn_filtered_mse_list.append(mse)\n",
    "    cnn_filtered_r_squared_list.append(r_squared)\n",
    "    cnn_filtered_prop10_list.append(prop10)\n",
    "    cnn_filtered_prop5_list.append(prop5)\n",
    "    \n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f\"best_model_fold_{fold_idx}.h5\")\n",
    "    \n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(cnn_filtered_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(cnn_filtered_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(cnn_filtered_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(cnn_filtered_prop5_list))\n",
    "\n",
    "CV_results2['cnn-filtered'] = {'mse':cnn_filtered_mse_list,\n",
    "                               'r_squared': cnn_filtered_r_squared_list,\n",
    "                               'prop10': cnn_filtered_prop10_list,\n",
    "                               'prop5': cnn_filtered_prop5_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CV_results2.pickle', 'wb') as file:\n",
    "    pickle.dump(CV_results2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lr-filtered-R025', 'rf-filtered-R025', 'lr-horvath', 'rf-horvath', 'svr-filtered', 'svr-horvath', 'mlp-filtered', 'mlp-horvath', 'gbdt-filtered', 'gbdt-horvath', 'cnn-horvath'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_results2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-horvath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 1169.5254\n",
      "Epoch 1: val_loss improved from inf to 228.10191, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 1075.1575 - val_loss: 228.1019 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 104.0267\n",
      "Epoch 2: val_loss improved from 228.10191 to 78.73215, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 103.8096 - val_loss: 78.7322 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 103.6253\n",
      "Epoch 3: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 102.5162 - val_loss: 83.8274 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 82.1277\n",
      "Epoch 4: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 82.0015 - val_loss: 83.3069 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 73.1710\n",
      "Epoch 5: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 73.0949 - val_loss: 178.5178 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 71.2845\n",
      "Epoch 6: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 71.2638 - val_loss: 359.7716 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 71.7293\n",
      "Epoch 7: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 71.2778 - val_loss: 90.0078 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 60.6328\n",
      "Epoch 8: val_loss did not improve from 78.73215\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 60.6622 - val_loss: 112.5155 - lr: 0.0060\n",
      "Epoch 9/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 62.3248\n",
      "Epoch 9: val_loss improved from 78.73215 to 69.38791, saving model to best_model_fold_0.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 62.3599 - val_loss: 69.3879 - lr: 0.0060\n",
      "Epoch 10/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 54.0581\n",
      "Epoch 10: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 53.8227 - val_loss: 125.1615 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 51.3727\n",
      "Epoch 11: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 51.3758 - val_loss: 124.4483 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 46.3798\n",
      "Epoch 12: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 46.4606 - val_loss: 112.4447 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 42.1855\n",
      "Epoch 13: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 42.4472 - val_loss: 90.1024 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 44.0274\n",
      "Epoch 14: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 44.3376 - val_loss: 101.4712 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 37.4133\n",
      "Epoch 15: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 37.2800 - val_loss: 87.7078 - lr: 0.0036\n",
      "Epoch 16/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 31.9184\n",
      "Epoch 16: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 31.8171 - val_loss: 112.6716 - lr: 0.0036\n",
      "Epoch 17/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 29.6473\n",
      "Epoch 17: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.7931 - val_loss: 84.3920 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 29.7119\n",
      "Epoch 18: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.8322 - val_loss: 109.7416 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 26.5086\n",
      "Epoch 19: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 26.5057 - val_loss: 149.4048 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 21.3152\n",
      "Epoch 20: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 21.2957 - val_loss: 109.4215 - lr: 0.0022\n",
      "Epoch 21/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 19.2051\n",
      "Epoch 21: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 19.2008 - val_loss: 133.3732 - lr: 0.0022\n",
      "Epoch 22/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 18.5288\n",
      "Epoch 22: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.4861 - val_loss: 117.6080 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.1241\n",
      "Epoch 23: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.1144 - val_loss: 110.0102 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.4943\n",
      "Epoch 24: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.4099 - val_loss: 128.4293 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 12.1950\n",
      "Epoch 25: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.1878 - val_loss: 124.4856 - lr: 0.0013\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 11.5723\n",
      "Epoch 26: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.5758 - val_loss: 116.0852 - lr: 0.0013\n",
      "Epoch 27/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 11.9071\n",
      "Epoch 27: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.9037 - val_loss: 102.8773 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 11.7405\n",
      "Epoch 28: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.6330 - val_loss: 126.1752 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.0466\n",
      "Epoch 29: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.1322 - val_loss: 110.8129 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 10.4272\n",
      "Epoch 30: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.2665 - val_loss: 123.2791 - lr: 7.7760e-04\n",
      "Epoch 31/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 9.4041\n",
      "Epoch 31: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.4094 - val_loss: 117.1353 - lr: 7.7760e-04\n",
      "Epoch 32/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 9.6049\n",
      "Epoch 32: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.6060 - val_loss: 124.9801 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 9.1232\n",
      "Epoch 33: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.0916 - val_loss: 119.7406 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 8.5272\n",
      "Epoch 34: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.5272 - val_loss: 126.1586 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 8.2622\n",
      "Epoch 35: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.2608 - val_loss: 128.9154 - lr: 4.6656e-04\n",
      "Epoch 36/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 8.2268\n",
      "Epoch 36: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.1726 - val_loss: 124.7372 - lr: 4.6656e-04\n",
      "Epoch 37/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.9007\n",
      "Epoch 37: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 7.9175 - val_loss: 116.3338 - lr: 4.6656e-04\n",
      "Epoch 38/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 8.1074\n",
      "Epoch 38: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.1141 - val_loss: 123.2069 - lr: 4.6656e-04\n",
      "Epoch 39/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.0292\n",
      "Epoch 39: val_loss did not improve from 69.38791\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.9963 - val_loss: 119.0532 - lr: 4.6656e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 2233.7776\n",
      "Epoch 1: val_loss improved from inf to 81.58623, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 2042.5168 - val_loss: 81.5862 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 99.7090 \n",
      "Epoch 2: val_loss did not improve from 81.58623\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 99.4991 - val_loss: 114.7625 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 99.5464\n",
      "Epoch 3: val_loss did not improve from 81.58623\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 99.3359 - val_loss: 158.1711 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 86.7377\n",
      "Epoch 4: val_loss did not improve from 81.58623\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 85.7899 - val_loss: 82.3311 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 80.3352\n",
      "Epoch 5: val_loss did not improve from 81.58623\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 80.0102 - val_loss: 119.7918 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 91.9097\n",
      "Epoch 6: val_loss did not improve from 81.58623\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 88.3841 - val_loss: 150.5203 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 68.8583\n",
      "Epoch 7: val_loss improved from 81.58623 to 62.08890, saving model to best_model_fold_1.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 68.4777 - val_loss: 62.0889 - lr: 0.0060\n",
      "Epoch 8/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 58.0207\n",
      "Epoch 8: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.1383 - val_loss: 84.6631 - lr: 0.0060\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 62.3481\n",
      "Epoch 9: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 62.5000 - val_loss: 72.8835 - lr: 0.0060\n",
      "Epoch 10/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 58.0124\n",
      "Epoch 10: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.1347 - val_loss: 79.2874 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 59.5615\n",
      "Epoch 11: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 59.7517 - val_loss: 108.3483 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 56.0656\n",
      "Epoch 12: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 56.0297 - val_loss: 170.4077 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 55.5377\n",
      "Epoch 13: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 54.7934 - val_loss: 73.5032 - lr: 0.0036\n",
      "Epoch 14/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 48.7587\n",
      "Epoch 14: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 48.7447 - val_loss: 93.1711 - lr: 0.0036\n",
      "Epoch 15/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 45.5249\n",
      "Epoch 15: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 45.1768 - val_loss: 89.4166 - lr: 0.0036\n",
      "Epoch 16/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 43.2904\n",
      "Epoch 16: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 43.5014 - val_loss: 88.7748 - lr: 0.0036\n",
      "Epoch 17/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 46.1051\n",
      "Epoch 17: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 46.2069 - val_loss: 100.4722 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 42.3881\n",
      "Epoch 18: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 42.0276 - val_loss: 100.0448 - lr: 0.0022\n",
      "Epoch 19/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 37.2814\n",
      "Epoch 19: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 37.9527 - val_loss: 95.8750 - lr: 0.0022\n",
      "Epoch 20/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 35.6623\n",
      "Epoch 20: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 35.3771 - val_loss: 91.0532 - lr: 0.0022\n",
      "Epoch 21/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 38.0587\n",
      "Epoch 21: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.0587 - val_loss: 74.6228 - lr: 0.0022\n",
      "Epoch 22/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 33.8897\n",
      "Epoch 22: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 33.8887 - val_loss: 106.9668 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 30.5360\n",
      "Epoch 23: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 30.6197 - val_loss: 109.8696 - lr: 0.0013\n",
      "Epoch 24/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 29.8408\n",
      "Epoch 24: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.7218 - val_loss: 93.2037 - lr: 0.0013\n",
      "Epoch 25/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 29.2787\n",
      "Epoch 25: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.3313 - val_loss: 108.0487 - lr: 0.0013\n",
      "Epoch 26/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 27.3701\n",
      "Epoch 26: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 27.3701 - val_loss: 105.0075 - lr: 0.0013\n",
      "Epoch 27/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 26.6519\n",
      "Epoch 27: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.7786 - val_loss: 119.2498 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 23.4683\n",
      "Epoch 28: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 23.4823 - val_loss: 99.9700 - lr: 7.7760e-04\n",
      "Epoch 29/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 23.1993\n",
      "Epoch 29: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 23.2061 - val_loss: 105.2916 - lr: 7.7760e-04\n",
      "Epoch 30/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 23.1193\n",
      "Epoch 30: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 23.1650 - val_loss: 113.5606 - lr: 7.7760e-04\n",
      "Epoch 31/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 22.5136\n",
      "Epoch 31: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.5042 - val_loss: 99.7279 - lr: 7.7760e-04\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 20.0460\n",
      "Epoch 32: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.0460 - val_loss: 93.4666 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 19.2335\n",
      "Epoch 33: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 19.4421 - val_loss: 99.2295 - lr: 4.6656e-04\n",
      "Epoch 34/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 18.3088\n",
      "Epoch 34: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.4296 - val_loss: 103.9623 - lr: 4.6656e-04\n",
      "Epoch 35/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 17.3863\n",
      "Epoch 35: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 17.4172 - val_loss: 109.5223 - lr: 4.6656e-04\n",
      "Epoch 36/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 17.5068\n",
      "Epoch 36: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 17.5394 - val_loss: 115.3394 - lr: 4.6656e-04\n",
      "Epoch 37/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 16.8454\n",
      "Epoch 37: val_loss did not improve from 62.08890\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.7764 - val_loss: 99.9089 - lr: 4.6656e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 2818.8562\n",
      "Epoch 1: val_loss improved from inf to 118.78659, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 2818.8562 - val_loss: 118.7866 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 126.0146\n",
      "Epoch 2: val_loss did not improve from 118.78659\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 125.9562 - val_loss: 122.1570 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 100.7511\n",
      "Epoch 3: val_loss improved from 118.78659 to 89.69227, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 100.8771 - val_loss: 89.6923 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 82.9542\n",
      "Epoch 4: val_loss did not improve from 89.69227\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 82.8592 - val_loss: 161.2830 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 78.1532\n",
      "Epoch 5: val_loss did not improve from 89.69227\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 78.3535 - val_loss: 120.0256 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 74.7120\n",
      "Epoch 6: val_loss did not improve from 89.69227\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 74.4285 - val_loss: 94.7089 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 75.7462\n",
      "Epoch 7: val_loss did not improve from 89.69227\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 75.7462 - val_loss: 114.2645 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 71.6465\n",
      "Epoch 8: val_loss did not improve from 89.69227\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 71.6437 - val_loss: 103.0302 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 59.1959\n",
      "Epoch 9: val_loss improved from 89.69227 to 69.27579, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 0s 6ms/step - loss: 59.6793 - val_loss: 69.2758 - lr: 0.0060\n",
      "Epoch 10/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 60.0761\n",
      "Epoch 10: val_loss did not improve from 69.27579\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 60.0600 - val_loss: 168.4487 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 61.7897\n",
      "Epoch 11: val_loss did not improve from 69.27579\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 61.8866 - val_loss: 80.2978 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 52.3573\n",
      "Epoch 12: val_loss did not improve from 69.27579\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 52.2973 - val_loss: 112.3816 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 50.8455\n",
      "Epoch 13: val_loss did not improve from 69.27579\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 50.7395 - val_loss: 112.8563 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 49.2019\n",
      "Epoch 14: val_loss did not improve from 69.27579\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 49.2543 - val_loss: 81.6501 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 44.3404\n",
      "Epoch 15: val_loss improved from 69.27579 to 65.49402, saving model to best_model_fold_2.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 44.4755 - val_loss: 65.4940 - lr: 0.0036\n",
      "Epoch 16/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 44.3486\n",
      "Epoch 16: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 44.3600 - val_loss: 178.6711 - lr: 0.0036\n",
      "Epoch 17/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 42.8983\n",
      "Epoch 17: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 42.9214 - val_loss: 87.3097 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 38.6317\n",
      "Epoch 18: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 38.7058 - val_loss: 112.2309 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 40.4806\n",
      "Epoch 19: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 40.4092 - val_loss: 85.2861 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 35.7582\n",
      "Epoch 20: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 35.7068 - val_loss: 94.0413 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 30.7445\n",
      "Epoch 21: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.6740 - val_loss: 82.2199 - lr: 0.0022\n",
      "Epoch 22/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 29.0733\n",
      "Epoch 22: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.0733 - val_loss: 93.5145 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 26.6306\n",
      "Epoch 23: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 26.7917 - val_loss: 112.7963 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 26.2714\n",
      "Epoch 24: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 26.3177 - val_loss: 99.6067 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 24.4749\n",
      "Epoch 25: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 24.4630 - val_loss: 113.4182 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 21.5222\n",
      "Epoch 26: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 21.5193 - val_loss: 109.0039 - lr: 0.0013\n",
      "Epoch 27/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 20.8993\n",
      "Epoch 27: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.9232 - val_loss: 96.8323 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 20.5611\n",
      "Epoch 28: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 20.5408 - val_loss: 100.4353 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 18.8119\n",
      "Epoch 29: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.8119 - val_loss: 89.3593 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 22.0174\n",
      "Epoch 30: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.0157 - val_loss: 89.2721 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 17.0185\n",
      "Epoch 31: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 17.0161 - val_loss: 89.6741 - lr: 7.7760e-04\n",
      "Epoch 32/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 16.7774\n",
      "Epoch 32: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 16.6856 - val_loss: 112.3234 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 15.1549\n",
      "Epoch 33: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 15.1549 - val_loss: 94.9543 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 15.4318\n",
      "Epoch 34: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 15.4206 - val_loss: 107.1530 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 14.4260\n",
      "Epoch 35: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.4350 - val_loss: 97.0587 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 14.6287\n",
      "Epoch 36: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.5909 - val_loss: 116.5830 - lr: 4.6656e-04\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.0711\n",
      "Epoch 37: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.0651 - val_loss: 111.2449 - lr: 4.6656e-04\n",
      "Epoch 38/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 12.1790\n",
      "Epoch 38: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.1901 - val_loss: 110.9556 - lr: 4.6656e-04\n",
      "Epoch 39/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 12.2395\n",
      "Epoch 39: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 12.2311 - val_loss: 105.0488 - lr: 4.6656e-04\n",
      "Epoch 40/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 12.0820\n",
      "Epoch 40: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 12.1608 - val_loss: 114.3653 - lr: 4.6656e-04\n",
      "Epoch 41/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 12.2414\n",
      "Epoch 41: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.0412 - val_loss: 111.2450 - lr: 2.7994e-04\n",
      "Epoch 42/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.8702\n",
      "Epoch 42: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.8838 - val_loss: 109.5105 - lr: 2.7994e-04\n",
      "Epoch 43/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.3756\n",
      "Epoch 43: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3757 - val_loss: 116.4806 - lr: 2.7994e-04\n",
      "Epoch 44/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 10.8341\n",
      "Epoch 44: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.7967 - val_loss: 108.9812 - lr: 2.7994e-04\n",
      "Epoch 45/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.0101\n",
      "Epoch 45: val_loss did not improve from 65.49402\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.0121 - val_loss: 113.9247 - lr: 2.7994e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 1437.9203\n",
      "Epoch 1: val_loss improved from inf to 210.42017, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 1408.2611 - val_loss: 210.4202 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 111.6859\n",
      "Epoch 2: val_loss improved from 210.42017 to 77.81415, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 109.7370 - val_loss: 77.8141 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 100.8623\n",
      "Epoch 3: val_loss did not improve from 77.81415\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 100.6378 - val_loss: 261.2182 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 86.1908\n",
      "Epoch 4: val_loss improved from 77.81415 to 76.41003, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 86.0378 - val_loss: 76.4100 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 74.1631\n",
      "Epoch 5: val_loss improved from 76.41003 to 64.54371, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 74.3487 - val_loss: 64.5437 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 84.1342\n",
      "Epoch 6: val_loss did not improve from 64.54371\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 84.1361 - val_loss: 97.8301 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 77.3250\n",
      "Epoch 7: val_loss improved from 64.54371 to 60.92245, saving model to best_model_fold_3.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 77.3082 - val_loss: 60.9224 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 64.8694\n",
      "Epoch 8: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 64.8872 - val_loss: 73.2003 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 73.6025\n",
      "Epoch 9: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 72.0709 - val_loss: 68.9489 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 63.5272\n",
      "Epoch 10: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 64.1794 - val_loss: 74.2945 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 61.3371\n",
      "Epoch 11: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 61.1941 - val_loss: 109.5220 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 57.0810\n",
      "Epoch 12: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 57.2546 - val_loss: 164.7571 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 51.8357\n",
      "Epoch 13: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 51.8463 - val_loss: 74.4344 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 49.5986\n",
      "Epoch 14: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 49.3694 - val_loss: 89.0318 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 46.7370\n",
      "Epoch 15: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 46.5317 - val_loss: 100.3384 - lr: 0.0060\n",
      "Epoch 16/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 42.3124\n",
      "Epoch 16: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 42.3124 - val_loss: 168.4952 - lr: 0.0060\n",
      "Epoch 17/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 47.8773\n",
      "Epoch 17: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 47.9473 - val_loss: 135.9995 - lr: 0.0060\n",
      "Epoch 18/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 39.0323\n",
      "Epoch 18: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 39.2093 - val_loss: 117.0165 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 34.6563\n",
      "Epoch 19: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 34.6583 - val_loss: 86.7559 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 33.2123\n",
      "Epoch 20: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 33.1322 - val_loss: 87.3927 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 30.4984\n",
      "Epoch 21: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.5318 - val_loss: 119.8983 - lr: 0.0036\n",
      "Epoch 22/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 29.6907\n",
      "Epoch 22: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.9026 - val_loss: 92.1881 - lr: 0.0036\n",
      "Epoch 23/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 25.1854\n",
      "Epoch 23: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 25.1750 - val_loss: 108.1067 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 24.5268\n",
      "Epoch 24: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 24.4677 - val_loss: 84.2914 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 22.1640\n",
      "Epoch 25: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.2044 - val_loss: 84.0460 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 20.8133\n",
      "Epoch 26: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 20.9190 - val_loss: 103.0574 - lr: 0.0022\n",
      "Epoch 27/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 19.5134\n",
      "Epoch 27: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 19.5216 - val_loss: 87.6757 - lr: 0.0022\n",
      "Epoch 28/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 16.6162\n",
      "Epoch 28: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.6378 - val_loss: 118.9075 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 16.2287\n",
      "Epoch 29: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.3172 - val_loss: 102.2144 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 14.6311\n",
      "Epoch 30: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.6081 - val_loss: 95.2099 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.1025\n",
      "Epoch 31: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.1580 - val_loss: 108.2384 - lr: 0.0013\n",
      "Epoch 32/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 13.8198\n",
      "Epoch 32: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.0629 - val_loss: 101.3828 - lr: 0.0013\n",
      "Epoch 33/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 12.7012\n",
      "Epoch 33: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 12.6789 - val_loss: 97.6724 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.3549\n",
      "Epoch 34: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3224 - val_loss: 103.4651 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 10.8897\n",
      "Epoch 35: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.9953 - val_loss: 113.1605 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.3931\n",
      "Epoch 36: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3493 - val_loss: 108.3803 - lr: 7.7760e-04\n",
      "Epoch 37/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.4701\n",
      "Epoch 37: val_loss did not improve from 60.92245\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.4701 - val_loss: 98.9760 - lr: 7.7760e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 1275.6545\n",
      "Epoch 1: val_loss improved from inf to 83.15632, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 1262.2993 - val_loss: 83.1563 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 97.4131\n",
      "Epoch 2: val_loss improved from 83.15632 to 77.35900, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 97.4214 - val_loss: 77.3590 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 103.4597\n",
      "Epoch 3: val_loss did not improve from 77.35900\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 103.1282 - val_loss: 157.4694 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 88.0944\n",
      "Epoch 4: val_loss improved from 77.35900 to 64.07652, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 87.5572 - val_loss: 64.0765 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 74.6045\n",
      "Epoch 5: val_loss did not improve from 64.07652\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 74.6687 - val_loss: 70.2646 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 76.5398\n",
      "Epoch 6: val_loss did not improve from 64.07652\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 76.5183 - val_loss: 149.2315 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 83.9267\n",
      "Epoch 7: val_loss did not improve from 64.07652\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 83.8526 - val_loss: 81.5902 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 70.6753\n",
      "Epoch 8: val_loss improved from 64.07652 to 62.53762, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 70.6362 - val_loss: 62.5376 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 63.7258\n",
      "Epoch 9: val_loss improved from 62.53762 to 61.11522, saving model to best_model_fold_4.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 64.0678 - val_loss: 61.1152 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 58.9590\n",
      "Epoch 10: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 59.2824 - val_loss: 206.9550 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 63.7748\n",
      "Epoch 11: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 63.7684 - val_loss: 87.8585 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 66.2618\n",
      "Epoch 12: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 66.4135 - val_loss: 155.4270 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 58.8237\n",
      "Epoch 13: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.5789 - val_loss: 101.7459 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 53.8232\n",
      "Epoch 14: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 53.7349 - val_loss: 85.3815 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 44.0939\n",
      "Epoch 15: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 44.2820 - val_loss: 115.1150 - lr: 0.0060\n",
      "Epoch 16/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 44.5499\n",
      "Epoch 16: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 44.5531 - val_loss: 87.8298 - lr: 0.0060\n",
      "Epoch 17/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 44.2597\n",
      "Epoch 17: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 44.2597 - val_loss: 121.7108 - lr: 0.0060\n",
      "Epoch 18/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 40.8362\n",
      "Epoch 18: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 40.5577 - val_loss: 74.1572 - lr: 0.0060\n",
      "Epoch 19/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 38.0684\n",
      "Epoch 19: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 38.0684 - val_loss: 92.5894 - lr: 0.0060\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 30.9253\n",
      "Epoch 20: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 30.8928 - val_loss: 94.0967 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 30.4704\n",
      "Epoch 21: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 30.6852 - val_loss: 79.7043 - lr: 0.0036\n",
      "Epoch 22/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 28.4771\n",
      "Epoch 22: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 28.4771 - val_loss: 73.5608 - lr: 0.0036\n",
      "Epoch 23/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 25.4476\n",
      "Epoch 23: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 25.4476 - val_loss: 76.1006 - lr: 0.0036\n",
      "Epoch 24/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 27.2112\n",
      "Epoch 24: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 27.2164 - val_loss: 83.7873 - lr: 0.0036\n",
      "Epoch 25/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 20.8541\n",
      "Epoch 25: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 20.8259 - val_loss: 95.2775 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 18.9612\n",
      "Epoch 26: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.9414 - val_loss: 109.8075 - lr: 0.0022\n",
      "Epoch 27/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 18.1212\n",
      "Epoch 27: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.2583 - val_loss: 121.8061 - lr: 0.0022\n",
      "Epoch 28/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 16.2880\n",
      "Epoch 28: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 16.2062 - val_loss: 86.5928 - lr: 0.0022\n",
      "Epoch 29/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 15.7397\n",
      "Epoch 29: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 15.7573 - val_loss: 92.6122 - lr: 0.0022\n",
      "Epoch 30/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.7702\n",
      "Epoch 30: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.7997 - val_loss: 101.2042 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 13.0473\n",
      "Epoch 31: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.1669 - val_loss: 91.6821 - lr: 0.0013\n",
      "Epoch 32/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 12.0900\n",
      "Epoch 32: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 12.0300 - val_loss: 99.2051 - lr: 0.0013\n",
      "Epoch 33/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 11.3965\n",
      "Epoch 33: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3998 - val_loss: 98.3349 - lr: 0.0013\n",
      "Epoch 34/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 11.0443\n",
      "Epoch 34: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.0443 - val_loss: 97.7899 - lr: 0.0013\n",
      "Epoch 35/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 10.0796\n",
      "Epoch 35: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.0728 - val_loss: 99.3356 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.5583\n",
      "Epoch 36: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.6011 - val_loss: 98.6280 - lr: 7.7760e-04\n",
      "Epoch 37/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 9.8721\n",
      "Epoch 37: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 9.8080 - val_loss: 108.1271 - lr: 7.7760e-04\n",
      "Epoch 38/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 9.2177\n",
      "Epoch 38: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.2154 - val_loss: 97.1823 - lr: 7.7760e-04\n",
      "Epoch 39/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 8.7985\n",
      "Epoch 39: val_loss did not improve from 61.11522\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.7604 - val_loss: 101.1801 - lr: 7.7760e-04\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 4114.9731\n",
      "Epoch 1: val_loss improved from inf to 99.31980, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 3933.7146 - val_loss: 99.3198 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 122.5915\n",
      "Epoch 2: val_loss did not improve from 99.31980\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 120.3629 - val_loss: 131.0061 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 90.9695\n",
      "Epoch 3: val_loss did not improve from 99.31980\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 90.1589 - val_loss: 167.5674 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 83.5039\n",
      "Epoch 4: val_loss improved from 99.31980 to 90.42136, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 82.7965 - val_loss: 90.4214 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 77.1840\n",
      "Epoch 5: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 76.4968 - val_loss: 97.2076 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 71.5359\n",
      "Epoch 6: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 71.6666 - val_loss: 95.1972 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 64.6833\n",
      "Epoch 7: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 65.1281 - val_loss: 98.9088 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 60.4150\n",
      "Epoch 8: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 60.4089 - val_loss: 106.6410 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 57.9713\n",
      "Epoch 9: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 57.9713 - val_loss: 128.1450 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 52.3977\n",
      "Epoch 10: val_loss did not improve from 90.42136\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 52.2121 - val_loss: 90.5939 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 50.4598\n",
      "Epoch 11: val_loss improved from 90.42136 to 75.14285, saving model to best_model_fold_5.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 50.5220 - val_loss: 75.1429 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 54.4543\n",
      "Epoch 12: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 53.8832 - val_loss: 90.5199 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 46.6332\n",
      "Epoch 13: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 46.4989 - val_loss: 77.1249 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 45.4962\n",
      "Epoch 14: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 45.3025 - val_loss: 146.8557 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 42.8779\n",
      "Epoch 15: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 42.8888 - val_loss: 95.8484 - lr: 0.0060\n",
      "Epoch 16/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 39.5941\n",
      "Epoch 16: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 41.0218 - val_loss: 123.3461 - lr: 0.0060\n",
      "Epoch 17/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 35.3000\n",
      "Epoch 17: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 35.7631 - val_loss: 106.1790 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 33.6324\n",
      "Epoch 18: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 33.6440 - val_loss: 87.7793 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 30.1311\n",
      "Epoch 19: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 30.1311 - val_loss: 96.1865 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 29.0586\n",
      "Epoch 20: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.3303 - val_loss: 139.5078 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 29.0570\n",
      "Epoch 21: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.0944 - val_loss: 117.8872 - lr: 0.0036\n",
      "Epoch 22/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 23.4077\n",
      "Epoch 22: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.4420 - val_loss: 89.7477 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 24.4723\n",
      "Epoch 23: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.2045 - val_loss: 108.9392 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 20.6711\n",
      "Epoch 24: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 20.6380 - val_loss: 121.2272 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 20.6336\n",
      "Epoch 25: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 20.7184 - val_loss: 94.6275 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 18.8741\n",
      "Epoch 26: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 18.8957 - val_loss: 123.6431 - lr: 0.0022\n",
      "Epoch 27/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 16.3517\n",
      "Epoch 27: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.4186 - val_loss: 105.4899 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 15.3993\n",
      "Epoch 28: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 15.5150 - val_loss: 105.1773 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.8612\n",
      "Epoch 29: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.8521 - val_loss: 106.6871 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 13.3751\n",
      "Epoch 30: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 13.4838 - val_loss: 119.2650 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.8935\n",
      "Epoch 31: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 12.9223 - val_loss: 96.1319 - lr: 0.0013\n",
      "Epoch 32/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.0653\n",
      "Epoch 32: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 12.1104 - val_loss: 118.2752 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.5918\n",
      "Epoch 33: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.5908 - val_loss: 119.1300 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 11.3305\n",
      "Epoch 34: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3164 - val_loss: 126.4629 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 11.2417\n",
      "Epoch 35: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.1086 - val_loss: 109.5291 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 10.4802\n",
      "Epoch 36: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.4195 - val_loss: 127.1356 - lr: 7.7760e-04\n",
      "Epoch 37/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 9.7431\n",
      "Epoch 37: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.7701 - val_loss: 112.5282 - lr: 4.6656e-04\n",
      "Epoch 38/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 9.5574\n",
      "Epoch 38: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.5707 - val_loss: 115.5503 - lr: 4.6656e-04\n",
      "Epoch 39/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 9.3300\n",
      "Epoch 39: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.3300 - val_loss: 114.8197 - lr: 4.6656e-04\n",
      "Epoch 40/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 8.9993\n",
      "Epoch 40: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.0567 - val_loss: 110.5942 - lr: 4.6656e-04\n",
      "Epoch 41/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 9.1823\n",
      "Epoch 41: val_loss did not improve from 75.14285\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 9.1056 - val_loss: 114.1354 - lr: 4.6656e-04\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 934.5420 \n",
      "Epoch 1: val_loss improved from inf to 105.42181, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 934.5420 - val_loss: 105.4218 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 111.4225\n",
      "Epoch 2: val_loss improved from 105.42181 to 68.89869, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 111.0307 - val_loss: 68.8987 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 92.7866\n",
      "Epoch 3: val_loss did not improve from 68.89869\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 93.5295 - val_loss: 445.4404 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 123.3693\n",
      "Epoch 4: val_loss did not improve from 68.89869\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 123.0879 - val_loss: 87.7210 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 74.6704\n",
      "Epoch 5: val_loss improved from 68.89869 to 67.88601, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 74.6704 - val_loss: 67.8860 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 70.1151\n",
      "Epoch 6: val_loss did not improve from 67.88601\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 69.8661 - val_loss: 116.9800 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 73.0506\n",
      "Epoch 7: val_loss did not improve from 67.88601\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 72.2937 - val_loss: 95.7904 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 63.6655\n",
      "Epoch 8: val_loss improved from 67.88601 to 59.28606, saving model to best_model_fold_6.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 63.6655 - val_loss: 59.2861 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 57.9384\n",
      "Epoch 9: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 57.8820 - val_loss: 68.7982 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 59.4742\n",
      "Epoch 10: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 59.4823 - val_loss: 65.5672 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 54.0091\n",
      "Epoch 11: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 53.8930 - val_loss: 208.8010 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 60.2813\n",
      "Epoch 12: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 59.6303 - val_loss: 82.4214 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 53.4956\n",
      "Epoch 13: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 53.2287 - val_loss: 96.4202 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 37.5891\n",
      "Epoch 14: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 37.5125 - val_loss: 81.3688 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 31.7025\n",
      "Epoch 15: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 32.4266 - val_loss: 70.8288 - lr: 0.0060\n",
      "Epoch 16/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 30.9265\n",
      "Epoch 16: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 30.7609 - val_loss: 109.1072 - lr: 0.0060\n",
      "Epoch 17/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 26.2969\n",
      "Epoch 17: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.6483 - val_loss: 93.7237 - lr: 0.0060\n",
      "Epoch 18/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 29.1832\n",
      "Epoch 18: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 29.2237 - val_loss: 76.0322 - lr: 0.0060\n",
      "Epoch 19/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 20.2192\n",
      "Epoch 19: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 19.9877 - val_loss: 99.4788 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 17.0192\n",
      "Epoch 20: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 17.0192 - val_loss: 94.3030 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 16.0919\n",
      "Epoch 21: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.0888 - val_loss: 92.7846 - lr: 0.0036\n",
      "Epoch 22/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 13.8598\n",
      "Epoch 22: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 14.0076 - val_loss: 86.2822 - lr: 0.0036\n",
      "Epoch 23/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 13.2619\n",
      "Epoch 23: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 13.2616 - val_loss: 104.1620 - lr: 0.0036\n",
      "Epoch 24/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 11.0773\n",
      "Epoch 24: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.0703 - val_loss: 88.8647 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 9.8341\n",
      "Epoch 25: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.8289 - val_loss: 101.9100 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 9.7841\n",
      "Epoch 26: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 9.8106 - val_loss: 94.7905 - lr: 0.0022\n",
      "Epoch 27/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 8.8017\n",
      "Epoch 27: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 8.9425 - val_loss: 82.4303 - lr: 0.0022\n",
      "Epoch 28/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 9.0088\n",
      "Epoch 28: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 9.0304 - val_loss: 90.7223 - lr: 0.0022\n",
      "Epoch 29/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 7.9088\n",
      "Epoch 29: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 7.8174 - val_loss: 99.3485 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 7.5327\n",
      "Epoch 30: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 7.5869 - val_loss: 93.0659 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 7.5519\n",
      "Epoch 31: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.5249 - val_loss: 92.0469 - lr: 0.0013\n",
      "Epoch 32/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 7.5715\n",
      "Epoch 32: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 7.5486 - val_loss: 98.5741 - lr: 0.0013\n",
      "Epoch 33/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.1386\n",
      "Epoch 33: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.2625 - val_loss: 88.3223 - lr: 0.0013\n",
      "Epoch 34/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 6.9062\n",
      "Epoch 34: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 6.8787 - val_loss: 94.6992 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 6.4497\n",
      "Epoch 35: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 6.4215 - val_loss: 95.4516 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 6.4165\n",
      "Epoch 36: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.4165 - val_loss: 95.9328 - lr: 7.7760e-04\n",
      "Epoch 37/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 6.1018\n",
      "Epoch 37: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 6.1018 - val_loss: 94.4194 - lr: 7.7760e-04\n",
      "Epoch 38/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 5.9346\n",
      "Epoch 38: val_loss did not improve from 59.28606\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 5.9397 - val_loss: 94.1366 - lr: 7.7760e-04\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "Epoch 1/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 1489.6014\n",
      "Epoch 1: val_loss improved from inf to 123.17969, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 1397.7330 - val_loss: 123.1797 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 101.6216\n",
      "Epoch 2: val_loss improved from 123.17969 to 121.02379, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 101.2017 - val_loss: 121.0238 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 92.9943\n",
      "Epoch 3: val_loss improved from 121.02379 to 103.53310, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 92.9943 - val_loss: 103.5331 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 81.8384\n",
      "Epoch 4: val_loss improved from 103.53310 to 64.09393, saving model to best_model_fold_7.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 81.8819 - val_loss: 64.0939 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 75.2451\n",
      "Epoch 5: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 1s 5ms/step - loss: 75.0531 - val_loss: 65.4653 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 69.5785\n",
      "Epoch 6: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 69.8109 - val_loss: 87.5140 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 90.4593\n",
      "Epoch 7: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 90.2619 - val_loss: 102.1247 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 67.8748\n",
      "Epoch 8: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 67.5003 - val_loss: 122.5645 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 63.3174\n",
      "Epoch 9: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 63.1439 - val_loss: 98.0459 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 56.0882\n",
      "Epoch 10: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 56.0882 - val_loss: 71.3328 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 53.8238\n",
      "Epoch 11: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 53.7984 - val_loss: 80.3057 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 48.7860\n",
      "Epoch 12: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 48.9632 - val_loss: 87.9154 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 49.9726\n",
      "Epoch 13: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 50.0941 - val_loss: 104.2519 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 44.4910\n",
      "Epoch 14: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 44.4910 - val_loss: 73.8071 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 40.1189\n",
      "Epoch 15: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 40.1189 - val_loss: 75.4575 - lr: 0.0036\n",
      "Epoch 16/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 39.7532\n",
      "Epoch 16: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 39.7532 - val_loss: 78.7648 - lr: 0.0036\n",
      "Epoch 17/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 36.4388\n",
      "Epoch 17: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 36.5068 - val_loss: 70.1112 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 36.4040\n",
      "Epoch 18: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 36.5191 - val_loss: 125.6327 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 33.3719\n",
      "Epoch 19: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 33.2100 - val_loss: 109.5052 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 29.6705\n",
      "Epoch 20: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 29.9051 - val_loss: 133.9852 - lr: 0.0022\n",
      "Epoch 21/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 28.1466\n",
      "Epoch 21: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 28.1367 - val_loss: 108.6945 - lr: 0.0022\n",
      "Epoch 22/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 26.0193\n",
      "Epoch 22: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.9887 - val_loss: 94.6418 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 23.1323\n",
      "Epoch 23: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 23.2242 - val_loss: 105.3868 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 21.6297\n",
      "Epoch 24: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 21.5428 - val_loss: 104.8449 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 18.1986\n",
      "Epoch 25: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.1057 - val_loss: 98.9251 - lr: 0.0013\n",
      "Epoch 26/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.7387\n",
      "Epoch 26: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.6611 - val_loss: 112.6297 - lr: 0.0013\n",
      "Epoch 27/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 16.2103\n",
      "Epoch 27: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 16.0708 - val_loss: 96.8986 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 14.9854\n",
      "Epoch 28: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.9202 - val_loss: 100.9236 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 14.8714\n",
      "Epoch 29: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 15.1765 - val_loss: 96.4583 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 13.0092\n",
      "Epoch 30: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.8761 - val_loss: 111.7867 - lr: 7.7760e-04\n",
      "Epoch 31/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.9468\n",
      "Epoch 31: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.9569 - val_loss: 103.5420 - lr: 7.7760e-04\n",
      "Epoch 32/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 11.4507\n",
      "Epoch 32: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 11.5685 - val_loss: 99.9881 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 11.2898\n",
      "Epoch 33: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.3785 - val_loss: 111.1870 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 10.7560\n",
      "Epoch 34: val_loss did not improve from 64.09393\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.6706 - val_loss: 103.3919 - lr: 7.7760e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 6792.8691\n",
      "Epoch 1: val_loss improved from inf to 137.68433, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 6792.8691 - val_loss: 137.6843 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 112.8442\n",
      "Epoch 2: val_loss improved from 137.68433 to 137.11765, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 112.8442 - val_loss: 137.1176 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 94.9242\n",
      "Epoch 3: val_loss improved from 137.11765 to 73.97956, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 96.2611 - val_loss: 73.9796 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 83.1448\n",
      "Epoch 4: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 82.6491 - val_loss: 171.2097 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 76.2639\n",
      "Epoch 5: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 76.1953 - val_loss: 159.2572 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 70.4389\n",
      "Epoch 6: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 70.2513 - val_loss: 86.2934 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 65.5724\n",
      "Epoch 7: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 65.5325 - val_loss: 74.6399 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 69.1586\n",
      "Epoch 8: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 77.9656 - val_loss: 127.6894 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 72.4762\n",
      "Epoch 9: val_loss did not improve from 73.97956\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 72.4762 - val_loss: 110.1534 - lr: 0.0060\n",
      "Epoch 10/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 66.8382\n",
      "Epoch 10: val_loss improved from 73.97956 to 72.52817, saving model to best_model_fold_8.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 66.8485 - val_loss: 72.5282 - lr: 0.0060\n",
      "Epoch 11/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 59.0243\n",
      "Epoch 11: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 58.8122 - val_loss: 77.8858 - lr: 0.0060\n",
      "Epoch 12/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 57.0120\n",
      "Epoch 12: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 57.2972 - val_loss: 83.4839 - lr: 0.0060\n",
      "Epoch 13/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 54.3035\n",
      "Epoch 13: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 55.0666 - val_loss: 81.1496 - lr: 0.0060\n",
      "Epoch 14/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 52.6099\n",
      "Epoch 14: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 52.6201 - val_loss: 86.1465 - lr: 0.0060\n",
      "Epoch 15/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 48.9092\n",
      "Epoch 15: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 48.8980 - val_loss: 94.3267 - lr: 0.0060\n",
      "Epoch 16/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 46.3499\n",
      "Epoch 16: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 46.1698 - val_loss: 122.4330 - lr: 0.0036\n",
      "Epoch 17/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 45.0813\n",
      "Epoch 17: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 45.1381 - val_loss: 87.9958 - lr: 0.0036\n",
      "Epoch 18/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 43.3782\n",
      "Epoch 18: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 43.3954 - val_loss: 91.9094 - lr: 0.0036\n",
      "Epoch 19/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 42.8509\n",
      "Epoch 19: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 42.8055 - val_loss: 83.9116 - lr: 0.0036\n",
      "Epoch 20/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 39.3907\n",
      "Epoch 20: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 39.3907 - val_loss: 90.3362 - lr: 0.0036\n",
      "Epoch 21/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 36.9926\n",
      "Epoch 21: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 36.9796 - val_loss: 90.3719 - lr: 0.0022\n",
      "Epoch 22/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 34.9803\n",
      "Epoch 22: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 36.4323 - val_loss: 125.6685 - lr: 0.0022\n",
      "Epoch 23/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 36.8022\n",
      "Epoch 23: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 36.9717 - val_loss: 108.3816 - lr: 0.0022\n",
      "Epoch 24/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 35.1521\n",
      "Epoch 24: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 34.8298 - val_loss: 108.5460 - lr: 0.0022\n",
      "Epoch 25/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 32.5054\n",
      "Epoch 25: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 32.4822 - val_loss: 91.8454 - lr: 0.0022\n",
      "Epoch 26/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 30.7149\n",
      "Epoch 26: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 30.6757 - val_loss: 100.6522 - lr: 0.0013\n",
      "Epoch 27/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 30.8196\n",
      "Epoch 27: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 30.8669 - val_loss: 105.0454 - lr: 0.0013\n",
      "Epoch 28/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 27.0972\n",
      "Epoch 28: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 27.3235 - val_loss: 94.5784 - lr: 0.0013\n",
      "Epoch 29/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 28.4164\n",
      "Epoch 29: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 28.4308 - val_loss: 100.8998 - lr: 0.0013\n",
      "Epoch 30/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 26.3770\n",
      "Epoch 30: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 26.6766 - val_loss: 104.9424 - lr: 0.0013\n",
      "Epoch 31/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 25.9850\n",
      "Epoch 31: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 26.0532 - val_loss: 110.9762 - lr: 7.7760e-04\n",
      "Epoch 32/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 25.6221\n",
      "Epoch 32: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 25.5922 - val_loss: 103.8366 - lr: 7.7760e-04\n",
      "Epoch 33/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 25.9557\n",
      "Epoch 33: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 25.8115 - val_loss: 97.3410 - lr: 7.7760e-04\n",
      "Epoch 34/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 24.2249\n",
      "Epoch 34: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 24.0554 - val_loss: 92.1856 - lr: 7.7760e-04\n",
      "Epoch 35/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 23.1825\n",
      "Epoch 35: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 23.2764 - val_loss: 111.4201 - lr: 7.7760e-04\n",
      "Epoch 36/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 22.9318\n",
      "Epoch 36: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.7355 - val_loss: 109.9184 - lr: 4.6656e-04\n",
      "Epoch 37/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 22.6846\n",
      "Epoch 37: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 22.9475 - val_loss: 94.8828 - lr: 4.6656e-04\n",
      "Epoch 38/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 23.1058\n",
      "Epoch 38: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 23.1058 - val_loss: 106.9464 - lr: 4.6656e-04\n",
      "Epoch 39/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 22.5366\n",
      "Epoch 39: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 22.5038 - val_loss: 103.9471 - lr: 4.6656e-04\n",
      "Epoch 40/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 21.1876\n",
      "Epoch 40: val_loss did not improve from 72.52817\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 21.1878 - val_loss: 105.8290 - lr: 4.6656e-04\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "Epoch 1/500\n",
      "79/91 [=========================>....] - ETA: 0s - loss: 2465.1829\n",
      "Epoch 1: val_loss improved from inf to 106.71918, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 2176.3806 - val_loss: 106.7192 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 111.7328\n",
      "Epoch 2: val_loss did not improve from 106.71918\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 112.1586 - val_loss: 128.9540 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 108.5361\n",
      "Epoch 3: val_loss did not improve from 106.71918\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 108.6297 - val_loss: 175.2453 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 120.0689\n",
      "Epoch 4: val_loss did not improve from 106.71918\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 119.4492 - val_loss: 205.7004 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 82.4805\n",
      "Epoch 5: val_loss did not improve from 106.71918\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 82.0944 - val_loss: 119.1654 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 76.9641\n",
      "Epoch 6: val_loss improved from 106.71918 to 95.83790, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 76.2629 - val_loss: 95.8379 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 67.2958\n",
      "Epoch 7: val_loss improved from 95.83790 to 75.45052, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 67.8081 - val_loss: 75.4505 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 67.5622\n",
      "Epoch 8: val_loss did not improve from 75.45052\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 67.8155 - val_loss: 106.8152 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 98.8577 \n",
      "Epoch 9: val_loss did not improve from 75.45052\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 96.6403 - val_loss: 105.2291 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 62.4349\n",
      "Epoch 10: val_loss did not improve from 75.45052\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 63.1983 - val_loss: 119.3285 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "84/91 [==========================>...] - ETA: 0s - loss: 60.7466\n",
      "Epoch 11: val_loss did not improve from 75.45052\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 60.7397 - val_loss: 123.7151 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 58.8423\n",
      "Epoch 12: val_loss improved from 75.45052 to 65.91640, saving model to best_model_fold_9.h5\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.8087 - val_loss: 65.9164 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 54.5686\n",
      "Epoch 13: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 54.4843 - val_loss: 102.0325 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 51.5507\n",
      "Epoch 14: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 51.4911 - val_loss: 90.5298 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 50.4906\n",
      "Epoch 15: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 49.9670 - val_loss: 88.9438 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 44.5388\n",
      "Epoch 16: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 45.7308 - val_loss: 91.0805 - lr: 0.0100\n",
      "Epoch 17/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 57.6687\n",
      "Epoch 17: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 58.0778 - val_loss: 71.2786 - lr: 0.0100\n",
      "Epoch 18/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 41.5961\n",
      "Epoch 18: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 41.4206 - val_loss: 111.1855 - lr: 0.0060\n",
      "Epoch 19/500\n",
      "78/91 [========================>.....] - ETA: 0s - loss: 33.9660\n",
      "Epoch 19: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 34.5000 - val_loss: 107.2374 - lr: 0.0060\n",
      "Epoch 20/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 32.5324\n",
      "Epoch 20: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 32.3925 - val_loss: 124.3725 - lr: 0.0060\n",
      "Epoch 21/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 27.6769\n",
      "Epoch 21: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 27.8391 - val_loss: 90.8422 - lr: 0.0060\n",
      "Epoch 22/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 25.3786\n",
      "Epoch 22: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 24.9992 - val_loss: 98.6495 - lr: 0.0060\n",
      "Epoch 23/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 19.8512\n",
      "Epoch 23: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 19.8601 - val_loss: 129.4379 - lr: 0.0036\n",
      "Epoch 24/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 18.3373\n",
      "Epoch 24: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 18.4075 - val_loss: 123.8419 - lr: 0.0036\n",
      "Epoch 25/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 18.6870\n",
      "Epoch 25: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 18.6870 - val_loss: 106.2821 - lr: 0.0036\n",
      "Epoch 26/500\n",
      "85/91 [===========================>..] - ETA: 0s - loss: 16.1733\n",
      "Epoch 26: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 16.0986 - val_loss: 103.0610 - lr: 0.0036\n",
      "Epoch 27/500\n",
      "90/91 [============================>.] - ETA: 0s - loss: 14.2201\n",
      "Epoch 27: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 14.2261 - val_loss: 99.1975 - lr: 0.0036\n",
      "Epoch 28/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 12.9206\n",
      "Epoch 28: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 12.8990 - val_loss: 108.4887 - lr: 0.0022\n",
      "Epoch 29/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 11.4898\n",
      "Epoch 29: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 11.4650 - val_loss: 107.8959 - lr: 0.0022\n",
      "Epoch 30/500\n",
      "89/91 [============================>.] - ETA: 0s - loss: 10.8981\n",
      "Epoch 30: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 10.9473 - val_loss: 100.8670 - lr: 0.0022\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.7340\n",
      "Epoch 31: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 10.7340 - val_loss: 117.8362 - lr: 0.0022\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - ETA: 0s - loss: 10.0147\n",
      "Epoch 32: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 10.0147 - val_loss: 117.5553 - lr: 0.0022\n",
      "Epoch 33/500\n",
      "88/91 [============================>.] - ETA: 0s - loss: 8.9808\n",
      "Epoch 33: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.9782 - val_loss: 106.8376 - lr: 0.0013\n",
      "Epoch 34/500\n",
      "83/91 [==========================>...] - ETA: 0s - loss: 9.3182\n",
      "Epoch 34: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 9.1663 - val_loss: 112.3855 - lr: 0.0013\n",
      "Epoch 35/500\n",
      "81/91 [=========================>....] - ETA: 0s - loss: 7.9299\n",
      "Epoch 35: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.9917 - val_loss: 93.9845 - lr: 0.0013\n",
      "Epoch 36/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 7.8695\n",
      "Epoch 36: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 7.8861 - val_loss: 105.8035 - lr: 0.0013\n",
      "Epoch 37/500\n",
      "80/91 [=========================>....] - ETA: 0s - loss: 7.8519\n",
      "Epoch 37: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.8849 - val_loss: 100.3036 - lr: 0.0013\n",
      "Epoch 38/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 8.0335\n",
      "Epoch 38: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.0204 - val_loss: 97.6800 - lr: 7.7760e-04\n",
      "Epoch 39/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 8.1100\n",
      "Epoch 39: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 8.1632 - val_loss: 90.6061 - lr: 7.7760e-04\n",
      "Epoch 40/500\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 7.8904\n",
      "Epoch 40: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 7.8404 - val_loss: 96.8938 - lr: 7.7760e-04\n",
      "Epoch 41/500\n",
      "86/91 [===========================>..] - ETA: 0s - loss: 7.1527\n",
      "Epoch 41: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 7.2711 - val_loss: 104.6936 - lr: 7.7760e-04\n",
      "Epoch 42/500\n",
      "82/91 [==========================>...] - ETA: 0s - loss: 7.2064\n",
      "Epoch 42: val_loss did not improve from 65.91640\n",
      "91/91 [==============================] - 0s 4ms/step - loss: 7.4233 - val_loss: 98.4041 - lr: 7.7760e-04\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "Mean MSE: 65.59758937323588\n",
      "Mean R-squared: 0.8228137191136421\n",
      "Average prop10: 0.7937460998439937\n",
      "Average prop5: 0.5498678822152886\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "# Function to calculate R-squared and MSE\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X).flatten()  # Flatten to make it 1-dimensional\n",
    "    error = y_pred - y\n",
    "    prop10 = np.sum(np.abs(error) <= 10) / len(y)\n",
    "    prop5 = np.sum(np.abs(error) <= 5) / len(y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    return mse, r_squared, prop10, prop5\n",
    "\n",
    "# Extract the expression values for the horvath's probes \n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# Perform cross-validation for transformer+CNN\n",
    "def cnn_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    \n",
    "    # MaxPooling Layer\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # LayerNormalization\n",
    "    x = LayerNormalization(epsilon=1e-10)(x)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=30,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.6,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "cnn_horvath_mse_list = []\n",
    "cnn_horvath_r_squared_list = []\n",
    "cnn_horvath_prop10_list = []\n",
    "cnn_horvath_prop5_list = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create CNN model with dropout\n",
    "    model_cnn = cnn_model_with_dropout((X_train.shape[1], 1), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'best_model_fold_{fold_idx}.h5',  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_cnn.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks including ModelCheckpoint\n",
    "    model_cnn.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # After training, you can load the best model using the following:\n",
    "    best_model_cnn = load_model(f'best_model_fold_{fold_idx}.h5')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared, prop10, prop5 = evaluate_model(best_model_cnn, X_test_scaled, y_test)\n",
    "    \n",
    "    cnn_horvath_mse_list.append(mse)\n",
    "    cnn_horvath_r_squared_list.append(r_squared)\n",
    "    cnn_horvath_prop10_list.append(prop10)\n",
    "    cnn_horvath_prop5_list.append(prop5)\n",
    "    \n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f'best_model_fold_{fold_idx}.h5')\n",
    "    \n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(cnn_horvath_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(cnn_horvath_r_squared_list))\n",
    "print(\"Average prop10:\", np.mean(cnn_horvath_prop10_list))\n",
    "print(\"Average prop5:\", np.mean(cnn_horvath_prop5_list))\n",
    "\n",
    "CV_results2['cnn-horvath'] = {'mse':cnn_horvath_mse_list,\n",
    "                               'r_squared': cnn_horvath_r_squared_list,\n",
    "                               'prop10': cnn_horvath_prop10_list,\n",
    "                               'prop5': cnn_horvath_prop5_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    return mse, r_squared\n",
    "\n",
    "# Assuming combined_pivoted_samples_2_07_resetindex_clean and combined_phenodata_clean are defined\n",
    "\n",
    "# Extract the expression values for the probes in filtered_probe_ids_07\n",
    "#X = combined_pivoted_samples_2_07_resetindex_clean.T\n",
    "# Extract the expression values for the probes in horvath's CpGs\n",
    "X = combined_pivoted_samples_2_horvath_clean.T\n",
    "\n",
    "# Impute missing values (NaN) by KNNimputer for Expression\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Load the age values for the dataset\n",
    "y = combined_phenodata_clean['Age']\n",
    "\n",
    "# Convert the target variable to a numeric type\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# Perform cross-validation for transformer+CNN\n",
    "def transformer_cnn_model_with_dropout(input_shape, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    \n",
    "    # MaxPooling Layer\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Dropout Layer\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # MultiHeadAttention Layer\n",
    "    x = MultiHeadAttention(num_heads=10, key_dim=10)(x, x)\n",
    "    \n",
    "    # LayerNormalization\n",
    "    x = LayerNormalization(epsilon=1e-10)(x)\n",
    "    \n",
    "    # MultiHeadAttention Layer\n",
    "    x = MultiHeadAttention(num_heads=10, key_dim=10)(x, x)\n",
    "    \n",
    "    # LayerNormalization\n",
    "    x = LayerNormalization(epsilon=1e-10)(x)\n",
    "    \n",
    "    # MultiHeadAttention Layer\n",
    "    x = MultiHeadAttention(num_heads=10, key_dim=10)(x, x)\n",
    "    \n",
    "    # LayerNormalization\n",
    "    x = LayerNormalization(epsilon=1e-10)(x)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense Layers with Dropout\n",
    "    #x = Dense(units=512, activation='relu')(x)\n",
    "    #x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    patience=30,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Define ReduceLROnPlateau callback for adaptive learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Use validation loss as the metric to monitor\n",
    "    factor=0.6,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
    "    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-7           # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "transformer_cnn_mse_list = []\n",
    "transformer_cnn_r_squared_list = []\n",
    "\n",
    "'''\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "'''\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Standardize the input features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create Transformer + CNN model with dropout\n",
    "    model_transformer_cnn = transformer_cnn_model_with_dropout((X_train.shape[1], 1), dropout_rate=0.05)\n",
    "\n",
    "    # Define ModelCheckpoint callback for each fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'best_model_fold_{fold_idx}.h5',  # Unique filename for each fold\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile the model with an initial learning rate\n",
    "    initial_learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "    model_transformer_cnn.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model with callbacks\n",
    "    model_transformer_cnn.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint]  # Use EarlyStopping, ReduceLROnPlateau, and checkpoint\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse, r_squared = evaluate_model(model_transformer_cnn, X_test_scaled, y_test)\n",
    "    \n",
    "    transformer_cnn_mse_list.append(mse)\n",
    "    transformer_cnn_r_squared_list.append(r_squared)\n",
    "    \n",
    "    # Optionally: Remove the temporary file to save disk space\n",
    "    os.remove(f'best_model_fold_{fold_idx}.h5')\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", np.mean(transformer_cnn_mse_list))\n",
    "print(\"Mean R-squared:\", np.mean(transformer_cnn_r_squared_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
