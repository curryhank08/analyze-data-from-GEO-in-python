{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Building a environment to install necessary libraries by Anaconda is recommended."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# conda install -c conda-forge geoparse\n","# conda install -c anaconda scikit-learn\n","# conda install -c anaconda statsmodels\n","# conda install -c anaconda pandas\n","# conda install -c anaconda numpy\n","# conda install -c anaconda requests\n","# conda install -c conda-forge hdbscan\n","# conda install -c conda-forge tensorflow"]},{"cell_type":"markdown","metadata":{},"source":["### Importing necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1693676074212,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"eYKEmFcP5sqw"},"outputs":[],"source":["#Importing a series of libraries used throughout the pipeline\n","import GEOparse\n","import pandas as pd\n","import numpy as np\n","import os\n","import json\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import quantile_transform\n","from sklearn.decomposition import PCA\n","import requests\n","# os.chdir('../Data')\n","# os.chdir('../Scripts')\n","# from microarray_analysis import *"]},{"cell_type":"markdown","metadata":{"id":"WmymPt6v5sqy"},"source":["### Download data from NCBI GEO based on the GEO accession ID"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":493,"status":"ok","timestamp":1693676346647,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"yyjyF9Hh5sq6"},"outputs":[],"source":["#The series accession id for the study you are analyzing\n","geo_accession_id = \"GSE40279\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Srl9dbnp5sq7","outputId":"8c1fb8c2-0611-47c1-e15e-a76c878da79a"},"outputs":[],"source":["#Parse the GEO data using the Accession ID\n","gse = GEOparse.get_GEO(geo=geo_accession_id, destdir=\"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import GEOparse"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Parse the GEO data using the Accession ID\n","gse30870 = GEOparse.get_GEO(geo=\"GSE30870\", destdir=\"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Parse the GEO data using the Accession ID\n","gse50660 = GEOparse.get_GEO(geo=\"GSE50660\", destdir=\"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Parse the GEO data using the Accession ID\n","gse56105 = GEOparse.get_GEO(geo=\"GSE56105\", destdir=\"./\")"]},{"cell_type":"markdown","metadata":{},"source":["### Save the downloaded data by pickle"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Save the gse object to a local file\n","with open('gse_data.pickle', 'wb') as file:\n","    pickle.dump(gse, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Save the gse object to a local file\n","with open('gse30870_data.pickle', 'wb') as file:\n","    pickle.dump(gse30870, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Save the gse object to a local file\n","with open('gse50660_data.pickle', 'wb') as file:\n","    pickle.dump(gse50660, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Save the gse object to a local file\n","with open('gse56105_data.pickle', 'wb') as file:\n","    pickle.dump(gse56105, file)"]},{"cell_type":"markdown","metadata":{},"source":["### Load the data from saved data by pickle next time"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the gse object from the local file\n","with open('gse_data.pickle', 'rb') as file:\n","    gse = pickle.load(file)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the gse object from the local file\n","with open('gse30870_data.pickle', 'rb') as file:\n","    gse30870 = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the gse object from the local file\n","with open('gse50660_data.pickle', 'rb') as file:\n","    gse50660 = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the gse object from the local file\n","with open('gse56105_data.pickle', 'rb') as file:\n","    gse56105 = pickle.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["### Extract expression value (beta value) for each probes corresponding to each samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"elapsed":1301,"status":"ok","timestamp":1693676296602,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"2WH6VNda-Sr9","outputId":"6af1af34-f6a7-4079-c201-a19f8cc29e06"},"outputs":[],"source":["#Visualization of expression matrix\n","pivoted_samples_40279 = gse.pivot_samples('VALUE')\n","pivoted_samples_40279.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(pivoted_samples_40279)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Visualization of expression matrix\n","pivoted_samples_30870 = gse30870.pivot_samples('VALUE')\n","pivoted_samples_30870.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Visualization of expression matrix\n","pivoted_samples_50660 = gse50660.pivot_samples('VALUE')\n","pivoted_samples_50660.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Visualization of expression matrix\n","pivoted_samples_56105 = gse56105.pivot_samples('VALUE')\n","pivoted_samples_56105.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### Save pivoted_samples by pickle"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the pivoted_samples\n","import pickle\n","with open('pivoted_data_40279.pickle', 'wb') as file:\n","    pickle.dump(pivoted_samples_40279, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the pivoted_samples\n","import pickle\n","with open('pivoted_data_30870.pickle', 'wb') as file:\n","    pickle.dump(pivoted_samples_30870, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the pivoted_samples\n","import pickle\n","with open('pivoted_data_50660.pickle', 'wb') as file:\n","    pickle.dump(pivoted_samples_50660, file)"]},{"cell_type":"markdown","metadata":{},"source":["#### Load pivoted_samples by pickle"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the pivoted_data_40279 object from the local file\n","with open('pivoted_data_40279.pickle', 'rb') as file:\n","    pivoted_samples_40279 = pickle.load(file)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the pivoted_data_30870 object from the local file\n","with open('pivoted_data_30870.pickle', 'rb') as file:\n","    pivoted_samples_30870 = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the pivoted_data_50660 object from the local file\n","with open('pivoted_data_50660.pickle', 'rb') as file:\n","    pivoted_samples_50660 = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_40279.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(pivoted_samples_40279)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_30870.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_30870"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_50660"]},{"cell_type":"markdown","metadata":{},"source":["#### Check information of this data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1693676326354,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"LKMu9KtP5sq8","outputId":"d1a18fb3-bad9-46f4-abd3-6d62ec429201"},"outputs":[],"source":["#Determine the total amount of probes used in the study\n","pivoted_samples_average = pivoted_samples.median(axis=1)\n","print(\"Number of probes before filtering: \", len(pivoted_samples_average))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(type(pivoted_samples))\n","print()\n","print(pivoted_samples.median(axis=0))\n","print()\n","print(pivoted_samples.median(axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples.min(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples.max(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metadata = gse.metadata\n","print(metadata)\n","print(metadata.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metadata_items = metadata.items()\n","keys = list(metadata.keys())\n","values = list(metadata.values())\n","\n","metadata_df = pd.DataFrame(keys, values)\n","print(metadata_df)"]},{"cell_type":"markdown","metadata":{},"source":["##### phenotype_data"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                  title geo_accession                 status submission_date  \\\n","GSM989827  age 67y 1001     GSM989827  Public on Nov 21 2012     Aug 21 2012   \n","GSM989828  age 89y 1002     GSM989828  Public on Nov 21 2012     Aug 21 2012   \n","GSM989829  age 66y 1003     GSM989829  Public on Nov 21 2012     Aug 21 2012   \n","GSM989830  age 64y 1004     GSM989830  Public on Nov 21 2012     Aug 21 2012   \n","GSM989831  age 62y 1005     GSM989831  Public on Nov 21 2012     Aug 21 2012   \n","\n","          last_update_date     type channel_count source_name_ch1  \\\n","GSM989827      Nov 21 2012  genomic             1           X1001   \n","GSM989828      Nov 21 2012  genomic             1           X1002   \n","GSM989829      Nov 21 2012  genomic             1           X1003   \n","GSM989830      Nov 21 2012  genomic             1           X1004   \n","GSM989831      Nov 21 2012  genomic             1           X1005   \n","\n","           organism_ch1 taxid_ch1  ...                contact_email  \\\n","GSM989827  Homo sapiens      9606  ...  justin.guinney@sagebase.org   \n","GSM989828  Homo sapiens      9606  ...  justin.guinney@sagebase.org   \n","GSM989829  Homo sapiens      9606  ...  justin.guinney@sagebase.org   \n","GSM989830  Homo sapiens      9606  ...  justin.guinney@sagebase.org   \n","GSM989831  Homo sapiens      9606  ...  justin.guinney@sagebase.org   \n","\n","          contact_institute contact_address contact_city contact_state  \\\n","GSM989827  Sage Bionetworks   2606 E Roy St      Seattle            WA   \n","GSM989828  Sage Bionetworks   2606 E Roy St      Seattle            WA   \n","GSM989829  Sage Bionetworks   2606 E Roy St      Seattle            WA   \n","GSM989830  Sage Bionetworks   2606 E Roy St      Seattle            WA   \n","GSM989831  Sage Bionetworks   2606 E Roy St      Seattle            WA   \n","\n","          contact_zip/postal_code contact_country supplementary_file  \\\n","GSM989827                   98110             USA               NONE   \n","GSM989828                   98110             USA               NONE   \n","GSM989829                   98110             USA               NONE   \n","GSM989830                   98110             USA               NONE   \n","GSM989831                   98110             USA               NONE   \n","\n","          series_id data_row_count  \n","GSM989827  GSE40279         473034  \n","GSM989828  GSE40279         473034  \n","GSM989829  GSE40279         473034  \n","GSM989830  GSE40279         473034  \n","GSM989831  GSE40279         473034  \n","\n","[5 rows x 36 columns]\n"]}],"source":["phenotype_data_40279 = gse.phenotype_data\n","print(phenotype_data_40279.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(phenotype_data_40279.columns)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["           Age Gender\n","GSM989827   67      F\n","GSM989828   89      F\n","GSM989829   66      F\n","GSM989830   64      F\n","GSM989831   62      F\n","...        ...    ...\n","GSM990623   78      F\n","GSM990624   71      M\n","GSM990625   68      M\n","GSM990626   61      F\n","GSM990627   73      M\n","\n","[656 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\tokyo\\AppData\\Local\\Temp\\ipykernel_1416\\3604874324.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  sample_age_gender_data_40279['Age'] = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n"]}],"source":["# Assuming 'phenotype_data' is the DataFrame with sample information\n","# Extract the 'title' and 'characteristics_ch1.0.age (y)' columns\n","sample_age_gender_data_40279 = phenotype_data_40279[['characteristics_ch1.0.age (y)', 'characteristics_ch1.3.gender']]\n","\n","# Rename the columns for clarity\n","sample_age_gender_data_40279.columns = ['Age', 'Gender']\n","\n","# Convert the 'Age' columns to numeric values\n","sample_age_gender_data_40279['Age'] = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n","\n","# Display the new DataFrame\n","print(sample_age_gender_data_40279)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(sample_age_gender_data_40279)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                 title geo_accession                 status submission_date  \\\n","GSM765860      CDH+103     GSM765860  Public on Jun 13 2012     Jul 22 2011   \n","GSM765861  CDH+NewBorn     GSM765861  Public on Jun 13 2012     Jul 22 2011   \n","GSM765862        OLD16     GSM765862  Public on Jun 13 2012     Jul 22 2011   \n","GSM765863        OLD17     GSM765863  Public on Jun 13 2012     Jul 22 2011   \n","GSM765864        OLD18     GSM765864  Public on Jun 13 2012     Jul 22 2011   \n","\n","          last_update_date     type channel_count source_name_ch1  \\\n","GSM765860      Jun 13 2012  genomic             1   Nonagenarians   \n","GSM765861      Jun 13 2012  genomic             1        Newborns   \n","GSM765862      Jun 13 2012  genomic             1   Nonagenarians   \n","GSM765863      Jun 13 2012  genomic             1   Nonagenarians   \n","GSM765864      Jun 13 2012  genomic             1   Nonagenarians   \n","\n","           organism_ch1 taxid_ch1  ... contact_department contact_institute  \\\n","GSM765860  Homo sapiens      9606  ...               PEBC           IDIBELL   \n","GSM765861  Homo sapiens      9606  ...               PEBC           IDIBELL   \n","GSM765862  Homo sapiens      9606  ...               PEBC           IDIBELL   \n","GSM765863  Homo sapiens      9606  ...               PEBC           IDIBELL   \n","GSM765864  Homo sapiens      9606  ...               PEBC           IDIBELL   \n","\n","                                             contact_address  \\\n","GSM765860  Hospital Duran i Reynals Av. Gran Via s/n km, 2.7   \n","GSM765861  Hospital Duran i Reynals Av. Gran Via s/n km, 2.7   \n","GSM765862  Hospital Duran i Reynals Av. Gran Via s/n km, 2.7   \n","GSM765863  Hospital Duran i Reynals Av. Gran Via s/n km, 2.7   \n","GSM765864  Hospital Duran i Reynals Av. Gran Via s/n km, 2.7   \n","\n","                        contact_city contact_state contact_zip/postal_code  \\\n","GSM765860  L'Hospitalet de Llobregat     Barcelona                   08908   \n","GSM765861  L'Hospitalet de Llobregat     Barcelona                   08908   \n","GSM765862  L'Hospitalet de Llobregat     Barcelona                   08908   \n","GSM765863  L'Hospitalet de Llobregat     Barcelona                   08908   \n","GSM765864  L'Hospitalet de Llobregat     Barcelona                   08908   \n","\n","          contact_country supplementary_file series_id data_row_count  \n","GSM765860           Spain               NONE  GSE30870         485577  \n","GSM765861           Spain               NONE  GSE30870         485577  \n","GSM765862           Spain               NONE  GSE30870         485577  \n","GSM765863           Spain               NONE  GSE30870         485577  \n","GSM765864           Spain               NONE  GSE30870         485577  \n","\n","[5 rows x 34 columns]\n"]}],"source":["phenotype_data_30870 = gse30870.phenotype_data\n","print(phenotype_data_30870.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(phenotype_data_30870.columns)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                 Age\n","GSM765860  103 years\n","GSM765861    Newborn\n","GSM765862   97 years\n","GSM765863   95 years\n","GSM765864   97 years\n","GSM765865   97 years\n","GSM765866   98 years\n","GSM765867   96 years\n","GSM765868  100 years\n","GSM765869   90 years\n","GSM765870   91 years\n","GSM765871   92 years\n","GSM765872   89 years\n","GSM765873   90 years\n","GSM765874   89 years\n","GSM765875   90 years\n","GSM765876   91 years\n","GSM765877   89 years\n","GSM765878   89 years\n","GSM765879   90 years\n","GSM765880   90 years\n","GSM765881    Newborn\n","GSM765882    Newborn\n","GSM765883    Newborn\n","GSM765884    Newborn\n","GSM765885    Newborn\n","GSM765886    Newborn\n","GSM765887    Newborn\n","GSM765888    Newborn\n","GSM765889    Newborn\n","GSM765890    Newborn\n","GSM765891    Newborn\n","GSM765892    Newborn\n","GSM765893    Newborn\n","GSM765894    Newborn\n","GSM765895    Newborn\n","GSM765896    Newborn\n","GSM765897    Newborn\n","GSM765898    Newborn\n","GSM765899    Newborn\n"]}],"source":["# Assuming 'phenotype_data' is the DataFrame with sample information\n","# Extract the 'title' and 'characteristics_ch1.0.age (y)' columns\n","sample_age_data_30870 = pd.DataFrame(phenotype_data_30870['characteristics_ch1.0.age'])\n","\n","# Rename the columns for clarity\n","sample_age_data_30870.columns = ['Age']\n","\n","# Display the new DataFrame\n","print(sample_age_data_30870)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>GSM765860</th>\n","      <td>103</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765861</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765862</th>\n","      <td>97</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765863</th>\n","      <td>95</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765864</th>\n","      <td>97</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765865</th>\n","      <td>97</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765866</th>\n","      <td>98</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765867</th>\n","      <td>96</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765868</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765869</th>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765870</th>\n","      <td>91</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765871</th>\n","      <td>92</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765872</th>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765873</th>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765874</th>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765875</th>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765876</th>\n","      <td>91</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765877</th>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765878</th>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765879</th>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765880</th>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765881</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765882</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765883</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765884</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765885</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765886</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765887</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765888</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765889</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765890</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765891</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765892</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765893</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765894</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765895</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765896</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765897</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765898</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>GSM765899</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Age\n","GSM765860  103\n","GSM765861    0\n","GSM765862   97\n","GSM765863   95\n","GSM765864   97\n","GSM765865   97\n","GSM765866   98\n","GSM765867   96\n","GSM765868  100\n","GSM765869   90\n","GSM765870   91\n","GSM765871   92\n","GSM765872   89\n","GSM765873   90\n","GSM765874   89\n","GSM765875   90\n","GSM765876   91\n","GSM765877   89\n","GSM765878   89\n","GSM765879   90\n","GSM765880   90\n","GSM765881    0\n","GSM765882    0\n","GSM765883    0\n","GSM765884    0\n","GSM765885    0\n","GSM765886    0\n","GSM765887    0\n","GSM765888    0\n","GSM765889    0\n","GSM765890    0\n","GSM765891    0\n","GSM765892    0\n","GSM765893    0\n","GSM765894    0\n","GSM765895    0\n","GSM765896    0\n","GSM765897    0\n","GSM765898    0\n","GSM765899    0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["sample_age_data_30870['Age'] = sample_age_data_30870['Age'].str.replace(' years', '').replace('Newborn', '0')\n","sample_age_data_30870"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_age_data_30870['Age'] = pd.to_numeric(sample_age_data_30870['Age'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(sample_age_data_30870['Age'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["phenotype_data_50660 = gse50660.phenotype_data\n","print(phenotype_data_50660.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["phenotype_data_50660.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming 'phenotype_data' is the DataFrame with sample information\n","# Extract the 'title' and 'characteristics_ch1.0.age (y)' columns\n","sample_age_data_50660 = pd.DataFrame(phenotype_data_50660['characteristics_ch1.1.age'])\n","\n","# Rename the columns for clarity\n","sample_age_data_50660.columns = ['Age']\n","\n","# Display the new DataFrame\n","print(sample_age_data_50660)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'characteristics_ch1.1.age', 'characteristics_ch1.2.gender'"]},{"cell_type":"markdown","metadata":{},"source":["##### Age of Combined samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_age_gender_data_40279['Age']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_age_data_30870['Age']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming sample_age_gender_data_40279 and sample_age_data_30870 are your dataframes\n","\n","# Combine the 'Age' columns from both dataframes into a single column\n","combined_age_data = pd.concat([sample_age_gender_data_40279['Age'], sample_age_data_50660['Age']], axis=0, ignore_index=False)\n","combined_age_data = pd.DataFrame(combined_age_data)\n","\n","# Now combined_age_data contains the 'Age' values from both dataframes in a single column\n","print(combined_age_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the 'Age' columns to numeric values\n","sample_age_gender_data_40279['Age'] = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n","sample_age_data_30870['Age'] = pd.to_numeric(sample_age_data_30870['Age'], errors='coerce')\n","\n","# Check for missing values\n","print(\"Missing values in sample_age_gender_data_40279:\", sample_age_gender_data_40279['Age'].isnull().sum())\n","print(\"Missing values in sample_age_data_30870:\", sample_age_data_30870['Age'].isnull().sum())\n","\n","# Try combining the data again\n","combined_samples_data_age = sample_age_gender_data_40279['Age'] + sample_age_data_30870['Age']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the 'Age' columns to numeric values\n","sample_age_gender_data_40279.loc[:, 'Age'] = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n","sample_age_data_30870.loc[:, 'Age'] = pd.to_numeric(sample_age_data_30870['Age'], errors='coerce')\n","\n","# Check for missing values\n","print(\"Missing values in sample_age_gender_data_40279:\", sample_age_gender_data_40279['Age'].isnull().sum())\n","print(\"Missing values in sample_age_data_30870:\", sample_age_data_30870['Age'].isnull().sum())\n","\n","# Try combining the data again\n","combined_samples_data_age = sample_age_gender_data_40279['Age'] + sample_age_data_30870['Age']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["combined_age_data"]},{"cell_type":"markdown","metadata":{},"source":["#### Combine pivoted samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_40279.index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_30870.index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_50660.index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine the 'Age' columns from both dataframes into a single column\n","pivoted_samples = pd.concat([pivoted_samples_50660, pivoted_samples_40279], axis=1, join='inner', ignore_index=False)\n","pivoted_samples = pd.DataFrame(pivoted_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples"]},{"cell_type":"markdown","metadata":{},"source":["##### Save combined pivoted_samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the pivoted_samples\n","import pickle\n","with open('pivoted_data.pickle', 'wb') as file:\n","    pickle.dump(pivoted_samples, file)"]},{"cell_type":"markdown","metadata":{},"source":["##### Load saved pivoted_samples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the pivoted_data object from the local file\n","with open('pivoted_data.pickle', 'rb') as file:\n","    pivoted_samples = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pivoted_samples.index)\n","len(pivoted_samples.index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples.GSM1225377"]},{"cell_type":"markdown","metadata":{},"source":["### Results from multiple linear regression model "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Load the results_df object from the local file\n","with open('results_df.pickle', 'rb') as file:\n","    results_df = pickle.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["#### Adjust p.values by BH method for multiple testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy import stats\n","from statsmodels.stats.multitest import multipletests"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply the Benjamini-Hochberg correction\n","results_df['Adjusted_p-value_Age'] = multipletests(results_df['p-value_Age'], method='fdr_bh')[1]\n","results_df['Adjusted_p-value_Gender'] = multipletests(results_df['p-value_Gender'], method='fdr_bh')[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(results_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort the results_df DataFrame by 'R-squared' column in descending order\n","results_df = results_df.sort_values(by='R-squared', ascending=False)\n","\n","# Print the sorted results DataFrame\n","print(results_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["#### Save the results after performing regression and p.value adjusting "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Save the results_df object to a local file\n","with open('results_df.pickle', 'wb') as file:\n","    pickle.dump(results_df, file)"]},{"cell_type":"markdown","metadata":{},"source":["#### Load the results_df"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import pickle\n","with open('results_df.pickle', 'rb') as file:\n","    results_df = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(results_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["### Filter the probes"]},{"cell_type":"markdown","metadata":{},"source":["#### Probes with R-squared > 0.7"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["          Probe_ID  coef_Intercept  coef_Age  p-value_Age  coef_Gender  \\\n","87051   cg04462931        0.451527  0.000204     0.026797     0.400134   \n","217983  cg11955727        0.582826  0.000169     0.022359     0.261685   \n","315555  cg17765025        0.568103 -0.000058     0.473091     0.229386   \n","8226    cg00399683        0.583450  0.000115     0.195509     0.240899   \n","233448  cg12949927        0.617390  0.000183     0.037384     0.232580   \n","\n","        p-value_Gender  R-squared  Adjusted_p-value_Age  \\\n","87051              0.0   0.970963              0.062213   \n","217983             0.0   0.957005              0.053590   \n","315555             0.0   0.935353              0.594971   \n","8226               0.0   0.928524              0.305902   \n","233448             0.0   0.925232              0.081718   \n","\n","        Adjusted_p-value_Gender  \n","87051                       0.0  \n","217983                      0.0  \n","315555                      0.0  \n","8226                        0.0  \n","233448                      0.0  \n"]}],"source":["# Filter the probes with R-squared > 0.7\n","results_df_07 = results_df[results_df['R-squared'] > 0.7]\n","\n","# Print the filtered DataFrame\n","print(results_df_07.head())"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of probes in results_df_0.7: 53\n"]}],"source":["# Amount of the probes in results_df_07\n","num_probes_in_df_07 = results_df_07.shape[0]  # Access the number of rows using shape\n","print(\"Number of probes in results_df_0.7:\", num_probes_in_df_07)"]},{"cell_type":"markdown","metadata":{},"source":["#### Filter the expression values (beta values) based on the probe with R-squared > 0.7"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["name        GSM989827  GSM989828  GSM989829  GSM989830  GSM989831  GSM989832  \\\n","ID_REF                                                                         \n","cg00167275   0.339794   0.308087   0.384180   0.381271   0.329035   0.370114   \n","cg00399683   0.837955   0.811371   0.809503   0.823190   0.800675   0.822112   \n","cg00500229   0.137794   0.150368   0.136496   0.119805   0.123281   0.157386   \n","cg00774458   0.792943   0.801659   0.765358   0.778534   0.791380   0.766673   \n","cg00804338   0.296725   0.278522   0.327339   0.255138   0.252064   0.293368   \n","\n","name        GSM989833  GSM989834  GSM989835  GSM989836  ...  GSM990618  \\\n","ID_REF                                                  ...              \n","cg00167275   0.061615   0.339474   0.370807   0.375922  ...   0.336506   \n","cg00399683   0.567726   0.821688   0.824752   0.778578  ...   0.845356   \n","cg00500229   0.250605   0.139630   0.160768   0.166296  ...   0.161149   \n","cg00774458   0.630556   0.791275   0.769961   0.781188  ...   0.796697   \n","cg00804338   0.023712   0.189021   0.299181   0.257204  ...   0.238153   \n","\n","name        GSM990619  GSM990620  GSM990621  GSM990622  GSM990623  GSM990624  \\\n","ID_REF                                                                         \n","cg00167275   0.359742   0.319099   0.324783   0.378838   0.331313   0.020347   \n","cg00399683   0.858045   0.840026   0.816347   0.872988   0.841172   0.638641   \n","cg00500229   0.131334   0.135626   0.154558   0.148485   0.143271   0.292526   \n","cg00774458   0.822458   0.838231   0.791069   0.858231   0.816444   0.670371   \n","cg00804338   0.276944   0.237360   0.187106   0.117892   0.282462   0.021289   \n","\n","name        GSM990625  GSM990626  GSM990627  \n","ID_REF                                       \n","cg00167275   0.032338   0.312950   0.031974  \n","cg00399683   0.613879   0.837266   0.638344  \n","cg00500229   0.293512   0.133025   0.244099  \n","cg00774458   0.649653   0.806822   0.643721  \n","cg00804338   0.045037   0.258240   0.010706  \n","\n","[5 rows x 656 columns]\n"]}],"source":["# Get the list of 'Probe_ID' values from results_df_07\n","filtered_probe_ids_07 = results_df_07['Probe_ID'].tolist()\n","\n","# Filter the rows in pivoted_samples using the 'Probe_ID' values\n","pivoted_samples_07 = pivoted_samples_40279[pivoted_samples_40279.index.isin(filtered_probe_ids_07)]\n","\n","# Print the filtered expression values\n","print(pivoted_samples_07.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_rows_in_pivoted_samples_07 = pivoted_samples_07.shape[0]\n","print(\"Number of rows in pivoted_samples_07:\", num_rows_in_pivoted_samples_07)"]},{"cell_type":"markdown","metadata":{},"source":["#### Probes with R-squared > 0.5"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["          Probe_ID  coef_Intercept  coef_Age  p-value_Age  coef_Gender  \\\n","87051   cg04462931        0.451527  0.000204     0.026797     0.400134   \n","217983  cg11955727        0.582826  0.000169     0.022359     0.261685   \n","315555  cg17765025        0.568103 -0.000058     0.473091     0.229386   \n","8226    cg00399683        0.583450  0.000115     0.195509     0.240899   \n","233448  cg12949927        0.617390  0.000183     0.037384     0.232580   \n","\n","        p-value_Gender  R-squared  Adjusted_p-value_Age  \\\n","87051              0.0   0.970963              0.062213   \n","217983             0.0   0.957005              0.053590   \n","315555             0.0   0.935353              0.594971   \n","8226               0.0   0.928524              0.305902   \n","233448             0.0   0.925232              0.081718   \n","\n","        Adjusted_p-value_Gender  \n","87051                       0.0  \n","217983                      0.0  \n","315555                      0.0  \n","8226                        0.0  \n","233448                      0.0  \n"]}],"source":["# Filter the probes with R-squared > 0.5\n","results_df_05 = results_df[results_df['R-squared'] > 0.5]\n","\n","# Print the filtered DataFrame\n","print(results_df_05.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Amount of the probes in results_df_05\n","num_probes_in_df_05 = results_df_05.shape[0]  # Access the number of rows using shape\n","print(\"Number of probes in results_df_0.5:\", num_probes_in_df_05)"]},{"cell_type":"markdown","metadata":{},"source":["#### Filter the expression values (beta values) based on the probe with R-squared > 0.5"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["name        GSM989827  GSM989828  GSM989829  GSM989830  GSM989831  GSM989832  \\\n","ID_REF                                                                         \n","cg00167275   0.339794   0.308087   0.384180   0.381271   0.329035   0.370114   \n","cg00351283   0.641760   0.654487   0.624891   0.627309   0.609425   0.662757   \n","cg00399683   0.837955   0.811371   0.809503   0.823190   0.800675   0.822112   \n","cg00500229   0.137794   0.150368   0.136496   0.119805   0.123281   0.157386   \n","cg00739353   0.850796   0.889854   0.886508   0.856968   0.879049   0.900876   \n","\n","name        GSM989833  GSM989834  GSM989835  GSM989836  ...  GSM990618  \\\n","ID_REF                                                  ...              \n","cg00167275   0.061615   0.339474   0.370807   0.375922  ...   0.336506   \n","cg00351283   0.706329   0.617947   0.623201   0.619334  ...   0.661313   \n","cg00399683   0.567726   0.821688   0.824752   0.778578  ...   0.845356   \n","cg00500229   0.250605   0.139630   0.160768   0.166296  ...   0.161149   \n","cg00739353   0.848153   0.896316   0.877942   0.885333  ...   0.892998   \n","\n","name        GSM990619  GSM990620  GSM990621  GSM990622  GSM990623  GSM990624  \\\n","ID_REF                                                                         \n","cg00167275   0.359742   0.319099   0.324783   0.378838   0.331313   0.020347   \n","cg00351283   0.699705   0.709105   0.722758   0.632389   0.662782   0.763568   \n","cg00399683   0.858045   0.840026   0.816347   0.872988   0.841172   0.638641   \n","cg00500229   0.131334   0.135626   0.154558   0.148485   0.143271   0.292526   \n","cg00739353   0.879628   0.927795   0.930042   0.955769   0.911228   0.850925   \n","\n","name        GSM990625  GSM990626  GSM990627  \n","ID_REF                                       \n","cg00167275   0.032338   0.312950   0.031974  \n","cg00351283   0.734699   0.670368   0.797882  \n","cg00399683   0.613879   0.837266   0.638344  \n","cg00500229   0.293512   0.133025   0.244099  \n","cg00739353   0.845226   0.911167   0.877559  \n","\n","[5 rows x 656 columns]\n"]}],"source":["# Get the list of 'Probe_ID' values from results_df_5\n","filtered_probe_ids_05 = results_df_05['Probe_ID'].tolist()\n","\n","# Filter the rows in pivoted_samples using the 'Probe_ID' values\n","pivoted_samples_05 = pivoted_samples_40279[pivoted_samples_40279.index.isin(filtered_probe_ids_05)]\n","\n","# Print the filtered expression values\n","print(pivoted_samples_05.head())"]},{"cell_type":"markdown","metadata":{},"source":["##### Check expression value for a specific CpG site"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check beta value for a specific probe\n","pivoted_samples.loc['cg10501210'].values"]},{"cell_type":"markdown","metadata":{},"source":["### Predict age on other dataset by the trained model based on the beta values of the probes in filtered probes_id"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 82.28538914163563\n","R-squared: 0.6453738840108743\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","#y = combined_age_data['Age']\n","y = sample_age_gender_data_40279['Age']\n","\n","# Split the data into training and testing sets (you can adjust the test_size)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","# Create a linear regression model\n","model = LinearRegression()\n","\n","# Fit the model to the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ID_REF     cg00167275  cg00399683  cg00500229  cg00774458  cg00804338  \\\n","name                                                                    \n","GSM765860    0.064670    0.612996    0.431196    0.630378    0.050361   \n","GSM765861    0.030757    0.696062    0.275024    0.683184    0.032541   \n","GSM765862    0.326615    0.805149    0.152980    0.812822    0.271788   \n","GSM765863    0.363115    0.819858    0.138455    0.830171    0.287900   \n","GSM765864    0.291458    0.852226    0.153179    0.809179    0.332746   \n","GSM765865    0.066853    0.725100    0.233710    0.714946    0.242626   \n","GSM765866    0.382497    0.867149    0.151728    0.856242    0.261537   \n","GSM765867    0.378826    0.888188    0.169993    0.901945    0.273652   \n","GSM765868    0.107250    0.561516    0.347267    0.631007    0.084587   \n","GSM765869    0.355936    0.823331    0.229820    0.781816    0.286212   \n","GSM765870    0.310161    0.846230    0.183523    0.831232    0.296096   \n","GSM765871    0.343867    0.860226    0.190548    0.833991    0.322001   \n","GSM765872    0.371256    0.876450    0.231025    0.844528    0.380313   \n","GSM765873    0.271189    0.895833    0.217501    0.889998    0.191451   \n","GSM765874    0.325145    0.840212    0.174204    0.826668    0.381811   \n","GSM765875    0.374952    0.829702    0.176206    0.824710    0.426310   \n","GSM765876    0.096972    0.641502    0.380209    0.668787    0.138766   \n","GSM765877    0.177868    0.607131    0.382494    0.640324    0.049264   \n","GSM765878    0.295676    0.847392    0.178858    0.803918    0.144256   \n","GSM765879    0.420225    0.885529    0.170810    0.869549    0.175421   \n","GSM765880    0.310963    0.848806    0.115660    0.842250    0.160059   \n","GSM765881    0.330983    0.840152    0.127619    0.820780    0.296409   \n","GSM765882    0.293090    0.833137    0.156606    0.827056    0.293289   \n","GSM765883    0.131391    0.620888    0.318760    0.659754    0.040468   \n","GSM765884    0.279530    0.868850    0.135829    0.866918    0.315303   \n","GSM765885    0.300311    0.868309    0.148213    0.889776    0.206060   \n","GSM765886    0.327184    0.861596    0.128127    0.837617    0.284171   \n","GSM765887    0.036799    0.615985    0.314047    0.656617    0.037529   \n","GSM765888    0.052025    0.605970    0.321277    0.681943         NaN   \n","GSM765889    0.346654    0.825471    0.176639    0.803191    0.335859   \n","GSM765890    0.267564    0.860803    0.176627    0.842189    0.329058   \n","GSM765891    0.359148    0.886171    0.187179    0.876278    0.267292   \n","GSM765892    0.259355    0.855560    0.173474    0.864156    0.246563   \n","GSM765893    0.314122    0.854847    0.150535    0.800000    0.282227   \n","GSM765894    0.056006    0.675930    0.334416    0.648272    0.045270   \n","GSM765895    0.342869    0.860377    0.155914    0.800080    0.290735   \n","GSM765896    0.040569    0.678968    0.350548    0.702399    0.024738   \n","GSM765897    0.036179    0.592035    0.276406    0.637866    0.032775   \n","GSM765898    0.303545    0.853939    0.161557    0.797440    0.211255   \n","GSM765899    0.262888    0.846801    0.132350    0.823449    0.270003   \n","\n","ID_REF     cg02325951  cg02556954  cg02987832  cg03618918  cg03691818  ...  \\\n","name                                                                   ...   \n","GSM765860    0.749222    0.739988    0.147105    0.814932    0.074917  ...   \n","GSM765861    0.794209    0.720161    0.130210    0.866566    0.072480  ...   \n","GSM765862    0.612004    0.535651    0.097450    0.646461    0.214890  ...   \n","GSM765863    0.632408    0.578165    0.110311    0.693339    0.196287  ...   \n","GSM765864    0.643012    0.581043    0.102409    0.701865    0.201108  ...   \n","GSM765865    0.698827    0.689195    0.140076    0.772882    0.063030  ...   \n","GSM765866    0.621712    0.597455    0.114766    0.711413    0.196679  ...   \n","GSM765867    0.639970    0.639400    0.108561    0.769653    0.176946  ...   \n","GSM765868    0.725668    0.666054    0.162900    0.777177    0.061799  ...   \n","GSM765869    0.583402    0.596854    0.132175    0.722051    0.243400  ...   \n","GSM765870    0.615471    0.587533    0.120271    0.723821    0.201617  ...   \n","GSM765871    0.613231    0.600695    0.126154    0.700140    0.194866  ...   \n","GSM765872    0.648053    0.600472    0.099269    0.719515    0.228716  ...   \n","GSM765873         NaN    0.630288    0.124242    0.735010    0.128249  ...   \n","GSM765874    0.620226    0.585627    0.090522    0.694432    0.189525  ...   \n","GSM765875    0.648139    0.575556    0.126288    0.665414    0.193513  ...   \n","GSM765876    0.708320    0.711456    0.174685    0.804110    0.047930  ...   \n","GSM765877    0.722943    0.704485    0.173609    0.808244    0.064706  ...   \n","GSM765878    0.622274    0.614883    0.124877    0.722033    0.220556  ...   \n","GSM765879    0.656942    0.645010    0.118170    0.733827    0.137550  ...   \n","GSM765880    0.626174    0.585356    0.103023    0.726144    0.195143  ...   \n","GSM765881    0.612216    0.590650    0.103749    0.683317    0.203522  ...   \n","GSM765882    0.603064    0.588051    0.112992    0.695589    0.170145  ...   \n","GSM765883    0.684017    0.712844    0.169382    0.815032    0.056989  ...   \n","GSM765884    0.666552    0.636155    0.106245    0.741872    0.174732  ...   \n","GSM765885    0.607884    0.598701    0.116686    0.733813    0.157198  ...   \n","GSM765886    0.620378    0.600611    0.147566    0.714600    0.168538  ...   \n","GSM765887    0.706710    0.718588    0.141182    0.805720    0.044356  ...   \n","GSM765888    0.756968    0.731339    0.168907    0.790483    0.052362  ...   \n","GSM765889    0.621614    0.598812    0.124518    0.706743    0.225725  ...   \n","GSM765890    0.669061    0.658768    0.122031    0.763200    0.180296  ...   \n","GSM765891    0.672828    0.667497    0.113787    0.756018    0.179485  ...   \n","GSM765892    0.650025    0.654739    0.091186    0.723597    0.171020  ...   \n","GSM765893    0.657579    0.620345    0.106304    0.727522    0.174085  ...   \n","GSM765894    0.728367    0.724240    0.183966    0.820651    0.050181  ...   \n","GSM765895    0.654908    0.628720    0.118825    0.731301    0.168863  ...   \n","GSM765896    0.730499    0.756658    0.176444    0.831777    0.036354  ...   \n","GSM765897    0.716639    0.692607    0.166078    0.757228    0.047715  ...   \n","GSM765898    0.648259    0.604646    0.113288    0.691084    0.165614  ...   \n","GSM765899    0.653250    0.632863    0.131379    0.692071    0.197590  ...   \n","\n","ID_REF     cg22547286  cg23256579  cg23719534  cg24056269  cg24919522  \\\n","name                                                                    \n","GSM765860    0.878125    0.485111    0.846231    0.785637    0.154865   \n","GSM765861    0.852191    0.778225    0.817481    0.821346    0.250939   \n","GSM765862    0.856700    0.599043    0.928366    0.699159    0.235294   \n","GSM765863    0.860523    0.759259    0.951090    0.740349    0.239979   \n","GSM765864    0.870172    0.563532    0.953550    0.721806    0.237946   \n","GSM765865    0.904342    0.399292    0.862698    0.799854    0.109359   \n","GSM765866    0.840836    0.617229    0.944792    0.743579    0.263727   \n","GSM765867    0.906279    0.687108    0.931319    0.821476    0.262614   \n","GSM765868    0.881421    0.329661    0.832383    0.783650    0.107471   \n","GSM765869    0.868926    0.637716    0.937453    0.737484    0.253092   \n","GSM765870    0.804833    0.689586    0.941828    0.741573    0.216209   \n","GSM765871    0.882771    0.610644    0.943089    0.726576    0.218691   \n","GSM765872    0.901236    0.541332    0.951902    0.764619    0.238296   \n","GSM765873    0.901052    0.707091    0.945790    0.800915    0.250000   \n","GSM765874    0.858459    0.585782    0.932108    0.721426    0.178212   \n","GSM765875    0.833149    0.575467    0.920410    0.737619    0.221422   \n","GSM765876    0.902937    0.392157    0.824218    0.818622    0.120782   \n","GSM765877    0.906071    0.495669    0.807676    0.809448    0.159291   \n","GSM765878    0.868479    0.672366    0.932071    0.741238    0.213758   \n","GSM765879    0.899966    0.636323    0.949724    0.767945    0.241955   \n","GSM765880    0.860529    0.619396    0.941387    0.740884    0.154782   \n","GSM765881    0.869471    0.852221    0.936406    0.711980    0.271793   \n","GSM765882    0.884243    0.873285    0.923004    0.726377    0.282681   \n","GSM765883    0.906517    0.867683    0.793795    0.827189    0.140279   \n","GSM765884    0.900408    0.861945    0.935330    0.766389    0.244165   \n","GSM765885    0.900890    0.827551    0.944701    0.785814    0.320619   \n","GSM765886    0.875394    0.847709    0.947477    0.735549    0.266100   \n","GSM765887    0.894688    0.805229    0.830771    0.815703    0.150308   \n","GSM765888    0.912509    0.842997    0.773011    0.804399    0.117168   \n","GSM765889    0.885916    0.849244    0.923613    0.745376    0.337288   \n","GSM765890    0.877107    0.842477    0.925216    0.764894    0.271031   \n","GSM765891    0.900162    0.851933    0.932205    0.789950    0.348806   \n","GSM765892    0.864436    0.795528    0.927047    0.747278    0.249870   \n","GSM765893    0.883710    0.826125    0.930656    0.763295    0.261878   \n","GSM765894    0.919489    0.867761    0.799947    0.850154    0.161963   \n","GSM765895    0.865285    0.864503    0.873293    0.750760    0.257716   \n","GSM765896    0.912140    0.842385    0.842932    0.842897    0.134844   \n","GSM765897    0.892649    0.836554    0.846607    0.785515    0.142042   \n","GSM765898    0.884431    0.827503    0.924587    0.732927    0.294597   \n","GSM765899    0.865707    0.843628    0.928439    0.746290    0.264724   \n","\n","ID_REF     cg25304146  cg25568337  cg26324366  cg26350814  cg27540865  \n","name                                                                   \n","GSM765860    0.668353    0.165702    0.156922    0.725609    0.460952  \n","GSM765861    0.731571    0.209164    0.122819    0.719719    0.429044  \n","GSM765862    0.530087    0.219134    0.172200    0.814435    0.330067  \n","GSM765863    0.582458    0.223588    0.241683    0.776650    0.094712  \n","GSM765864    0.581426    0.274843    0.240480    0.808169    0.122947  \n","GSM765865    0.655326    0.175025    0.126908    0.773672    0.395890  \n","GSM765866    0.581329    0.224650    0.211194    0.790949    0.081439  \n","GSM765867    0.619375    0.173735    0.196139    0.864750    0.076409  \n","GSM765868    0.619641    0.184356    0.145537    0.732397    0.443167  \n","GSM765869    0.574572    0.235487    0.229476    0.798812    0.245506  \n","GSM765870    0.584342    0.247414    0.225506    0.798114    0.134741  \n","GSM765871    0.615914    0.265390    0.234236    0.804407    0.163934  \n","GSM765872    0.603936    0.237308    0.247263    0.798529    0.119589  \n","GSM765873    0.629082    0.202565    0.191248    0.857398    0.062131  \n","GSM765874    0.550978    0.218172    0.200834    0.791904    0.104973  \n","GSM765875    0.568781    0.218087    0.193974    0.790644    0.113110  \n","GSM765876    0.669676    0.179804    0.152511    0.737910    0.423414  \n","GSM765877    0.678927    0.168748    0.153488    0.730974    0.468999  \n","GSM765878    0.546138    0.270999    0.201231    0.795440    0.275139  \n","GSM765879    0.625134    0.197470    0.196843    0.811356    0.058750  \n","GSM765880    0.656608    0.229322    0.247490    0.795732    0.066206  \n","GSM765881    0.563008    0.241037    0.161918    0.760605    0.048936  \n","GSM765882    0.570685    0.236916    0.186496    0.776189    0.092496  \n","GSM765883    0.670722    0.174596    0.103818    0.710110    0.413711  \n","GSM765884    0.624397    0.251957    0.214555    0.768141    0.051790  \n","GSM765885    0.620522    0.193031    0.189446    0.812300    0.060192  \n","GSM765886    0.588839    0.221196    0.202403    0.810827    0.052254  \n","GSM765887    0.645346    0.202429    0.121690    0.707670    0.386422  \n","GSM765888    0.658059    0.143600    0.100699    0.713410    0.379815  \n","GSM765889    0.617807    0.222781    0.217611    0.758950    0.059160  \n","GSM765890    0.633744    0.256734    0.205711    0.736599    0.073489  \n","GSM765891    0.636998    0.183467    0.193316    0.806399    0.060125  \n","GSM765892    0.582020    0.205426    0.190695    0.792528    0.053681  \n","GSM765893    0.631751    0.218890    0.214863    0.771489    0.067989  \n","GSM765894    0.706431    0.183990    0.131896    0.708075    0.406082  \n","GSM765895    0.608422    0.256291    0.203442    0.769474    0.075294  \n","GSM765896    0.708780    0.133593    0.103748    0.739406    0.375878  \n","GSM765897    0.630247    0.157533    0.100969    0.706662    0.382542  \n","GSM765898    0.585341    0.219637    0.156361    0.799295    0.063507  \n","GSM765899    0.599919    0.231113    0.178471    0.790512    0.071125  \n","\n","[40 rows x 53 columns]\n"]}],"source":["# Get the list of 'Probe_ID' values from results_df_07\n","#filtered_probe_ids_07 = results_df_07['Probe_ID'].tolist()\n","\n","# Filter the rows in pivoted_samples using the 'Probe_ID' values\n","#pivoted_samples_07 = pivoted_samples[pivoted_samples.index.isin(filtered_probe_ids_07)]\n","\n","# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","print(X_new_data_to_predict)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 77.40732386 -22.61074486  75.02465711  74.6531991   82.98700515\n","  91.83395895  78.3114189   80.44403381  74.2033714   70.17896235\n","  73.46140065  72.59213666  80.96184464  80.59290631 100.44724724\n","  77.07939098  78.9667787   82.90179019  86.19081774  95.38231743\n","  79.87508164 -15.7689885  -17.50272501 -20.16759755 -10.28749106\n"," -16.67940925   1.22354145 -19.85720768 -18.33193122   6.61997542\n","  -4.13161802  -8.53046755  -7.87778739 -11.53253063 -16.3453718\n","  -5.98208151 -20.499198   -24.19686556  -9.09368746  -5.96395079]\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n","  warnings.warn(\n"]}],"source":["from sklearn.impute import SimpleImputer\n","\n","# Impute missing values (NaN) with mean values for Expression\n","imputer = SimpleImputer(strategy='mean')\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","predicted_age = model.predict(X_new_data_to_predict)\n","print(predicted_age)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GSM765860    103 years\n","GSM765861      Newborn\n","GSM765862     97 years\n","GSM765863     95 years\n","GSM765864     97 years\n","GSM765865     97 years\n","GSM765866     98 years\n","GSM765867     96 years\n","GSM765868    100 years\n","GSM765869     90 years\n","GSM765870     91 years\n","GSM765871     92 years\n","GSM765872     89 years\n","GSM765873     90 years\n","GSM765874     89 years\n","GSM765875     90 years\n","GSM765876     91 years\n","GSM765877     89 years\n","GSM765878     89 years\n","GSM765879     90 years\n","GSM765880     90 years\n","GSM765881      Newborn\n","GSM765882      Newborn\n","GSM765883      Newborn\n","GSM765884      Newborn\n","GSM765885      Newborn\n","GSM765886      Newborn\n","GSM765887      Newborn\n","GSM765888      Newborn\n","GSM765889      Newborn\n","GSM765890      Newborn\n","GSM765891      Newborn\n","GSM765892      Newborn\n","GSM765893      Newborn\n","GSM765894      Newborn\n","GSM765895      Newborn\n","GSM765896      Newborn\n","GSM765897      Newborn\n","GSM765898      Newborn\n","GSM765899      Newborn\n","Name: characteristics_ch1.0.age, dtype: object\n"]}],"source":["Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","print(Chronological_age_30870)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["           Predicted age Chronological age\n","GSM765860      77.407324         103 years\n","GSM765861     -22.610745           Newborn\n","GSM765862      75.024657          97 years\n","GSM765863      74.653199          95 years\n","GSM765864      82.987005          97 years\n","GSM765865      91.833959          97 years\n","GSM765866      78.311419          98 years\n","GSM765867      80.444034          96 years\n","GSM765868      74.203371         100 years\n","GSM765869      70.178962          90 years\n","GSM765870      73.461401          91 years\n","GSM765871      72.592137          92 years\n","GSM765872      80.961845          89 years\n","GSM765873      80.592906          90 years\n","GSM765874     100.447247          89 years\n","GSM765875      77.079391          90 years\n","GSM765876      78.966779          91 years\n","GSM765877      82.901790          89 years\n","GSM765878      86.190818          89 years\n","GSM765879      95.382317          90 years\n","GSM765880      79.875082          90 years\n","GSM765881     -15.768989           Newborn\n","GSM765882     -17.502725           Newborn\n","GSM765883     -20.167598           Newborn\n","GSM765884     -10.287491           Newborn\n","GSM765885     -16.679409           Newborn\n","GSM765886       1.223541           Newborn\n","GSM765887     -19.857208           Newborn\n","GSM765888     -18.331931           Newborn\n","GSM765889       6.619975           Newborn\n","GSM765890      -4.131618           Newborn\n","GSM765891      -8.530468           Newborn\n","GSM765892      -7.877787           Newborn\n","GSM765893     -11.532531           Newborn\n","GSM765894     -16.345372           Newborn\n","GSM765895      -5.982082           Newborn\n","GSM765896     -20.499198           Newborn\n","GSM765897     -24.196866           Newborn\n","GSM765898      -9.093687           Newborn\n","GSM765899      -5.963951           Newborn\n"]}],"source":["import pandas as pd\n","import numpy as np  # Make sure you have numpy imported\n","\n","data = {\n","    'Predicted age': predicted_age,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870 = pd.DataFrame(data)\n","print(predicted_30870)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["           Predicted age  Chronological age\n","GSM765860      77.407324                103\n","GSM765861     -22.610745                  0\n","GSM765862      75.024657                 97\n","GSM765863      74.653199                 95\n","GSM765864      82.987005                 97\n","GSM765865      91.833959                 97\n","GSM765866      78.311419                 98\n","GSM765867      80.444034                 96\n","GSM765868      74.203371                100\n","GSM765869      70.178962                 90\n","GSM765870      73.461401                 91\n","GSM765871      72.592137                 92\n","GSM765872      80.961845                 89\n","GSM765873      80.592906                 90\n","GSM765874     100.447247                 89\n","GSM765875      77.079391                 90\n","GSM765876      78.966779                 91\n","GSM765877      82.901790                 89\n","GSM765878      86.190818                 89\n","GSM765879      95.382317                 90\n","GSM765880      79.875082                 90\n","GSM765881     -15.768989                  0\n","GSM765882     -17.502725                  0\n","GSM765883     -20.167598                  0\n","GSM765884     -10.287491                  0\n","GSM765885     -16.679409                  0\n","GSM765886       1.223541                  0\n","GSM765887     -19.857208                  0\n","GSM765888     -18.331931                  0\n","GSM765889       6.619975                  0\n","GSM765890      -4.131618                  0\n","GSM765891      -8.530468                  0\n","GSM765892      -7.877787                  0\n","GSM765893     -11.532531                  0\n","GSM765894     -16.345372                  0\n","GSM765895      -5.982082                  0\n","GSM765896     -20.499198                  0\n","GSM765897     -24.196866                  0\n","GSM765898      -9.093687                  0\n","GSM765899      -5.963951                  0\n"]}],"source":["# Remove ' years' from Chronological age and replace 'Newborn' with 1\n","predicted_30870['Chronological age'] = predicted_30870['Chronological age'].str.replace(' years', '').replace('Newborn', '0')\n","\n","# Convert the Chronological age column to numeric\n","predicted_30870['Chronological age'] = pd.to_numeric(predicted_30870['Chronological age'])\n","\n","# Print the updated DataFrame\n","print(predicted_30870)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 231.60403754104263\n","R-squared: 0.8936642245679487\n"]}],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_30870['Chronological age']\n","Predicted = predicted_30870['Predicted age']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(predicted_30870.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(gse30870.phenotype_data.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pivoted_samples_07.head())"]},{"cell_type":"markdown","metadata":{},"source":["### Linear regression model for finding CpG cites relating to the age-related CpG sites former found."]},{"cell_type":"markdown","metadata":{},"source":["#### Example of cg10501210"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","import statsmodels.api as sm\n","\n","# Specify the probe ID you want to analyze\n","specific_probe_id = 'cg10501210'  # Replace with the specific probe ID you're interested in\n","\n","# Get the list of 'Probe_ID' values from results_df_07\n","filtered_probe_ids_07 = results_df_07['Probe_ID'].tolist()\n","\n","# Filter the rows in pivoted_samples using the 'Probe_ID' values\n","pivoted_samples_07 = pivoted_samples[pivoted_samples.index.isin(filtered_probe_ids_07)]\n","\n","# Extract a CpG site from pivoted_samples_07 as input (or Feature)\n","X = pivoted_samples_07.loc['cg04462931'].values\n","\n","mask = pivoted_samples.index != 'cg04462931'\n","# target value\n","all_y = pivoted_samples[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(filtered_probe_ids_07)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(all_y.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(all_y.index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an empty list to store the results\n","results_list = []\n","# Loop through each row (probe) in the pivoted_samples DataFrame\n","for index, row in all_y.iterrows():\n","    # Extract a CpG site from pivoted_samples_07 as input (or Feature)\n","    X = pivoted_samples_07.loc['cg04462931'].values\n","    y = row.values\n","    \n","    # Reshape the 1D arrays to 2D arrays\n","    X = X.reshape(-1, 1)\n","    y = y.reshape(-1, 1)\n","    \n","    # Impute missing values (NaN) with mean values for both Age, Gender, and Expression\n","    imputer = SimpleImputer(strategy='mean')\n","    y = imputer.fit_transform(y)\n","    X = imputer.fit_transform(X)\n","     \n","    # Add a constant term for the intercept in the linear regression model\n","    X = sm.add_constant(X)\n","    \n","    # Fit the linear regression model using statsmodels\n","    model = sm.OLS(y, X).fit()\n","    \n","    # Extract the coefficients and p-values for each coefficient\n","    coefficients = model.params\n","    p_values = model.pvalues\n","    \n","    # Calculate R-squared\n","    y_pred = model.predict(X)\n","    r_squared = r2_score(y, y_pred)\n","    \n","    # Append results to the results list\n","    results_list.append({'Probe_ID': index, \n","                         'coef_Intercept': coefficients[0],  # Add coefficient for Intercept\n","                         'coef_fixed_CpGsite': coefficients[1], \n","                         'p-value_fixed_CpGsite': p_values[1],  # Add p-value for fixed_CpG\n","                         'R-squared': r_squared})\n","\n","# Convert the results list to a DataFrame\n","results_df_cg04462931 = pd.DataFrame(results_list)\n","\n","# Print the results DataFrame\n","print(results_df_cg04462931.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort the results_df DataFrame by 'R-squared' column in descending order\n","results_df_cg04462931 = results_df_cg04462931.sort_values(by='R-squared', ascending=False)\n","\n","# Print the sorted results DataFrame\n","print(results_df_cg04462931.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filter the probes with R-squared > 0.7\n","results_df_cg04462931_07 = results_df_cg04462931[results_df_cg04462931['R-squared'] > 0.7]\n","\n","# Print the filtered DataFrame\n","print(results_df_cg04462931_07[['R-squared', 'Probe_ID']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(results_df_cg04462931_07.index)"]},{"cell_type":"markdown","metadata":{},"source":["#### for all probes in 53 age-relted CpG"]},{"cell_type":"markdown","metadata":{},"source":["##### by LR on statsmodels"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Set this to the number of physical cores"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"endog and exog matrices are different sizes","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\tokyo\\OneDrive - 國立成功大學 National Cheng Kung University\\Python\\專題\\analyze data from GEO in python\\NN.ipynb 儲存格 134\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tokyo/OneDrive%20-%20%E5%9C%8B%E7%AB%8B%E6%88%90%E5%8A%9F%E5%A4%A7%E5%AD%B8%20National%20Cheng%20Kung%20University/Python/%E5%B0%88%E9%A1%8C/analyze%20data%20from%20GEO%20in%20python/NN.ipynb#Y626sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m X \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39madd_constant(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tokyo/OneDrive%20-%20%E5%9C%8B%E7%AB%8B%E6%88%90%E5%8A%9F%E5%A4%A7%E5%AD%B8%20National%20Cheng%20Kung%20University/Python/%E5%B0%88%E9%A1%8C/analyze%20data%20from%20GEO%20in%20python/NN.ipynb#Y626sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Fit the linear regression model using statsmodels\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tokyo/OneDrive%20-%20%E5%9C%8B%E7%AB%8B%E6%88%90%E5%8A%9F%E5%A4%A7%E5%AD%B8%20National%20Cheng%20Kung%20University/Python/%E5%B0%88%E9%A1%8C/analyze%20data%20from%20GEO%20in%20python/NN.ipynb#Y626sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mOLS(y, X)\u001b[39m.\u001b[39mfit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tokyo/OneDrive%20-%20%E5%9C%8B%E7%AB%8B%E6%88%90%E5%8A%9F%E5%A4%A7%E5%AD%B8%20National%20Cheng%20Kung%20University/Python/%E5%B0%88%E9%A1%8C/analyze%20data%20from%20GEO%20in%20python/NN.ipynb#Y626sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Calculate R-squared\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tokyo/OneDrive%20-%20%E5%9C%8B%E7%AB%8B%E6%88%90%E5%8A%9F%E5%A4%A7%E5%AD%B8%20National%20Cheng%20Kung%20University/Python/%E5%B0%88%E9%A1%8C/analyze%20data%20from%20GEO%20in%20python/NN.ipynb#Y626sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X)\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:922\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mAn exception will be raised in the next version.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    921\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 922\u001b[0m \u001b[39msuper\u001b[39m(OLS, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, missing\u001b[39m=\u001b[39mmissing,\n\u001b[0;32m    923\u001b[0m                           hasconst\u001b[39m=\u001b[39mhasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    924\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys:\n\u001b[0;32m    925\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:748\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m--> 748\u001b[0m \u001b[39msuper\u001b[39m(WLS, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, missing\u001b[39m=\u001b[39mmissing,\n\u001b[0;32m    749\u001b[0m                           weights\u001b[39m=\u001b[39mweights, hasconst\u001b[39m=\u001b[39mhasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    750\u001b[0m nobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    751\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 202\u001b[0m     \u001b[39msuper\u001b[39m(RegressionModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpinv_wexog: Float64Array \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_attr\u001b[39m.\u001b[39mextend([\u001b[39m'\u001b[39m\u001b[39mpinv_wexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwendog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mhasconst\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexog\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_handle_data\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, missing, hasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[39m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[39m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs:\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[39m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[39mreturn\u001b[39;00m klass(endog, exog\u001b[39m=\u001b[39mexog, missing\u001b[39m=\u001b[39mmissing, hasconst\u001b[39m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\data.py:89\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_constant(hasconst)\n\u001b[1;32m---> 89\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n","File \u001b[1;32mc:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\statsmodels\\base\\data.py:436\u001b[0m, in \u001b[0;36mModelData._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendog):\n\u001b[1;32m--> 436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mendog and exog matrices are different sizes\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: endog and exog matrices are different sizes"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import r2_score\n","import statsmodels.api as sm\n","\n","# Get the list of 'Probe_ID' values from results_df_07\n","filtered_probe_ids_07 = results_df_07['Probe_ID'].tolist()\n","\n","# Create an empty list to store the filtered results\n","filtered_results_list = []\n","\n","# Loop through each probe in filtered_probe_ids_07\n","for current_probe_id in filtered_probe_ids_07:\n","    # Exclude the current probe from the target values\n","    mask = pivoted_samples_40279.index != current_probe_id\n","    \n","    # Extract the feature (X) and target (y) values\n","    X = pivoted_samples_07.loc[current_probe_id].values\n","    all_y = pivoted_samples_40279[mask]\n","    \n","    # Loop through each row (probe) in the pivoted_samples DataFrame\n","    for index, row in all_y.iterrows():\n","        y = row.values\n","        \n","        # Reshape the 1D arrays to 2D arrays\n","        X = X.reshape(-1, 1)\n","        y = y.reshape(-1, 1)\n","        \n","        # Impute missing values (NaN) with mean values for both Age, Gender, and Expression\n","        imputer = SimpleImputer(strategy='mean')\n","        y = imputer.fit_transform(y)\n","        X = imputer.fit_transform(X)\n","         \n","        # Add a constant term for the intercept in the linear regression model\n","        X = sm.add_constant(X)\n","        \n","        # Fit the linear regression model using statsmodels\n","        model = sm.OLS(y, X).fit()\n","        \n","        # Calculate R-squared\n","        y_pred = model.predict(X)\n","        r_squared = r2_score(y, y_pred)\n","        \n","        # Check if R-squared is greater than 0.7\n","        if r_squared > 0.7:\n","            # Append filtered results to the list\n","            filtered_results_list.append({'Probe_ID': current_probe_id,\n","                                          'Target_Probe_ID': index,\n","                                          'R-squared': r_squared})\n","\n","# Convert the filtered results list to a DataFrame\n","filtered_results_df = pd.DataFrame(filtered_results_list)\n","\n","# Print the filtered results DataFrame\n","print(filtered_results_df.head())"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import r2_score\n","import statsmodels.api as sm\n","\n","# Get the list of 'Probe_ID' values from results_df_07\n","filtered_probe_ids_07 = results_df_07['Probe_ID'].tolist()\n","\n","# Create an empty list to store the filtered results\n","filtered_results_list = []\n","\n","# Loop through each probe in filtered_probe_ids_07\n","for current_probe_id in filtered_probe_ids_07:\n","    # Exclude the current probe from the target values\n","    mask = pivoted_samples_40279.index != current_probe_id\n","    \n","    # Extract the feature (X) and target (y) values\n","    X = pivoted_samples_07.loc[current_probe_id].values.reshape(-1, 1)\n","    all_y = pivoted_samples_40279[mask]\n","    \n","    # Loop through each row (probe) in the pivoted_samples DataFrame\n","    for index, row in all_y.iterrows():\n","        y = row.values.reshape(-1, 1)\n","        \n","        # Impute missing values (NaN) with mean values for both Age, Gender, and Expression\n","        imputer = SimpleImputer(strategy='mean')\n","        y = imputer.fit_transform(y)\n","        X = imputer.fit_transform(X)\n","         \n","        # Add a constant term for the intercept in the linear regression model\n","        X = sm.add_constant(X)\n","        \n","        # Fit the linear regression model using statsmodels\n","        model = sm.OLS(y, X).fit()\n","        \n","        # Calculate R-squared\n","        y_pred = model.predict(X)\n","        r_squared = r2_score(y, y_pred)\n","        \n","        # Check if R-squared is greater than 0.7\n","        if r_squared > 0.7:\n","            # Append filtered results to the list\n","            filtered_results_list.append({'Probe_ID': current_probe_id,\n","                                          'Target_Probe_ID': index,\n","                                          'R-squared': r_squared})\n","\n","# Convert the filtered results list to a DataFrame\n","filtered_results_df = pd.DataFrame(filtered_results_list)\n","\n","# Print the filtered results DataFrame\n","print(filtered_results_df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["##### by NN on tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import r2_score\n","import tensorflow as tf\n","\n","# Get probes that are not in filtered_probe_ids_07\n","other_probes = [probe for probe in pivoted_samples_40279.index if probe not in filtered_probe_ids_07]\n","other_probes\n","#len(other_probes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_40279"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_40279.T['cg00041368']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_07"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_40279.T['cg00000029'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an empty list to store the results\n","results_list = []\n","\n","# To make row is sample id ; column is probe id\n","pivoted_samples_07_T = pivoted_samples_07.T\n","pivoted_samples_40279_T = pivoted_samples_40279.T \n","\n","# Loop through each row (probe) in the pivoted_samples DataFrame for probes in filtered_probe_ids_07\n","for index, row in pivoted_samples_07.iterrows():\n","    # Extract a CpG site from pivoted_samples_07 as input (or Feature)\n","    X = row.values\n","    \n","    # Reshape the 1D arrays to 2D arrays\n","    X = X.reshape(-1, 1)\n","    \n","    # Impute missing values (NaN) with mean values for both Age, Gender, and Expression\n","    imputer = SimpleImputer(strategy='mean')\n","    X = imputer.fit_transform(X)\n","    \n","    # Add a constant term for the intercept in the linear regression model\n","    X = np.concatenate([np.ones_like(X), X], axis=1)  # Add a column of ones for the intercept\n","    \n","    # Loop through each probe not in filtered_probe_ids_07\n","    for other_probe in other_probes:\n","        # Extract data for the other probe\n","        y = pivoted_samples_40279_T[other_probe].values\n","        y = y.reshape(-1, 1)\n","        y = imputer.fit_transform(y)\n","        \n","        # Convert to TensorFlow tensors\n","        X_tf = tf.constant(X, dtype=tf.float32)\n","        y_tf = tf.constant(y, dtype=tf.float32)\n","        \n","        # Define a linear regression model\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Dense(units=1, input_dim=2)  # 2 features: intercept and X\n","        ])\n","        \n","        # Compile the model\n","        model.compile(optimizer='sgd', loss='mean_squared_error')\n","        \n","        # Fit the model\n","        model.fit(X_tf, y_tf, epochs=100, verbose=0)\n","        \n","        # Predictions\n","        y_pred_tf = model.predict(X_tf)\n","        \n","        # Calculate R-squared\n","        r_squared = r2_score(y, y_pred_tf)\n","        \n","        # Append results to the results list\n","        results_list.append({'Probe_ID_X': index,\n","                             'Probe_ID_Y': other_probe,\n","                             'R-squared': r_squared})\n","\n","# Convert the results list to a DataFrame\n","results_df_correlation = pd.DataFrame(results_list)\n","\n","# Print the results DataFrame\n","print(results_df_correlation.head())\n","\n","# Sort the results_df DataFrame by 'R-squared' column in descending order\n","results_df_correlation = results_df_correlation.sort_values(by='R-squared', ascending=False)\n","\n","# Filter the results with R-squared > 0.7\n","results_df_correlation_07 = results_df_correlation[results_df_correlation['R-squared'] > 0.7]\n","\n","# Print the filtered DataFrame\n","print(results_df_correlation_07[['R-squared', 'Probe_ID_X', 'Probe_ID_Y']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_07.T.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pivoted_samples_07.T['cg00167275'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract a CpG site from pivoted_samples_07 as input (or Feature)\n","X = pivoted_samples_07.T['cg00167275'].values\n","\n","# Reshape the 1D arrays to 2D arrays\n","X = X.reshape(-1, 1)\n","\n","# Impute missing values (NaN) with mean values for both Age, Gender, and Expression\n","imputer = SimpleImputer(strategy='mean')\n","X = imputer.fit_transform(X)\n","\n","# Add a constant term for the intercept in the linear regression model\n","X = np.concatenate([np.ones_like(X), X], axis=1)  # Add a column of ones for the intercept\n","\n","# Loop through each probe not in filtered_probe_ids_07\n","for other_probe in other_probes:\n","    # Extract data for the other probe\n","    y = pivoted_samples_40279_T[other_probe].values\n","    y = y.reshape(-1, 1)\n","    y = imputer.fit_transform(y)\n","    \n","    # Convert to TensorFlow tensors\n","    X_tf = tf.constant(X, dtype=tf.float32)\n","    y_tf = tf.constant(y, dtype=tf.float32)\n","    \n","    # Define a linear regression model\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=1, input_dim=2)  # 2 features: intercept and X\n","    ])\n","    \n","    # Compile the model\n","    model.compile(optimizer='sgd', loss='mean_squared_error')\n","    \n","    # Fit the model\n","    model.fit(X_tf, y_tf, epochs=5, verbose=0)\n","    \n","    # Predictions\n","    y_pred_tf = model.predict(X_tf)\n","    \n","    # Calculate R-squared\n","    r_squared = r2_score(y, y_pred_tf)\n","    \n","    # Append results to the results list\n","    results_list.append({'Probe_ID_X': index,\n","                            'Probe_ID_Y': other_probe,\n","                            'R-squared': r_squared})\n","\n","# Convert the results list to a DataFrame\n","results_df_correlation = pd.DataFrame(results_list)\n","\n","# Print the results DataFrame\n","print(results_df_correlation.head())"]},{"cell_type":"markdown","metadata":{},"source":["### Below is DBSCAN clustering algorithm perform on whole dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create a StandardScaler instance\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(pivoted_samples)\n","\n","# Convert the scaled data back to a DataFrame\n","scaled_df = pd.DataFrame(scaled_data, columns=pivoted_samples.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["#### Set max cpu cores to lower cpu temperature"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Set this to the number of physical cores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Applying DBSCAN\n","from sklearn.cluster import DBSCAN\n","\n","# Create a DBSCAN clustering model\n","dbscan = DBSCAN(eps=0.3, min_samples=5)\n","\n","# Fit the model to your scaled data\n","clusters = dbscan.fit_predict(scaled_data)\n","\n","# Add the cluster labels to your DataFrame\n","scaled_df['Cluster'] = clusters\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df['Cluster'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the summary of values in the 'Cluster' column\n","cluster_summary = scaled_df['Cluster'].value_counts()\n","\n","# Print the summary\n","print(cluster_summary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Visualizing the Clusters\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(scaled_data)\n","\n","# Create a DataFrame with PCA results\n","pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n","pca_df['Cluster'] = clusters\n","\n","# Create a scatterplot to visualize the clusters\n","plt.figure(figsize=(10, 6))\n","plt.scatter(pca_df['PCA1'], pca_df['PCA2'], c=pca_df['Cluster'], cmap='viridis')\n","plt.title('DBSCAN Clustering Results (PCA)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(scaled_data)\n","\n","# Create a DataFrame with PCA results\n","pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n","pca_df['Cluster'] = clusters\n","\n","# Define a list of colors for each cluster\n","cluster_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'purple', 'orange', 'pink', 'brown', 'gray']\n","\n","# Create a scatterplot to visualize the clusters with specified colors, excluding cluster = -1\n","plt.figure(figsize=(10, 6))\n","for cluster_label, color in zip(pca_df['Cluster'].unique(), cluster_colors):\n","    if cluster_label != -1:  # Exclude cluster = -1\n","        cluster_data = pca_df[pca_df['Cluster'] == cluster_label]\n","        plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], c=color, label=f'Cluster {cluster_label}')\n","\n","plt.title('DBSCAN Clustering Results (PCA)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(scaled_data)\n","\n","# Create a DataFrame with PCA results\n","pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n","pca_df['Cluster'] = clusters\n","\n","# Define a list of colors for each cluster\n","cluster_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'purple', 'orange', 'pink', 'brown', 'gray']\n","\n","# Create a scatterplot to visualize the clusters with specified colors\n","plt.figure(figsize=(10, 6))\n","for cluster_label, color in zip(pca_df['Cluster'].unique(), cluster_colors):\n","    cluster_data = pca_df[pca_df['Cluster'] == cluster_label]\n","    plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], c=color, label=f'Cluster {cluster_label}')\n","\n","plt.title('DBSCAN Clustering Results (PCA)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Below is DBSCAN clustering algorithm performed on the filtered probes (filtered by R-squared)"]},{"cell_type":"markdown","metadata":{},"source":["#### Probes with R-squared > 0.7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create a StandardScaler instance\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data_07 = scaler.fit_transform(pivoted_samples_07)\n","\n","# Convert the scaled data back to a DataFrame\n","scaled_df_07 = pd.DataFrame(scaled_data_07, columns=pivoted_samples_07.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df_07.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Add the 'Probe_ID' column from pivoted_samples_07 to scaled_df_07\n","scaled_df_07['Probe_ID'] = pivoted_samples_07.index\n","\n","# Print the updated scaled DataFrame\n","print(scaled_df_07.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set the 'Probe_ID' as the index of scaled_df_07\n","scaled_df_07.set_index('Probe_ID', inplace=True)\n","\n","# Print the updated scaled DataFrame\n","print(scaled_df_07.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Set max cpu cores to lower cpu temperature"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Set this to the number of CPU's physical cores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Applying DBSCAN\n","from sklearn.cluster import DBSCAN\n","\n","# Create a DBSCAN clustering model\n","dbscan = DBSCAN(eps=8, min_samples=2)\n","\n","# Fit the model to your scaled data\n","clusters = dbscan.fit_predict(scaled_data_07)\n","\n","# Add the cluster labels to your DataFrame\n","scaled_df_07['Cluster'] = clusters\n","\n","# Add the cluster labes to pivoted_samples_07 dataframe\n","pivoted_samples_07['DBSCAN_Cluster'] = clusters\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pivoted_samples_07.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df_07['Cluster'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the summary of values in the 'Cluster' column\n","cluster_summary_07 = scaled_df_07['Cluster'].value_counts()\n","\n","# Print the summary\n","print(cluster_summary_07)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Visualizing the Clusters\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(scaled_data_07)\n","\n","# Create a DataFrame with PCA results\n","pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n","pca_df['Cluster'] = clusters\n","\n","# Create a scatterplot to visualize the clusters\n","plt.figure(figsize=(10, 6))\n","plt.scatter(pca_df['PCA1'], pca_df['PCA2'], c=pca_df['Cluster'], cmap='viridis')\n","plt.title('DBSCAN Clustering Results (PCA) (Probes with R-squared > 0.7)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Probes with R-squared > 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create a StandardScaler instance\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data_05 = scaler.fit_transform(pivoted_samples_05)\n","\n","# Convert the scaled data back to a DataFrame\n","scaled_df_05 = pd.DataFrame(scaled_data_05, columns=pivoted_samples_05.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Add the 'Probe_ID' column from pivoted_samples_07 to scaled_df_07\n","scaled_df_05['Probe_ID'] = pivoted_samples_05.index\n","\n","# Print the updated scaled DataFrame\n","print(scaled_df_05.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set the 'Probe_ID' as the index of scaled_df_07\n","scaled_df_05.set_index('Probe_ID', inplace=True)\n","\n","# Print the updated scaled DataFrame\n","print(scaled_df_05.head())"]},{"cell_type":"markdown","metadata":{},"source":["#### Set max cpu cores to lower cpu temperature"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Set this to the number of CPU's physical cores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Applying DBSCAN\n","from sklearn.cluster import DBSCAN\n","\n","# Create a DBSCAN clustering model\n","dbscan = DBSCAN(eps=8, min_samples=2)\n","\n","# Fit the model to your scaled data\n","clusters = dbscan.fit_predict(scaled_data_05)\n","4\n","# Add the cluster labels to your DataFrame\n","scaled_df_05['Cluster'] = clusters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df_05['Cluster'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the summary of values in the 'Cluster' column\n","cluster_summary_05 = scaled_df_05['Cluster'].value_counts()\n","\n","# Print the summary\n","print(cluster_summary_05)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Visualizing the Clusters\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(scaled_data_05)\n","\n","# Create a DataFrame with PCA results\n","pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n","pca_df['Cluster'] = clusters\n","\n","# Create a scatterplot to visualize the clusters\n","plt.figure(figsize=(10, 6))\n","plt.scatter(pca_df['PCA1'], pca_df['PCA2'], c=pca_df['Cluster'], cmap='viridis')\n","plt.title('DBSCAN Clustering Results (PCA) (Probes with R-squared > 0.5)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### HDBSCANS model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run the following commend in anaconda prompt to install hdbscan\n","# conda install -c conda-forge hdbscan"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the HDBSCAN library\n","import hdbscan"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Set this to the number of CPU's physical cores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an HDBSCAN clustering model\n","hdbscan_clusterer = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3)\n","\n","# Fit the model to your scaled data\n","hdbscan_clusters_07 = hdbscan_clusterer.fit_predict(scaled_data_07)\n","\n","# Add the cluster labels to your DataFrame\n","scaled_df_07['HDBSCAN_Cluster'] = hdbscan_clusters_07\n","\n","# Check scaled_df\n","print(scaled_df_07['HDBSCAN_Cluster'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaled_df_07)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the summary of values in the 'HDBSCAN_Cluster' column\n","hdbscan_cluster_summary = scaled_df_07['HDBSCAN_Cluster'].value_counts()\n","\n","# Print the summary\n","print(hdbscan_cluster_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Visualizing the Clusters\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the HDBSCAN clusters using PCA (similar to your DBSCAN visualization)\n","pca_hdbscan = PCA(n_components=2)\n","pca_hdbscan_result = pca_hdbscan.fit_transform(scaled_data_07)\n","\n","# Create a DataFrame with PCA results for HDBSCAN\n","pca_hdbscan_df = pd.DataFrame(data=pca_hdbscan_result, columns=['PCA1', 'PCA2'])\n","pca_hdbscan_df['HDBSCAN_Cluster'] = hdbscan_clusters_07\n","\n","# Create a scatterplot to visualize the HDBSCAN clusters\n","plt.figure(figsize=(10, 6))\n","plt.scatter(pca_hdbscan_df['PCA1'], pca_hdbscan_df['PCA2'], c=pca_hdbscan_df['HDBSCAN_Cluster'], cmap='viridis')\n","plt.title('HDBSCAN Clustering Results (PCA)')\n","plt.xlabel('PCA1')\n","plt.ylabel('PCA2')\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Create a scatter plot for each sample in each clusters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","population = np.random.rand(100)\n","Area = np.random.randint(100,600,100)\n","continent =['North America','Europe', 'Asia', 'Australia']*25\n","\n","df = pd.DataFrame(dict(population=population, Area=Area, continent = continent))\n","print(df)\n","\n","fig, ax = plt.subplots()\n","\n","colors = {'North America':'red', 'Europe':'green', 'Asia':'blue', 'Australia':'yellow'}\n","\n","\n","ax.scatter(df['population'], df['Area'], c=df['continent'].map(colors))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["##### Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pivoted_samples_07.columns)"]},{"cell_type":"markdown","metadata":{},"source":["#### Line plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","# Assuming you have a DataFrame sample_age_gender_data_40279 with columns 'Age' and 'Gender'\n","# Assuming you have a DataFrame pivoted_samples_07 representing the expression data with a 'DBSCAN_Cluster' column\n","\n","# Define a color map for different clusters\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n","\n","# Filter the DataFrame to include only samples in Cluster 1\n","cluster_to_plot = 1\n","cluster_data = pivoted_samples_07[pivoted_samples_07['DBSCAN_Cluster'] == cluster_to_plot]\n","print(cluster_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Delete the 'DBSCAN_Cluster' column\n","cluster_data = cluster_data.drop(columns=['DBSCAN_Cluster'])\n","print(cluster_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(cluster_data.index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a = []  # List to store age values\n","e = []  # List to store expression values\n","\n","for probe in cluster_data.index:\n","    # Generate a unique color for each probe within the cluster\n","    color = colors[cluster_to_plot % len(colors)]\n","    \n","    # Extract age and expression values for the corresponding samples\n","    age_values = sample_age_gender_data_40279.loc[:, 'Age']  # Exclude the 'DBSCAN_Cluster' column\n","    a.append(age_values.tolist())\n","    \n","    expression_values = cluster_data.loc[probe, :].values\n","    e.append(expression_values.tolist())\n","\n","# Print the lists of age and expression values\n","print(a)\n","print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a line plot for each probe, color-coded by cluster\n","plt.figure(figsize=(20, 10))\n","\n","for probe in cluster_data.index:\n","    # Generate a unique color for each probe within the cluster\n","    color = colors[cluster_to_plot % len(colors)]\n","    \n","    # Extract age and expression values for the corresponding samples\n","    age_values = sample_age_gender_data_40279.loc[cluster_data.columns]['Age']\n","    expression_values = cluster_data.loc[probe, :].values\n","    \n","    plt.plot(age_values, expression_values, label=f'Cluster {cluster_to_plot}, Probe {probe}', color=color)\n","\n","# Customize the plot\n","plt.title(f'Expression Profiles for Cluster {cluster_to_plot}')\n","plt.xlabel('Age')\n","plt.ylabel('Expression Value')\n","\n","# Create a custom legend for the cluster\n","plt.legend(title=f'Cluster {cluster_to_plot}', bbox_to_anchor=(1.05, 1), loc='upper left')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Scatter plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","# Assuming you have a DataFrame sample_age_gender_data with columns 'Age' and 'Gender'\n","# Assuming you have a DataFrame pivoted_samples_07 representing the expression data with a 'DBSCAN_Cluster' column\n","\n","# Define a color map for different probes within Cluster 1\n","probe_colors = plt.cm.rainbow(np.linspace(0, 1, len(cluster_data.index)))\n","\n","# Create a scatter plot for each probe, color-coded by probe within Cluster 1\n","plt.figure(figsize=(50, 30))\n","\n","for i, probe in enumerate(cluster_data.index):\n","    # Extract age and expression values for the corresponding samples\n","    age_values = sample_age_gender_data_40279.loc[cluster_data.columns]['Age']\n","    expression_values = cluster_data.loc[probe, :].values\n","    \n","    # Scatter plot with a unique color for each probe\n","    plt.scatter(age_values, expression_values, label=f'Probe {probe}', color=probe_colors[i])\n","\n","# Customize the plot\n","plt.title(f'Expression Profiles for Cluster {cluster_to_plot}')\n","plt.xlabel('Age')\n","plt.ylabel('Expression Value')\n","\n","# Create a custom legend for the probes\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Box plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","# Assuming you have a DataFrame sample_age_gender_data with columns 'Age' and 'Gender'\n","# Assuming you have a DataFrame pivoted_samples_07 representing the expression data with a 'DBSCAN_Cluster' column\n","\n","# Filter the DataFrame to include only samples in Cluster 1\n","cluster_to_plot = 1\n","cluster_data = pivoted_samples_07[pivoted_samples_07['DBSCAN_Cluster'] == cluster_to_plot]\n","\n","# Create box plots for all probes in Cluster 1\n","plt.figure(figsize=(20, 10))\n","\n","# Create a list to store expression values for each probe\n","expression_values_list = []\n","\n","for probe in cluster_data.index:\n","    expression_values = cluster_data.loc[probe, :].values\n","    expression_values_list.append(expression_values)\n","\n","# Create a box plot for all probes within Cluster 1\n","plt.boxplot(expression_values_list, labels=cluster_data.index)\n","\n","# Customize the plot\n","plt.title(f'Box Plots for Expression Profiles in Cluster {cluster_to_plot}')\n","plt.xlabel('Probe')\n","plt.ylabel('Expression Value')\n","\n","# Show the plot\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Violin plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","\n","# Assuming you have a DataFrame sample_age_gender_data with columns 'Age' and 'Gender'\n","# Assuming you have a DataFrame pivoted_samples_07 representing the expression data with a 'DBSCAN_Cluster' column\n","\n","# Filter the DataFrame to include only samples in Cluster 1\n","cluster_to_plot = 1\n","cluster_data = pivoted_samples_07[pivoted_samples_07['DBSCAN_Cluster'] == cluster_to_plot]\n","\n","# Create a violin plot for all probes in Cluster 1\n","plt.figure(figsize=(20, 10))\n","\n","# Create a DataFrame for violin plot\n","violin_data = pd.DataFrame(expression_values_list).T\n","\n","# Rename columns to probe names\n","violin_data.columns = cluster_data.index\n","\n","# Create a violin plot using seaborn\n","sns.violinplot(data=violin_data, palette='husl')\n","\n","# Customize the plot\n","plt.title(f'Violin Plot for Expression Profiles in Cluster {cluster_to_plot}')\n","plt.xlabel('Probe')\n","plt.ylabel('Expression Value')\n","\n","# Show the plot\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","\n","# Assuming you have a DataFrame sample_age_gender_data with columns 'Age' and 'Gender'\n","# Assuming you have a DataFrame pivoted_samples_07 representing the expression data with a 'DBSCAN_Cluster' column\n","\n","# Filter the DataFrame to include only samples in Cluster 2\n","cluster_to_plot = 2\n","cluster_data = pivoted_samples_07[pivoted_samples_07['DBSCAN_Cluster'] == cluster_to_plot]\n","\n","# Create a violin plot for all probes in Cluster 2\n","plt.figure(figsize=(20, 10))\n","\n","# Create a DataFrame for violin plot\n","violin_data = pd.DataFrame(expression_values_list).T\n","\n","# Rename columns to probe names\n","violin_data.columns = cluster_data.index\n","\n","# Create a violin plot using seaborn\n","sns.violinplot(data=violin_data, palette='husl')\n","\n","# Customize the plot\n","plt.title(f'Violin Plot for Expression Profiles in Cluster {cluster_to_plot}')\n","plt.xlabel('Probe')\n","plt.ylabel('Expression Value')\n","\n","# Show the plot\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### MLP"]},{"cell_type":"markdown","metadata":{},"source":["#### MLP  by scikit-learn to predict age"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 94.11360375425754\n","R-squared: 0.594397716176997\n","[72.52851661 14.30623286 73.78131487 64.60850871 76.14706312 78.90954242\n"," 71.75783371 72.04155981 75.9578631  71.67638468 65.43981611 70.62094468\n"," 77.24489338 69.85461769 86.40099559 70.64215859 74.62111664 77.16814605\n"," 79.94954023 81.28264384 72.14699503 17.59677981 17.53995648 14.21073559\n"," 18.87939917 18.16135253 22.96421906 13.47366454 14.07870263 24.8520017\n"," 19.47534375 19.59582545 20.96378221 18.55194475 14.62465229 18.54770933\n"," 13.94216365 12.46296024 19.14676212 20.50801691]\n","           Predicted age (MLP) Chronological age\n","GSM765860            72.528517         103 years\n","GSM765861            14.306233           Newborn\n","GSM765862            73.781315          97 years\n","GSM765863            64.608509          95 years\n","GSM765864            76.147063          97 years\n","GSM765865            78.909542          97 years\n","GSM765866            71.757834          98 years\n","GSM765867            72.041560          96 years\n","GSM765868            75.957863         100 years\n","GSM765869            71.676385          90 years\n","GSM765870            65.439816          91 years\n","GSM765871            70.620945          92 years\n","GSM765872            77.244893          89 years\n","GSM765873            69.854618          90 years\n","GSM765874            86.400996          89 years\n","GSM765875            70.642159          90 years\n","GSM765876            74.621117          91 years\n","GSM765877            77.168146          89 years\n","GSM765878            79.949540          89 years\n","GSM765879            81.282644          90 years\n","GSM765880            72.146995          90 years\n","GSM765881            17.596780           Newborn\n","GSM765882            17.539956           Newborn\n","GSM765883            14.210736           Newborn\n","GSM765884            18.879399           Newborn\n","GSM765885            18.161353           Newborn\n","GSM765886            22.964219           Newborn\n","GSM765887            13.473665           Newborn\n","GSM765888            14.078703           Newborn\n","GSM765889            24.852002           Newborn\n","GSM765890            19.475344           Newborn\n","GSM765891            19.595825           Newborn\n","GSM765892            20.963782           Newborn\n","GSM765893            18.551945           Newborn\n","GSM765894            14.624652           Newborn\n","GSM765895            18.547709           Newborn\n","GSM765896            13.942164           Newborn\n","GSM765897            12.462960           Newborn\n","GSM765898            19.146762           Newborn\n","GSM765899            20.508017           Newborn\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n","c:\\Users\\tokyo\\anaconda3\\envs\\tensorflow_use\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n","  warnings.warn(\n"]}],"source":["# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","y = sample_age_gender_data_40279['Age']\n","\n","# Split the data into training and testing sets (you can adjust the test_size)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","# Set hyperparameters\n","hidden_layer_sizes = (5000, 5000, 1000, 500, 100, 53)\n","activation = 'relu'\n","solver = 'adam'\n","learning_rate = 'adaptive'\n","max_iter = 1000\n","batch_size = 'auto'\n","alpha = 0.0001\n","early_stopping = True\n","random_state = 42\n","\n","# Create MLPRegressor\n","mlp_model = MLPRegressor(\n","    hidden_layer_sizes=hidden_layer_sizes,\n","    activation=activation,\n","    solver=solver,\n","    learning_rate=learning_rate,\n","    max_iter=max_iter,\n","    batch_size=batch_size,\n","    alpha=alpha,\n","    early_stopping=early_stopping,\n","    random_state=random_state\n",")\n","\n","# Fit the model to the training data\n","mlp_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = mlp_model.predict(X_test)\n","\n","# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)\n","\n","# Now you can use the trained MLP model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","imputer = SimpleImputer(strategy='mean')\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","\n","predicted_age_mlp = mlp_model.predict(X_new_data_to_predict)\n","print(predicted_age_mlp)\n","\n","# Comparison with Chronological Age\n","Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","\n","data_mlp = {\n","    'Predicted age (MLP)': predicted_age_mlp,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870_mlp = pd.DataFrame(data_mlp)\n","print(predicted_30870_mlp)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove ' years' from Chronological age and replace 'Newborn' with 1\n","predicted_30870_mlp['Chronological age'] = predicted_30870_mlp['Chronological age'].str.replace(' years', '').replace('Newborn', '0')\n","\n","# Convert the Chronological age column to numeric\n","predicted_30870_mlp['Chronological age'] = pd.to_numeric(predicted_30870_mlp['Chronological age'])\n","\n","# Print the updated DataFrame\n","print(predicted_30870_mlp)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_30870_mlp['Chronological age']\n","Predicted = predicted_30870_mlp['Predicted age (MLP)']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"markdown","metadata":{},"source":["#### MLP by tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{},"source":["##### Construct model and training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n","# Impute missing values (NaN) with mean values for Expression\n","X = imputer.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the age values for the dataset\n","# y = combined_age_data['Age']\n","y = sample_age_gender_data_40279['Age']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","# Convert the target variable to a numeric type\n","y_train = y_train.astype('float32')\n","y_test = y_test.astype('float32')\n","\n","# Standardize the input features (important for neural networks)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Create a Sequential model\n","model = Sequential()\n","\n","'''\n","# Add hidden layers to the model\n","model.add(Dense(units=256, activation='relu', input_dim=X_train_scaled.shape[1]))\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dense(units=1, activation='linear'))  # Output layer with linear activation for regression\n","'''\n","\n","# Add hidden layers to the model with Batch Normalization\n","model.add(Dense(units=5120, input_dim=X_train_scaled.shape[1]))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=5120))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=2560))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=256))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=1, activation='linear'))  # Output layer with linear activation for regression\n","\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n","\n","# Train the model\n","# model.fit(X_train_scaled, y_train, epochs=500, batch_size=32, validation_data=(X_test_scaled, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model with callbacks\n","model.fit(\n","    X_train_scaled, y_train,\n","    epochs=500,\n","    batch_size=32,\n","    validation_data=(X_test_scaled, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","# Define EarlyStopping callback\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',  # Use validation loss as the metric to monitor\n","    patience=50,          # Number of epochs with no improvement after which training will be stopped\n","    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","# Define ReduceLROnPlateau callback for adaptive learning rate\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',  # Use validation loss as the metric to monitor\n","    factor=0.5,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n","    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-3           # Lower bound on the learning rate\n",")\n","\n","# Train the model with callbacks\n","model.fit(\n","    X_train_scaled, y_train,\n","    epochs=1000,\n","    batch_size=32,\n","    validation_data=(X_test_scaled, y_test),\n","    callbacks=[early_stopping, reduce_lr]  # Use both EarlyStopping and ReduceLROnPlateau\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### Check tensorflow is using GPU to train MLP model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","# List all available devices, including GPUs\n","print(tf.config.list_physical_devices('GPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","tf.test.is_built_with_cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","# Explicitly set the GPU device\n","physical_devices = tf.config.list_physical_devices('GPU')\n","print(physical_devices)\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Model summary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["##### MSE and R-squared for y_pred and y_test on X_scaled"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test data\n","y_pred = model.predict(X_test_scaled)\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pivoted_samples_30870.T\n","data"]},{"cell_type":"markdown","metadata":{},"source":["##### Predict on additional dataset "]},{"cell_type":"markdown","metadata":{},"source":["###### Impute missing value by KNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer, KNNImputer\n","\n","# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Standardize the input features (important for neural networks)\n","scaler = StandardScaler()\n","imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","# X_new_data_KNNimputed_to_predict = imputer.transform(X_new_data_to_predict)\n","\n","# Standardize the new data using the same scaler\n","X_new_data_to_predict_KNNimputed_scaled = scaler.fit_transform(X_new_data_to_predict)\n","\n","# Make predictions on the new data (Imputed and standardized)\n","predicted_KNN = model.predict(X_new_data_to_predict_KNNimputed_scaled)\n","\n","# Print the predicted ages\n","print(predicted_KNN)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Flatten the predicted_age_mlp_tensorflow array\n","predicted_age_flattened = predicted_KNN.flatten()\n","\n","# Comparison with Chronological Age\n","Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","\n","# Create the DataFrame\n","data_mlp_tensorflow = {\n","    'Predicted age (MLP)': predicted_age_flattened,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870_mlp_KNN = pd.DataFrame(data_mlp_tensorflow)\n","print(predicted_30870_mlp_KNN)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove ' years' from Chronological age and replace 'Newborn' with 1\n","predicted_30870_mlp_KNN['Chronological age'] = predicted_30870_mlp_KNN['Chronological age'].str.replace(' years', '').replace('Newborn', '0')\n","\n","# Convert the Chronological age column to numeric\n","predicted_30870_mlp_KNN['Chronological age'] = pd.to_numeric(predicted_30870_mlp_KNN['Chronological age'])\n","\n","# Print the updated DataFrame\n","print(predicted_30870_mlp_KNN)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_30870_mlp_KNN['Chronological age']\n","Predicted = predicted_30870_mlp_KNN['Predicted age (MLP)']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"markdown","metadata":{},"source":["###### Impute missing values by mean"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","imputer = SimpleImputer(strategy='mean')\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","\n","# Standardize the new data using the same scaler\n","X_new_data_to_predict_scaled = scaler.fit_transform(X_new_data_to_predict)\n","\n","# Make predictions on the new data\n","predicted_mean = model.predict(X_new_data_to_predict_scaled)\n","\n","# Print the predicted ages\n","print(predicted_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Flatten the predicted_age_mlp_tensorflow array\n","predicted_age_flattened = predicted_mean.flatten()\n","\n","# Comparison with Chronological Age\n","Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","\n","# Create the DataFrame\n","data_mlp_mean_tensorflow = {\n","    'Predicted age (MLP)': predicted_age_flattened,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870_mlp_mean = pd.DataFrame(data_mlp_mean_tensorflow)\n","print(predicted_30870_mlp_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove ' years' from Chronological age and replace 'Newborn' with 1\n","predicted_30870_mlp_mean['Chronological age'] = predicted_30870_mlp_mean['Chronological age'].str.replace(' years', '').replace('Newborn', '0')\n","\n","# Convert the Chronological age column to numeric\n","predicted_30870_mlp_mean['Chronological age'] = pd.to_numeric(predicted_30870_mlp_mean['Chronological age'])\n","\n","# Print the updated DataFrame\n","print(predicted_30870_mlp_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_30870_mlp_mean['Chronological age']\n","Predicted = predicted_30870_mlp_mean['Predicted age (MLP)']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"markdown","metadata":{},"source":["#### CNN by tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean')\n","# Impute missing values (NaN) with mean values for Expression\n","X = imputer.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reshape data for Conv1D (assuming each row corresponds to a sample)\n","# X = X.values.reshape(X.shape[0], X.shape[1], 1)\n","# X = X.reshape(X.shape[0], X.shape[1], 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the age values for the dataset and convert to numeric\n","# y = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n","\n","y = combined_age_data['Age']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","y_train = y_train.astype('float32')\n","y_test = y_test.astype('float32')\n","\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.fit_transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","# Create a Sequential model\n","model = Sequential()\n","\n","# Add Conv1D layer\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Flatten())\n","\n","# Add dense layers\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Dense(units=256, activation='relu'))\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dense(units=1, activation='linear'))  # Output layer with linear activation for regression\n","\n","'''\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","'''\n","# Compile the model with an initial learning rate\n","initial_learning_rate = 0.001\n","optimizer = Adam(learning_rate=initial_learning_rate)\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","# Train the model\n","# model.fit(X_train_scaled, y_train, epochs=500, batch_size=32, validation_data=(X_test_scaled, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the age values for the dataset and convert to numeric\n","y = pd.to_numeric(sample_age_gender_data_40279['Age'], errors='coerce')\n","\n","# y = combined_age_data['Age']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","y_train = y_train.astype('float32')\n","y_test = y_test.astype('float32')\n","\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.fit_transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","# Create a Sequential model\n","model = Sequential()\n","\n","# Add Conv1D layer with Batch Normalization\n","model.add(Conv1D(filters=32, kernel_size=2, input_shape=(X_train_scaled.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","# Add another Conv1D layer with Batch Normalization\n","model.add(Conv1D(filters=32, kernel_size=2))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","'''\n","# Add another Conv1D layer with Batch Normalization\n","model.add(Conv1D(filters=64, kernel_size=3))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","'''\n","\n","# Flatten before fully connected layers\n","model.add(Flatten())\n","\n","# Add dense layers with Batch Normalization\n","model.add(Dense(units=256))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=256))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=256))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","model.add(Dense(units=1, activation='linear'))  # Output layer with linear activation for regression\n","\n","# Compile the model with an initial learning rate\n","initial_learning_rate = 0.001\n","optimizer = Adam(learning_rate=initial_learning_rate)\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Advance way of adaptive learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define EarlyStopping callback\n","early_stopping = EarlyStopping(\n","    monitor='loss',  # Use validation loss as the metric to monitor\n","    patience=30,          # Number of epochs with no improvement after which training will be stopped\n","    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","# Define ReduceLROnPlateau callback for adaptive learning rate\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='loss',  # Use validation loss as the metric to monitor\n","    factor=0.5,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n","    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-7           # Lower bound on the learning rate\n",")\n","\n","'''\n","# Train the model with callbacks\n","trained_model = model.fit(\n","    X_train_scaled, y_train,\n","    epochs=1000,\n","    batch_size=32,\n","    validation_data=(X_test_scaled, y_test),\n","    callbacks=[early_stopping, reduce_lr]  # Use both EarlyStopping and ReduceLROnPlateau\n",")\n","'''\n","model.fit(\n","    X_train_scaled, y_train,\n","    epochs=1000,\n","    batch_size=64,\n","    validation_data=(X_test_scaled, y_test),\n","    callbacks=[early_stopping, reduce_lr]  # Use both EarlyStopping and ReduceLROnPlateau\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### Simple way of adaptive learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Define a simple learning rate schedule\n","def lr_schedule(epoch):\n","    return 0.001 * 0.9 ** epoch  # Adjust the function as needed\n","\n","# Create a LearningRateScheduler callback\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","# Define EarlyStopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n","\n","# During model.fit(), add the lr_scheduler callback\n","model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[lr_scheduler, early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the summary of the model\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the input shape from the first layer\n","input_shape = model.layers[0].input_shape[1:]\n","\n","# Print the input shape\n","print(\"Input shape:\", input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test data\n","y_pred = model.predict(X_test_scaled)\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_50660\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","imputer = SimpleImputer(strategy='mean')\n","\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","\n","# Standardize the new data using the same scaler\n","X_new_data_to_predict_scaled = scaler.transform(X_new_data_to_predict)\n","\n","# Reshape data for CNN input\n","# X_new_data_to_predict_reshaped = X_new_data_to_predict_scaled.reshape((X_new_data_to_predict_scaled.shape[0], X_new_data_to_predict_scaled.shape[1], 1))\n","\n","# Make predictions on the new data\n","predicted_age_cnn_tensorflow = model.predict(X_new_data_to_predict_scaled)\n","\n","# Print the predicted ages\n","print(predicted_age_cnn_tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Flatten the predicted_age_mlp_tensorflow array\n","predicted_age_flattened = predicted_age_cnn_tensorflow.flatten()\n","\n","# Comparison with Chronological Age\n","Chronological_age_50660 = sample_age_data_50660['Age']\n","\n","# Create the DataFrame\n","data_cnn_tensorflow = {\n","    'Predicted age (CNN)': predicted_age_flattened,\n","    'Chronological age': Chronological_age_50660\n","}\n","\n","predicted_50660_cnn_tensorflow = pd.DataFrame(data_cnn_tensorflow)\n","print(predicted_50660_cnn_tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","# Assuming predicted_50660_cnn_tensorflow is a DataFrame\n","predicted_50660_cnn_tensorflow = predicted_50660_cnn_tensorflow.sort_values(by='Chronological age', ascending=True)\n","\n","x_coordinate = predicted_50660_cnn_tensorflow['Chronological age']\n","y_coordinate = predicted_50660_cnn_tensorflow['Predicted age (CNN)']\n","plt.scatter(x_coordinate, y_coordinate)\n","\n","plt.title(f\"Scatter plot for gse50660\")\n","plt.xlabel('Chronological age')\n","plt.ylabel('Predicted age (CNN)')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_50660_cnn_tensorflow['Chronological age']\n","Predicted = predicted_50660_cnn_tensorflow['Predicted age (CNN)']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_50660\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","imputer = SimpleImputer(strategy='mean')\n","\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.fit_transform(X_new_data_to_predict)\n","\n","# Standardize the new data using the same scaler\n","X_new_data_to_predict_scaled = scaler.transform(X_new_data_to_predict)\n","\n","# Reshape data for CNN input\n","# X_new_data_to_predict_reshaped = X_new_data_to_predict_scaled.reshape((X_new_data_to_predict_scaled.shape[0], X_new_data_to_predict_scaled.shape[1], 1))\n","\n","# Make predictions on the new data\n","predicted_30870_age_cnn_tensorflow = model.predict(X_new_data_to_predict_scaled)\n","\n","# Print the predicted ages\n","print(predicted_30870_age_cnn_tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Flatten the predicted_age_mlp_tensorflow array\n","predicted_30870_age_flattened = predicted_30870_age_cnn_tensorflow.flatten()\n","\n","# Comparison with Chronological Age\n","Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","\n","# Create the DataFrame\n","data_30870_cnn_tensorflow = {\n","    'Predicted age (CNN)': predicted_30870_age_flattened,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870_cnn_tensorflow = pd.DataFrame(data_30870_cnn_tensorflow)\n","print(predicted_30870_cnn_tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove ' years' from Chronological age and replace 'Newborn' with 1\n","predicted_30870_cnn_tensorflow['Chronological age'] = predicted_30870_cnn_tensorflow['Chronological age'].str.replace(' years', '').replace('Newborn', '0')\n","\n","# Convert the Chronological age column to numeric\n","predicted_30870_cnn_tensorflow['Chronological age'] = pd.to_numeric(predicted_30870_cnn_tensorflow['Chronological age'])\n","\n","# Print the updated DataFrame\n","print(predicted_30870_cnn_tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate evaluation metrics (e.g., Mean Squared Error and R-squared)\n","Chronological = predicted_30870_cnn_tensorflow['Chronological age']\n","Predicted = predicted_30870_cnn_tensorflow['Predicted age (CNN)']\n","\n","mse = mean_squared_error(Chronological, Predicted)\n","r_squared = r2_score(Chronological, Predicted)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"markdown","metadata":{},"source":["### Transformer by tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Embedding, LayerNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","y = sample_age_gender_data_40279['Age']\n","\n","# Convert the target variable to a numeric type\n","y = y.astype('float32')\n","\n","# Reshape data for Transformer (assuming each row corresponds to a sample)\n","X = X.values.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","def transformer_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = Embedding(input_dim=max(input_shape[1], 54), output_dim=64)(inputs)  # Adjusted input_dim\n","    x = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","    x = Dense(units=256, activation='relu')(x)\n","    x = Dense(units=256, activation='relu')(x)\n","    x = Dense(units=53, activation='relu')(x)\n","    x = Dense(units=1, activation='linear')(x)\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n","\n","# Create Transformer model\n","model = transformer_model((X_train_scaled.shape[1], 1))\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n","\n","# Train the model\n","model.fit(X_train_scaled, y_train, epochs=500, batch_size=32, validation_data=(X_test_scaled, y_test))\n","\n","# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.transform(X_new_data_to_predict)\n","\n","# Reshape and standardize the new data\n","X_new_data_to_predict_scaled = scaler.transform(X_new_data_to_predict)\n","\n","# Make predictions on the new data\n","predicted_age = model.predict(X_new_data_to_predict_scaled)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the predicted age\n","print(predicted_age)"]},{"cell_type":"markdown","metadata":{},"source":["#### Fix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Embedding, LayerNormalization, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","y = sample_age_gender_data_40279['Age']\n","\n","# Convert the target variable to a numeric type\n","y = y.astype('float32')\n","\n","# Reshape data for Transformer (assuming each row corresponds to a sample)\n","X = X.values.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","def transformer_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = Embedding(input_dim=max(input_shape[1], 54), output_dim=64)(inputs)  # Adjusted input_dim\n","    x = MultiHeadAttention(num_heads=30, key_dim=256)(x, x)\n","    x = LayerNormalization(epsilon=1e-10)(x)\n","    x = Flatten()(x)  # Added Flatten layer\n","    #x = Dense(units=256, activation='relu')(x)\n","    #x = Dense(units=256, activation='relu')(x)\n","    #x = Dense(units=128, activation='relu')(x)\n","    #x = Dense(units=53, activation='relu')(x)\n","    x = Dense(units=1, activation='linear')(x)\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n","\n","# Create Transformer model\n","model = transformer_model((X_train_scaled.shape[1], 1))\n","\n","# Compile the model with an initial learning rate\n","initial_learning_rate = 0.01\n","optimizer = Adam(learning_rate=initial_learning_rate)\n","model.compile(optimizer=optimizer, loss='mean_squared_error')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define EarlyStopping callback\n","early_stopping = EarlyStopping(\n","    monitor='loss',  # Use validation loss as the metric to monitor\n","    patience=100,          # Number of epochs with no improvement after which training will be stopped\n","    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","# Define ReduceLROnPlateau callback for adaptive learning rate\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',  # Use validation loss as the metric to monitor\n","    factor=0.5,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n","    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-7           # Lower bound on the learning rate\n",")\n","\n","'''\n","# Train the model with callbacks\n","trained_model = model.fit(\n","    X_train_scaled, y_train,\n","    epochs=500,\n","    batch_size=32,\n","    validation_data=(X_test_scaled, y_test),\n","    callbacks=[early_stopping, reduce_lr]  # Use both EarlyStopping and ReduceLROnPlateau\n",")\n","'''\n","# Train the model with callbacks\n","model.fit(\n","    X_train_scaled, y_train,\n","    epochs=500,\n","    batch_size=64,\n","    validation_data=(X_test_scaled, y_test),\n","    callbacks=[early_stopping, reduce_lr]  # Use both EarlyStopping and ReduceLROnPlateau\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test data\n","y_pred = model.predict(X_test_scaled)\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.transform(X_new_data_to_predict)\n","\n","# Reshape and standardize the new data\n","X_new_data_to_predict_scaled = scaler.transform(X_new_data_to_predict)\n","\n","# Make predictions on the new data\n","predicted_age_transformer = model.predict(X_new_data_to_predict_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the predicted age\n","print(predicted_age_transformer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Flatten the predicted_age_mlp_tensorflow array\n","predicted_age_flattened = predicted_age_transformer.flatten()\n","\n","# Comparison with Chronological Age\n","Chronological_age_30870 = gse30870.phenotype_data['characteristics_ch1.0.age']\n","\n","# Create the DataFrame\n","data_transformer_tensorflow = {\n","    'Predicted age (Transformer)': predicted_age_flattened,\n","    'Chronological age': Chronological_age_30870\n","}\n","\n","predicted_30870_transformer_tensorflow = pd.DataFrame(data_transformer_tensorflow)\n","print(predicted_30870_transformer_tensorflow)"]},{"cell_type":"markdown","metadata":{},"source":["### Transformer and MLP head model to predict age"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["beta_values_df = pivoted_samples_07.T\n","age_df = sample_age_gender_data_40279['Age']\n","\n","# Convert dataframes to NumPy arrays\n","beta_values = beta_values_df.values\n","age_labels = age_df.values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(beta_values)\n","print(len(beta_values))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(beta_values.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(age_labels)\n","print(len(age_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into training and testing sets (you can adjust the split ratio)\n","from sklearn.model_selection import train_test_split\n","\n","beta_train, beta_test, age_train, age_test = train_test_split(\n","    beta_values, age_labels, test_size=0.2, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import BatchNormalization, Flatten\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout=0.1, mlp_dropout=0.1):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","\n","    # Create multiple TransformerEncoder blocks\n","    for _ in range(num_blocks):\n","        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n","\n","    # Flatten the output instead of GlobalAveragePooling1D\n","    x = Flatten()(x)\n","\n","    # BatchNormalization for better convergence\n","    x = BatchNormalization()(x)\n","    \n","    # MLP for age prediction\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","\n","    age_output = layers.Dense(1, activation=\"linear\")(x)\n","\n","    return keras.Model(inputs, age_output, name=\"transformer_age_prediction\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create and compile the Transformer-based age prediction model\n","embed_dim = 54  # Embedding dimension\n","num_heads = 4  # Number of attention heads\n","ff_dim = 656  # Feedforward dimension in the TransformerEncoder\n","num_blocks = 4  # Number of TransformerEncoder blocks\n","mlp_units = [128]  # MLP layer sizes\n","mlp_dropout = 0.4  # MLP dropout rate\n","dropout = 0.1  # Dropout rate\n","model = create_transformer_model(\n","    input_shape=(beta_values.shape[1],),\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    ff_dim=ff_dim,\n","    num_blocks=num_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")\n","\n","model.compile(\n","    loss=\"mean_squared_error\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"mean_squared_error\"],\n",")\n","\n","# Define a custom data generator\n","def data_generator(beta_values, age_labels, batch_size=32):\n","    num_samples = len(beta_values)\n","    while True:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        for start in range(0, num_samples, batch_size):\n","            batch_indices = indices[start : start + batch_size]\n","            yield beta_values[batch_indices], age_labels[batch_indices]\n","\n","# Create the data generators\n","batch_size = 32\n","train_data_generator = data_generator(beta_train, age_train, batch_size)\n","test_data_generator = data_generator(beta_test, age_test, batch_size)\n","\n","# Train the model\n","epochs = 10  # Adjust the number of epochs as needed\n","model.fit(train_data_generator, epochs=epochs, steps_per_epoch=len(beta_train) // batch_size)\n","\n","# Evaluate the model on the test set\n","test_loss = model.evaluate(test_data_generator, steps=len(beta_test) // batch_size)\n","print(f'Test loss: {test_loss}')\n","\n","# Make age predictions on new data\n","# Replace 'new_data.csv' with your new data file\n","#new_data_df = pd.read_csv('new_data.csv', index_col=0)\n","#new_beta_values = new_data_df.values\n","#age_predictions = model.predict(new_beta_values)\n","\n","# age_predictions now contains the predicted ages for the new data\n","#print(age_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Assuming beta_values, age_labels are already defined\n","\n","# Split the data into training and testing sets\n","beta_train, beta_test, age_train, age_test = train_test_split(\n","    beta_values, age_labels, test_size=0.2, random_state=42\n",")\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","beta_train_scaled = scaler.fit_transform(beta_train)\n","beta_test_scaled = scaler.transform(beta_test)\n","\n","# Define the TransformerEncoder layer\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Define the Transformer-based age prediction model\n","def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout=0.1, mlp_dropout=0.1):\n","    inputs = tf.keras.Input(shape=input_shape)\n","    x = inputs\n","\n","    # Create multiple TransformerEncoder blocks\n","    for _ in range(num_blocks):\n","        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(mlp_dropout)(x)\n","    \n","    # MLP for age prediction\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","\n","    age_output = layers.Dense(1, activation=\"linear\")(x)\n","\n","    return tf.keras.Model(inputs, age_output, name=\"transformer_age_prediction\")\n","\n","# Create and compile the Transformer-based age prediction model\n","embed_dim = 54  # Embedding dimension\n","num_heads = 4  # Number of attention heads\n","ff_dim = 656  # Feedforward dimension in the TransformerEncoder\n","num_blocks = 4  # Number of TransformerEncoder blocks\n","mlp_units = [128]  # MLP layer sizes\n","mlp_dropout = 0.4  # MLP dropout rate\n","dropout = 0.1  # Dropout rate\n","model = create_transformer_model(\n","    input_shape=(beta_train_scaled.shape[1],),\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    ff_dim=ff_dim,\n","    num_blocks=num_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")\n","\n","# Compile the model with an initial learning rate\n","initial_learning_rate = 0.1\n","optimizer = Adam(learning_rate=initial_learning_rate)\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","# Define EarlyStopping callback\n","early_stopping = EarlyStopping(\n","    monitor='loss',  # Use training loss as the metric to monitor\n","    patience=10,          # Number of epochs with no improvement after which training will be stopped\n","    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","# Define ReduceLROnPlateau callback for adaptive learning rate\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='loss',  # Use training loss as the metric to monitor\n","    factor=0.5,           # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n","    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-7           # Lower bound on the learning rate\n",")\n","\n","# Train the model with callbacks\n","epochs = 50\n","batch_size = 32\n","model.fit(\n","    beta_train_scaled, age_train,\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    callbacks=[early_stopping, reduce_lr],  # Use both EarlyStopping and ReduceLROnPlateau\n",")\n","\n","# Evaluate the model on the test set\n","test_loss = model.evaluate(beta_test_scaled, age_test)\n","print(f'Test loss: {test_loss}')\n","\n","# Make age predictions on new data\n","# Replace 'new_data.csv' with your new data file\n","# new_data_df = pd.read_csv('new_data.csv', index_col=0)\n","# new_beta_values = new_data_df.values\n","# new_beta_values_scaled = scaler.transform(new_beta_values)\n","# age_predictions = model.predict(new_beta_values_scaled)\n","\n","# age_predictions now contains the predicted ages for the new data\n","# print(age_predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the TransformerEncoder layer\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Define the Transformer-based age prediction model\n","def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout=0.1, mlp_dropout=0.1):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","\n","    # Create multiple TransformerEncoder blocks\n","    for _ in range(num_blocks):\n","        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(mlp_dropout)(x)\n","    \n","    # MLP for age prediction\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","\n","    age_output = layers.Dense(1, activation=\"linear\")(x)\n","\n","    return keras.Model(inputs, age_output, name=\"transformer_age_prediction\")\n","\n","# Create and compile the Transformer-based age prediction model\n","embed_dim = 54  # Embedding dimension\n","num_heads = 4  # Number of attention heads\n","ff_dim = 656  # Feedforward dimension in the TransformerEncoder\n","num_blocks = 4  # Number of TransformerEncoder blocks\n","mlp_units = [128]  # MLP layer sizes\n","mlp_dropout = 0.4  # MLP dropout rate\n","dropout = 0.1  # Dropout rate\n","model = create_transformer_model(\n","    input_shape=(beta_values.shape[1],),\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    ff_dim=ff_dim,\n","    num_blocks=num_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")\n","\n","model.compile(\n","    loss=\"mean_squared_error\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"mean_squared_error\"],\n",")\n","\n","# Define a custom data generator\n","def data_generator(beta_values, age_labels, batch_size=32):\n","    num_samples = len(beta_values)\n","    while True:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        for start in range(0, num_samples, batch_size):\n","            batch_indices = indices[start : start + batch_size]\n","            yield beta_values[batch_indices], age_labels[batch_indices]\n","\n","# Create the data generators\n","batch_size = 32\n","train_data_generator = data_generator(beta_train, age_train, batch_size)\n","test_data_generator = data_generator(beta_test, age_test, batch_size)\n","\n","# Train the model\n","epochs = 10  # Adjust the number of epochs as needed\n","model.fit(train_data_generator, epochs=epochs, steps_per_epoch=len(beta_train) // batch_size)\n","\n","# Evaluate the model on the test set\n","test_loss = model.evaluate(test_data_generator, steps=len(beta_test) // batch_size)\n","print(f'Test loss: {test_loss}')\n","\n","# Make age predictions on new data\n","# Replace 'new_data.csv' with your new data file\n","#new_data_df = pd.read_csv('new_data.csv', index_col=0)\n","#new_beta_values = new_data_df.values\n","#age_predictions = model.predict(new_beta_values)\n","\n","# age_predictions now contains the predicted ages for the new data\n","#print(age_predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the TransformerEncoder layer\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","    \n","    def call(self, inputs, training):\n","        print(\"Input shape:\", inputs.shape)  # Add this line for debugging\n","        attn_output = self.att(inputs, inputs)\n","        print(\"Attention output shape:\", attn_output.shape)  # Add this line for debugging\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        print(\"FFN output shape:\", ffn_output.shape)  # Add this line for debugging\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","# Define the Transformer-based age prediction model\n","def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout=0.1, mlp_dropout=0.1):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","\n","    # Create multiple TransformerEncoder blocks\n","    for _ in range(num_blocks):\n","        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(mlp_dropout)(x)\n","    \n","    # MLP for age prediction\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","\n","    age_output = layers.Dense(1, activation=\"linear\")(x)\n","\n","    return keras.Model(inputs, age_output, name=\"transformer_age_prediction\")\n","\n","# Create and compile the Transformer-based age prediction model\n","embed_dim = 53  # Embedding dimension\n","num_heads = 4  # Number of attention heads\n","ff_dim = 53  # Feedforward dimension in the TransformerEncoder\n","num_blocks = 4  # Number of TransformerEncoder blocks\n","mlp_units = [128]  # MLP layer sizes\n","mlp_dropout = 0.4  # MLP dropout rate\n","dropout = 0.1  # Dropout rate\n","model = create_transformer_model(\n","    input_shape=(beta_train.shape[1],),  # Adjust the input shape to match your data\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    ff_dim=ff_dim,\n","    num_blocks=num_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")\n","\n","model.compile(\n","    loss=\"mean_squared_error\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"mean_squared_error\"],\n",")\n","\n","# Define a custom data generator\n","def data_generator(beta_values, age_labels, batch_size=32):\n","    num_samples = len(beta_values)\n","    while True:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        for start in range(0, num_samples, batch_size):\n","            batch_indices = indices[start : start + batch_size]\n","            yield beta_values[batch_indices], age_labels[batch_indices]\n","\n","# Create the data generators\n","batch_size = 32\n","train_data_generator = data_generator(beta_train, age_train, batch_size)\n","test_data_generator = data_generator(beta_test, age_test, batch_size)\n","\n","# Train the model\n","epochs = 10  # Adjust the number of epochs as needed\n","model.fit(train_data_generator, epochs=epochs, steps_per_epoch=len(beta_train) // batch_size)\n","\n","# Evaluate the model on the test set\n","test_loss = model.evaluate(test_data_generator, steps=len(beta_test) // batch_size)\n","print(f'Test loss: {test_loss}')\n","\n","# Make age predictions on new data\n","# Replace 'new_data.csv' with your new data file\n","#new_data_df = pd.read_csv('new_data.csv', index_col=0)\n","#new_beta_values = new_data_df.values\n","#age_predictions = model.predict(new_beta_values)\n","\n","# age_predictions now contains the predicted ages for the new data\n","#print(age_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def call(self, inputs, training):\n","    print(\"Input shape:\", inputs.shape)  # Add this line for debugging\n","    attn_output = self.att(inputs, inputs)\n","    print(\"Attention output shape:\", attn_output.shape)  # Add this line for debugging\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(inputs + attn_output)\n","    ffn_output = self.ffn(out1)\n","    print(\"FFN output shape:\", ffn_output.shape)  # Add this line for debugging\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    return self.layernorm2(out1 + ffn_output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Assuming you have loaded your data into beta_values and age_labels\n","# Check the shape of beta_values\n","print(\"Shape of beta_values:\", beta_values.shape)\n","\n","# Check the shape of age_labels\n","print(\"Shape of age_labels:\", age_labels.shape)\n","\n","beta_values.shape[1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","\n","# Define the TransformerEncoder layer\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Define the Transformer-based age prediction model\n","def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_blocks, mlp_units, dropout=0.1, mlp_dropout=0.1):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","\n","    # Create multiple TransformerEncoder blocks\n","    for _ in range(num_blocks):\n","        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(mlp_dropout)(x)\n","    \n","    # MLP for age prediction\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","\n","    age_output = layers.Dense(1, activation=\"linear\")(x)\n","\n","    return keras.Model(inputs, age_output, name=\"transformer_age_prediction\")\n","\n","# Example data preparation (replace this with your own data loading)\n","# Assuming beta_values is a NumPy array with shape (656, 53)\n","# and age_labels is a NumPy array with shape (656,)\n","\n","# Create and compile the Transformer-based age prediction model\n","embed_dim = 53  # Embedding dimension (should match the number of CpG sites)\n","num_heads = 4  # Number of attention heads\n","ff_dim = 128  # Feedforward dimension in the TransformerEncoder\n","num_blocks = 4  # Number of TransformerEncoder blocks\n","mlp_units = [128]  # MLP layer sizes\n","mlp_dropout = 0.4  # MLP dropout rate\n","dropout = 0.1  # Dropout rate\n","\n","# Split your data into training and testing sets (adjust as needed)\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(beta_values, age_labels, test_size=0.2, random_state=42)\n","\n","model = create_transformer_model(\n","    input_shape=(beta_values.shape[0],),\n","    embed_dim=embed_dim,\n","    num_heads=num_heads,\n","    ff_dim=ff_dim,\n","    num_blocks=num_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")\n","\n","model.compile(\n","    loss=\"mean_squared_error\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"mean_squared_error\"],\n",")\n","\n","# Define a custom data generator\n","def data_generator(features, labels, batch_size=32):\n","    num_samples = len(features)\n","    while True:\n","        indices = np.arange(num_samples)\n","        np.random.shuffle(indices)\n","        for start in range(0, num_samples, batch_size):\n","            batch_indices = indices[start : start + batch_size]\n","            yield features[batch_indices], labels[batch_indices]\n","\n","# Create the data generators\n","batch_size = 32\n","train_data_generator = data_generator(X_train, y_train, batch_size)\n","test_data_generator = data_generator(X_test, y_test, batch_size)\n","\n","# Train the model\n","epochs = 10  # Adjust the number of epochs as needed\n","model.fit(train_data_generator, epochs=epochs, steps_per_epoch=len(X_train) // batch_size)\n","\n","# Evaluate the model on the test set\n","test_loss = model.evaluate(test_data_generator, steps=len(X_test) // batch_size)\n","print(f'Test loss: {test_loss}')\n","\n","# Make age predictions on new data\n","# Replace 'new_data.csv' with your new data file\n","# new_data_df = pd.read_csv('new_data.csv', index_col=0)\n","# new_beta_values = new_data_df.values\n","# age_predictions = model.predict(new_beta_values)\n","\n","# age_predictions now contains the predicted ages for the new data\n","# print(age_predictions)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Embedding, LayerNormalization, Flatten, Concatenate\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","y = sample_age_gender_data_40279['Age']\n","\n","# Convert the target variable to a numeric type\n","y = y.astype('float32')\n","\n","# Reshape data for Transformer (assuming each row corresponds to a sample)\n","X = X.values.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","# Define Transformer Encoder model\n","def transformer_encoder(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = Embedding(input_dim=input_shape[1], output_dim=64)(inputs)  # Embedding layer for positional encoding\n","    x = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","    x = Flatten()(x)\n","    return Model(inputs=inputs, outputs=x)\n","\n","# Create Transformer Encoder model\n","transformer_output = transformer_encoder((X_train_scaled.shape[1], 1))(Input(shape=(X_train_scaled.shape[1], 1)))\n","\n","# Create MLP model\n","mlp_input = Input(shape=(X_train_scaled.shape[1],))\n","x = Dense(100, activation='relu')(mlp_input)\n","x = Dense(50, activation='relu')(x)\n","\n","# Concatenate Transformer output and MLP output\n","combined = Concatenate()([transformer_output, x])\n","\n","# Final regression layer\n","output = Dense(units=1, activation='linear')(combined)\n","\n","# Create the combined model\n","model = Model(inputs=[transformer_encoder.input, mlp_input], outputs=output)\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n","\n","# Train the model\n","model.fit([X_train_scaled, X_train_scaled], y_train, epochs=50, batch_size=32, validation_data=([X_test_scaled, X_test_scaled], y_test))\n","\n","# Make predictions on the test data\n","y_pred = model.predict([X_test_scaled, X_test_scaled])\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)\n","\n","# Now you can use the trained model to predict age for new data\n","new_data_to_predict = pivoted_samples_30870\n","X_new_data_to_predict = new_data_to_predict[new_data_to_predict.index.isin(filtered_probe_ids_07)].T\n","\n","# Impute missing values (NaN) with mean values for Expression\n","X_new_data_to_predict = imputer.transform(X_new_data_to_predict)\n","\n","# Reshape and standardize the new data\n","X_new_data_to_predict_scaled = scaler.transform(X_new_data_to_predict.reshape(1, -1))\n","\n","# Make predictions on the new data\n","predicted_age = model.predict([X_new_data_to_predict_scaled, X_new_data_to_predict_scaled])\n","\n","# Print the predicted age\n","print(predicted_age)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Embedding, LayerNormalization, Flatten, Concatenate\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming pivoted_samples_07 and sample_age_gender_data are already defined\n","\n","# Extract the expression values for the probes in filtered_probe_ids_07\n","X = pivoted_samples_07.T\n","\n","# Load the age values for the dataset\n","y = sample_age_gender_data_40279['Age']\n","\n","# Convert the target variable to a numeric type\n","y = y.astype('float32')\n","\n","# Reshape data for Transformer (assuming each row corresponds to a sample)\n","X = X.values.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the input features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1]))\n","X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], X_test.shape[1]))\n","\n","# Define Transformer model\n","def transformer_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = Embedding(input_dim=input_shape[1], output_dim=64)(inputs)  # Embedding layer for positional encoding\n","    x = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","    x = Flatten()(x)  # Flatten the output for MLP\n","\n","    return Model(inputs=inputs, outputs=x)\n","\n","# Define MLP model\n","def mlp_model(input_dim):\n","    inputs = Input(shape=(input_dim,))\n","    x = Dense(units=100, activation='relu')(inputs)\n","    x = Dense(units=50, activation='relu')(x)\n","    outputs = Dense(units=1, activation='linear')(x)  # Output layer with linear activation for regression\n","\n","    return Model(inputs=inputs, outputs=outputs)\n","\n","# Combine Transformer and MLP models\n","transformer_input_shape = (X_train_scaled.shape[1], 1)\n","transformer_encoder = transformer_model(transformer_input_shape)\n","\n","mlp_input_dim = transformer_input_shape[0] * transformer_input_shape[1]\n","mlp = mlp_model(mlp_input_dim)\n","\n","combined_model_input = Input(shape=transformer_input_shape)\n","transformer_output = transformer_encoder(combined_model_input)\n","mlp_output = mlp(transformer_output)\n","\n","combined_model = Model(inputs=combined_model_input, outputs=mlp_output)\n","\n","# Compile the combined model\n","combined_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n","\n","# Train the combined model\n","combined_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n","\n","# Make predictions on the test data\n","y_pred = combined_model.predict(X_test_scaled)\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r_squared = r2_score(y_test, y_pred)\n","\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r_squared)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Below is original code "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1693675799320,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"NwXqa5gu5sq9","outputId":"3f0460d6-e058-4ba6-fa76-bbe05f066657"},"outputs":[],"source":["#Filtering out unexpressed probes\n","expression_threshold = pivoted_samples_average.quantile(0.3)\n","expressed_probes = pivoted_samples_average[pivoted_samples_average >= expression_threshold].index.tolist()\n","print(\"number of probes above threshold: \", len(expressed_probes))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1693675821813,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"9dgHBs0w5sq9","outputId":"8fba5812-9c0a-4165-a346-170cabad6b49"},"outputs":[],"source":["#Redefine expression data using only the expressed probes\n","exprsdata = gse.pivot_samples(\"VALUE\").loc[expressed_probes]\n","exprsdata = exprsdata.T\n","#Deletes additional samples that aren't being analyzed\n","exprsdata = exprsdata[exprsdata.index.isin(list_samples)]\n","#Drop any probe columns where expression data is missing or negative\n","exprsdata.dropna(axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1693675830533,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"bkz7pKGL5sq9","outputId":"563727ce-4500-4af3-a750-24f2a9bea865","scrolled":true},"outputs":[],"source":["#Quantile normalization of data\n","rank_mean = exprsdata.stack().groupby(exprsdata.rank(method='first').stack().astype(int)).mean()\n","exprsdata.rank(method='min').stack().astype(int).map(rank_mean).unstack()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":777,"status":"ok","timestamp":1693675844274,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"0DNyx11t5sq9"},"outputs":[],"source":["#Compute PCA\n","pca = PCA(n_components=3)\n","principalComponents = pca.fit_transform(exprsdata)\n","principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1693675847534,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"6dr_DAjf5sq-"},"outputs":[],"source":["#Making Dataframe of samples to concatenate with principal components\n","samplesDf = pd.DataFrame.from_dict(all_samples, orient = 'index', columns = ['type'])\n","samplesDf.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1693675857001,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"dd3BE7ui5sq-","outputId":"41d6c73c-8e70-4725-b602-ee90e8269077","scrolled":true},"outputs":[],"source":["#Concatenate sample data with PCA data\n","principalDf = pd.concat([samplesDf, principalDf], axis=1)\n","principalDf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"executionInfo":{"elapsed":347,"status":"error","timestamp":1693675865629,"user":{"displayName":"姚博瀚 YAO, BO-HAN H24111057","userId":"01799014117333168150"},"user_tz":-480},"id":"X0KI1jso5sq-","outputId":"a4432dce-1f32-49bc-87e6-86815ae1f7f2"},"outputs":[],"source":["#PCA scatter plot\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","fig = plt.figure(figsize = (12,12))\n","ax = fig.gca(projection='3d')\n","ax.set_xlabel('Principal Component 1', fontsize = 15)\n","ax.set_ylabel('Principal Component 2', fontsize = 15)\n","ax.set_zlabel('Principal Component 3', fontsize = 15)\n","ax.set_title('3 Component PCA', fontsize = 20)\n","\n","types = ('control', 'treated')\n","colors = ['green', 'violet']\n","for type, color in zip(types, colors):\n","    indicesToKeep = principalDf['type'] == type\n","    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1'],\n","               principalDf.loc[indicesToKeep, 'principal component 2'], principalDf.loc[indicesToKeep, 'principal component 3'], c = color, s = 50)\n","ax.legend(types)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnnZwCZ25sq_","outputId":"b30e4a42-b906-4136-d78b-038af1b5edf8"},"outputs":[],"source":["#Calculate variance ratio\n","pca.explained_variance_ratio_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZTLKkHY5sq_","outputId":"c18c21cd-2176-4049-c705-977af03af9ad","scrolled":true},"outputs":[],"source":["#Transpose data matrix for sorting, index correlated to probe IDs\n","exprsdata = exprsdata.T\n","exprsdata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZZAFV75sq_","outputId":"9525e28f-36d2-4ddb-8b3a-52f355430f5d","scrolled":true},"outputs":[],"source":["#Sort expression matrix using 800 genes with greatest variance\n","variances = np.var(exprsdata, axis=1)\n","srt_idx = variances.argsort()[::-1]\n","data_sub = exprsdata.iloc[srt_idx].iloc[:800]\n","data_sub.index = data_sub.index.map(str)\n","data_sub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_1x_WD85sq_","outputId":"a6aab750-eb20-4790-ff8c-67b6e8aa2ee4"},"outputs":[],"source":["#Extract probe ids from data\n","probeids = list(data_sub.index)\n","probeids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzifXpWo5srA"},"outputs":[],"source":["#Upload annotation file as dictionary\n","dict1 = {}\n","with open('../data/probe2gene.txt') as f:\n","    for line in f:\n","        line = line.strip()\n","        (platform, probe, symbol) = line.split()\n","        dict1[probe] = symbol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV8dIoyy5srA","outputId":"d31dd4a6-d5d8-4c94-def5-8217e13bb8e4"},"outputs":[],"source":["#Examine how many ids are duplicates for gene symbols/unmatched\n","len(set(probeids) - dict1.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9vKICP-5srA","outputId":"a334967d-bfb7-45b4-eeb2-5909131b3acb","scrolled":true},"outputs":[],"source":["#Reset index and replace with gene symbols, view as dataframe\n","exprsdata = pd.DataFrame(exprsdata)\n","exprsdata['symbol'] = exprsdata.index.to_series().map(dict1)\n","exprsdata.reset_index(inplace=True)\n","data = exprsdata.set_index('symbol')\n","#Drop probe id column\n","data = data.drop('ID_REF', axis=1)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG7rucdC5srA","outputId":"d23c511f-79d8-4589-cd19-62b266bda683","scrolled":true},"outputs":[],"source":["#Drop rows that aren't associated with a particular gene symbol\n","data = data.reset_index().dropna().set_index('symbol')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuBzTB2B5srA","outputId":"343ea763-e5d8-48f2-cf45-45e759e5d062"},"outputs":[],"source":["#Standardized data to a text file\n","data_file = ('../expression_matrix_top800_genes.txt')\n","data.to_csv(data_file, sep='\\t')\n","data_file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMgsbI0G5srB"},"outputs":[],"source":["#Import required packages for characteristic direction and utilize warning statements\n","import warnings\n","from scipy.stats import chi2\n","from scipy.stats.mstats import zscore\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO0SvwDS5srB","outputId":"ca4a2e2f-ab2e-4a01-9448-c9c0b60d8bfb","scrolled":true},"outputs":[],"source":["#Make sample classes, ensure that there is a distinction between control/treated samples\n","data_cd = {}\n","\n","sample_classes = {}\n","sample_class = np.zeros(data.shape[1], dtype=np.int32)\n","sample_class[samplesDf['type'].values == 'control'] = 1\n","sample_class[samplesDf['type'].values == 'treated'] = 2\n","sample_classes = sample_class\n","\n","print(sample_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jc3rzcyn5srB"},"outputs":[],"source":["#CD results\n","cd_res = chdir(data.values, sample_classes, data.index, gamma=.5, sort=False, calculate_sig=False)\n","cd_coefs = np.array(list(map(lambda x: x[0], cd_res)))\n","\n","srt_idx = np.abs(cd_coefs).argsort()[::-1]\n","cd_coefs = cd_coefs[srt_idx][:600]\n","sorted_DEGs = data.index[srt_idx][:600]\n","up_genes = dict(zip(sorted_DEGs[cd_coefs > 0], cd_coefs[cd_coefs > 0]))\n","dn_genes = dict(zip(sorted_DEGs[cd_coefs < 0], cd_coefs[cd_coefs < 0]))\n","data_cd['up'] = up_genes\n","data_cd['dn'] = dn_genes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM6gUOVW5srB"},"outputs":[],"source":["#Retrieve up and down gene sets\n","up_list = list(up_genes.keys())\n","dn_list = list(dn_genes.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnMTE4il5srC"},"outputs":[],"source":["import json\n","import requests\n","\n","ENRICHR_URL = 'https://amp.pharm.mssm.edu/Enrichr'\n","\n","def _enrichr_add_list(genes, meta=''):\n","    genes_str = '\\n'.join(genes)\n","    payload = {\n","        'list': (None, genes_str),\n","        'description': (None, meta)\n","    }\n","    # POST genes to the /addList endpoint\n","    response = requests.post(\"%s/addList\" % ENRICHR_URL, files=payload)\n","    list_ids = json.loads(response.text)\n","    return list_ids\n","\n","def enrichr_link(genes, meta=''):\n","    list_ids = _enrichr_add_list(genes, meta)\n","    shortId = list_ids['shortId']\n","    link = '%s/enrich?dataset=%s' % (ENRICHR_URL, shortId)\n","    return link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6D6EZbY5srC","outputId":"5e71df07-d913-4bb2-92bd-54bc7bb7f2af","scrolled":true},"outputs":[],"source":["#Print Enrichr links for further analysis\n","for key, d in data_cd.items():\n","    time.sleep(1)\n","    genes = list(data_cd[key].keys())\n","    genes = [str(g) for g in genes]\n","    link = enrichr_link(genes, key)\n","    print(key)\n","    print(link)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
